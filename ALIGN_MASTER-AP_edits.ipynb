{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# To-do list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Things for Alex to do:**\n",
    "* [ ] Handling requirements (after getting them)\n",
    "* [ ] Dockerizing\n",
    "* [ ] Jupyter app-ifying\n",
    "* [ ] Getting Stanford tagger included automatically\n",
    "* [ ] Clean up markdown text (when final notebooks are ready)\n",
    "* [ ] See if I can implement w2v function (https://github.com/a-paxton/Gensim-LSI-Word-Similarities)\n",
    "* [ ] Convert functions into library\n",
    "* [ ] When run analysis, run syntax over token, lemma is weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Things for Nick to do:**\n",
    "* [x] Implement surrogate to match by conversation order AND conversation type\n",
    "* [x] Make file names more intuitive\n",
    "* [ ] Identify condition/dyad/number flexibly (using regex) - SKIPPED\n",
    "* [x] Allow surrogate baseline to be created using a smaller subset (permutations) â€” 2-3x?\n",
    "* [**???**] Do pip freeze or conda list -e > req.txt\n",
    "* [**???**] Redo analysis with new baseline + consider doing sample-wise shuffled baseline - Have questions on how to proceed\n",
    "* [ ] Go over manuscript again with new baseline + review comments/edits\n",
    "* [ ] Need to create a simple other_filler_list as a text file that can be modified by a user and imported to be used here - make note that we only catch 2-letter fillers at this point with the regular expression default \n",
    "* [ ] Note that align_concatenated_dataframe.txt takes the place of forSemantic.txt. Make updates accordingly. \n",
    "* [ ] We could/should probably make `convobyconvo` an optional add-on from `turnbyturn`.\n",
    "* [ ] Consider other POS taggers: https://stackoverflow.com/questions/30821188/python-nltk-pos-tag-not-returning-the-correct-part-of-speech-tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ALIGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook provides an introduction to **ALIGN**, a tool for quantifying multi-level linguistic similarity between speakers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Table of Contents**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [Getting Started](#Getting-Started)\n",
    "    * [Prerequisites](#Prerequisites)\n",
    "    * [Preparing input data](#Preparing-input-data)\n",
    "    * [Filename conventions](#Filename-conventions)\n",
    "    * [User-specified parameters](#User-specified-parameters)\n",
    "    * [Main calls](#Main-calls)\n",
    "* [Setup](#Setup)\n",
    "    * [Import libraries](#Import-libraries)\n",
    "    * [User-specified settings](#User-specified-settings)\n",
    "* [Phase 1: Generate \"prepped\" transcripts](#Phase-1:-Generate-\"prepped\"-transcripts)\n",
    "    * [](#)\n",
    "* [Phase 2: Generate alignment scores](#Phase-2:-Generate-alignment-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Jupyter Notebook with Python 2.7.1.3 kernel\n",
    "* Packages in `requirements.txt`\n",
    "\n",
    "*See notes in \"DISTRIBUTION ISSUES\" Notebook for suggestions on how to package effectively and accomodate Python 3 users*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preparing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each input text file needs to contain a single conversation organized in an `N x 2` matrix\n",
    "    * Text file must be tab-delimited.\n",
    "* Each row must correspond to a single conversational turn from a speaker.\n",
    "    * Rows must be temporally ordered based on their occurrence in the conversation.\n",
    "    * Rows must alternate between speakers.\n",
    "* Speaker identifier and content for each turn are divided across two columns.\n",
    "    * Column 1 must have the header `participant`.\n",
    "        * Each cell specifies the speaker.\n",
    "        * Each speaker must have a unique label (e.g., `P1` and `P2`, `0` and `1`).\n",
    "    * Column 2 must have the header `content`.\n",
    "        * Each cell corresponds to the transcribed utterance from the speaker.\n",
    "        * Each cell must end with a newline character: `\\n`\n",
    "* See folder `examples > toy_data-original` in Github repository for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filename conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Each conversation text file needs to be named in the format: `A_B.txt`\n",
    "    * `A` corresponds to the dyad number for that conversation\n",
    "    * `B` corresponding to a condition code for that conversation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### User-specified parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Define input path\n",
    "* Define input folder where original transcripts are located\n",
    "* Define folder to save prepped transcripts \n",
    "* Define folder to save surrogate transcripts \n",
    "* Decide maximum size for n-gram chunking\n",
    "    * Default: 3\n",
    "* Decide the minimum number of words for each turn\n",
    "    * Default: 3\n",
    "    * CRITICAL: The minimum number of words has to be at least as long as n-gram maximum size otherwise error will be generated\n",
    "* Decide on whether to run the Stanford tagger along with NLTK default tagger (slow) or NLTK tagger alone (fast)\n",
    "    * Default: 0 (NTLK tagger alone)\n",
    "    \n",
    "* remove_regex_fillers\n",
    "* remove_other_list\n",
    "    * need to combine the above into an either use regular expression or user-generated list\n",
    "    * now USE_FILLER_LIST\n",
    "    \n",
    "* Decide on max delay between partner's turns to generate alignment score \n",
    "    * Currently only option is for contiguous turns\n",
    "    * Will be updated in a future version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Main calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE1RUN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Converts each conversation into standardized format.\n",
    "* Each utterance is tokenized and lemmatized and has POS tags added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE2RUN_REAL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generates turn-level and conversation-level alignment scores (lexical, conceptual, and syntactic) across a range of n-gram sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`PHASE2RUN_SURROGATE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Generates a surrogate corpus.\n",
    "* Runs identical analysis as PHASE2RUN_REAL on the surrogate corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Checking latest version of Python and 3rd-party packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version Info:\n",
      "0.21.1\n",
      "Numpy Version Info:\n",
      "1.11.3\n",
      "Scipy Version Info:\n",
      "0.19.0\n",
      "NLTK Version Info:\n",
      "3.2.5\n",
      "Gensim Version Info:\n",
      "3.1.0\n",
      "Python and Conda Environment Info:\n",
      "2.7.13 |Anaconda 2.3.0 (x86_64)| (default, Dec 20 2016, 23:05:08) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import scipy\n",
    "import nltk\n",
    "import gensim \n",
    "\n",
    "print(\"Pandas Version Info:\\n{}\".format(pandas.__version__))\n",
    "print(\"Numpy Version Info:\\n{}\".format(numpy.__version__))\n",
    "print(\"Scipy Version Info:\\n{}\".format(scipy.__version__))\n",
    "print(\"NLTK Version Info:\\n{}\".format(nltk.__version__))\n",
    "print(\"Gensim Version Info:\\n{}\".format(gensim.__version__))\n",
    "\n",
    "import sys\n",
    "print(\"Python and Conda Environment Info:\\n{}\".format(sys.version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, we'll get ready to run ALIGN over our target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top](#ALIGN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os,re,math,csv,string,random,logging,glob,ast,itertools,operator\n",
    "from os import listdir \n",
    "from os.path import isfile, join \n",
    "from collections import Counter, defaultdict \n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Third-party libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For data analysis and data handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For natural language processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn \n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Specify the NLTK default POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nduran/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** With older version of NLTK (pre 3.1), the maxent_treebank_pos_tagger is also available: `nltk.download('maxent_treebank_pos_tagger')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Specify additional POS tagger: Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note**: The `StanfordPOSTagger` will be\n",
    "used in conjunction with local folder `stanford-postagger-2015-04-20` and `.jar` file. Both files will be called below if analysis is being run with the Stanford tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For building semantic space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## User-specified settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Directories and folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set working directory, in which all notebook and supporting files are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Or \"Pathname for the unzipped project folder\" if going with `anaconda-project.yml` configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INPUT_PATH=os.getcwd()+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder containing the original transcript files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TRANSCRIPTS = 'toy_data-original/'\n",
    "TRANSCRIPTS = \"TRANSCRIPTS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder into which prepared transcript files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PREPPED_TRANSCRIPTS = 'toy_data-prepped/'\n",
    "PREPPED_TRANSCRIPTS = \"PREPPED_TRANSCRIPTS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder into which analysis-ready dataframe files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ANALYSIS_READY = 'toy_data-analysis/'\n",
    "ANALYSIS_READY = \"ANALYSIS_RESULTS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set variable for folder name (as string) for relative location of folder into which all prepared surrogate transcript files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SURROGATE_TRANSCRIPTS = 'toy_data-surrogate/'\n",
    "SURROGATE_TRANSCRIPTS = \"SURROGATE_TRANSCRIPTS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analysis settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set maximum size for n-gram chunking. (Default: 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAXNGRAM = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set minimum number of words for each turn. (Default: 3)\n",
    "\n",
    "CRITICAL: The minimum number of words has to be at least as long as n-gram maximum size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MINWORDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Choose POS tagger. \n",
    "\n",
    "* DEFAULT: Enter `0` to run NLTK default POS tagger (NLTK 3.0.3: maxent_treebank_pos_tagger)\n",
    "* Enter `1` to run both NLTK default POS tagger and Stanford POS tagger. Adding the Stanford POS tagger will lead to an increase in processing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ADD_STANFORD_TAGS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set max delay between partner's turns when generating alignment score. Currently, the only acceptable value is 1 (i.e., contiguous turns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DELAY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Choose method for removing speech fillers \n",
    "\n",
    "* DEFAULT: Choose to remove 2-letter fillers (via regular expressions)    \n",
    "* .txt list of filler words or regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "USE_FILLER_LIST = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 1: Generate \"prepped\" transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initial clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **[Clean up text](#Clean-up-text)** by removing:\n",
    "    * numbers, punctuation, and other non-ASCII alphabet characters\n",
    "    * common speech fillers (e.g., \"um\", \"huh\") and their derivations\n",
    "    * empty turns that may have inadvertently been included\n",
    "    * user-specified short turns\n",
    "        * removes short turns that are at least as long as maximum n-gram\n",
    "* **[Merge adjacent turns by the same participant](#Merge-adjacent-turns-by-the-same-participant)** into a single utterance row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top](#ALIGN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean up text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ND** Problem in initial function in that empty turns were throwing in an error in Line 30. It is necessary to drop empty turns before removing fillers (see Line 27)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ND** Corrected problem in how list comprehension on Line 40 was searching through filler_list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ALEX** Is there a better way of handling Line 19? Add it to the main function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ND** Also changed how we were counting number of words. Before I was using regular expressions that counted something like \"let's\" as one word when should be considered as two words (let us) given this is the what the ngram analysis is based on. To keep what is considered a word consistent across functions, now counting words based on NLTK tokenizer (Line 51) which breaks something like \"let's\" into two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def InitialCleanup(dataframe,\n",
    "                        MINWORDS=3,\n",
    "                        USE_FILLER_LIST=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform basic text cleaning to prepare dataframe\n",
    "    for analysis. Remove non-letter/-space characters,\n",
    "    empty turns, turns below a minimum length (default:\n",
    "    1 word), and fillers.\n",
    "    \n",
    "    By default, remove 2-letter fillers through regex.\n",
    "    If desired, skip regex filtering of fillers with\n",
    "    `USE_FILLER_LIST=1`.\n",
    "    \n",
    "    If desired, remove other words (e.g., fillers) \n",
    "    passed as a list to `filler_list` argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    filler_list=ast.literal_eval(file(INPUT_PATH+'fillers.txt').read().lower())\n",
    "    \n",
    "    # only allow strings, spaces, and newlines to pass\n",
    "    WHITELIST = string.letters + '\\'' + ' '\n",
    "    clean = []\n",
    "    utteranceLen = []\n",
    "     \n",
    "    ## remove inadvertent empty turns \n",
    "    dataframe = dataframe[pd.notnull(dataframe['content'])]\n",
    "    \n",
    "    for value in dataframe['content'].values:            \n",
    "        cleantext = ''.join(c for c in value if c in WHITELIST).lower() \n",
    "\n",
    "        ## remove typical speech fillers, examples: \"um, mm, oh, hm, uh\"\n",
    "        if USE_FILLER_LIST == 0:\n",
    "            cleantext = re.sub('^[uhmo]+[mh]+\\s', ' ', cleantext) ##// at the start of a string\n",
    "            cleantext = re.sub('\\s[uhmo]+[mh]+\\s', ' ', cleantext) ##// within a string \n",
    "            cleantext = re.sub('\\s[uhmo]+[mh]$', ' ', cleantext) ##// end of a string \n",
    "            \n",
    "        # OPTIONAL: remove speech fillers or other words specified by user in a list\n",
    "        if USE_FILLER_LIST == 1:\n",
    "            cleantext = [word for word in cleantext.split(\" \") if word not in filler_list]\n",
    "            cleantext = \" \".join(cleantext)\n",
    "                    \n",
    "        # append cleaned lines\n",
    "        clean.append(cleantext)        \n",
    "                \n",
    "    ## drop the old \"content\" column and add the clean \"content\" column\n",
    "    dataframe = dataframe.iloc[:, [0,1]]\n",
    "    dataframe['content'] = clean\n",
    "        \n",
    "    ## remove rows that are now blank or do not meet \"MINWORDS\" requirement, then drop length column    \n",
    "    dataframe['utteranceLen'] = dataframe['content'].apply(lambda x: word_tokenize(x)).str.len()\n",
    "    dataframe = dataframe.drop(dataframe[dataframe.utteranceLen < int(MINWORDS)].index)\n",
    "    dataframe = dataframe.iloc[:, [0,1]]\n",
    "        \n",
    "    # return the cleaned dataframe    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Merge adjacent turns by the same participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def AdjacentMerge(dataframe):\n",
    "    \"\"\"\n",
    "    ADD HERE\n",
    "    \"\"\"    \n",
    "    \n",
    "    repeat=1\n",
    "    while repeat==1:\n",
    "        l1=len(dataframe) \n",
    "        DfMerge = []\n",
    "        k = 0\n",
    "        if len(dataframe) > 0:\n",
    "            while k < len(dataframe)-1: \n",
    "                if dataframe['participant'].iloc[k] != dataframe['participant'].iloc[k+1]:\n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])         \n",
    "                    k = k + 1\n",
    "                elif dataframe['participant'].iloc[k] == dataframe['participant'].iloc[k+1]:                    \n",
    "                    DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k] + \" \" + dataframe['content'].iloc[k+1]])           \n",
    "                    k = k + 2   \n",
    "            if k == len(dataframe)-1:\n",
    "                DfMerge.append([dataframe['participant'].iloc[k], dataframe['content'].iloc[k]])      \n",
    "        \n",
    "        dataframe=pd.DataFrame(DfMerge,columns=('participant','content'))\n",
    "        if l1==len(dataframe): \n",
    "            repeat=0 \n",
    "                \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare transcript text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* **[Check spelling](#Check-spelling)** via a Bayesian spell-checking algorithm (http://norvig.com/spell-correct.html).\n",
    "* **[Tokenize and apply spell correction](#Tokenize-and-apply-spell-correction)** to the original transcript text.\n",
    "* **[Lemmatize](#Lemmatize)** using WordNet-derived categories.\n",
    "* [**Part-of-speech tagging**](#Part-of-speech-tagging) with user-defined tagger(s) on both lemmatized and non-lemmatized tokens.\n",
    "    * Users may choose to use the NLTK default POS tagger (default) and/or the Stanford POS tagger (optional). The NLTK default tagger is more time-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ND** For tokenization, I noticed a problem that when you tokenize something any contraction, like \"let's,\" the tokenizer creates \"let\" and \"'s\", so when the spell checker sees \"'s\" is automatically changes it to \"is\" when it should be \"us.\" To minimize as many problems as possible, I now have set up Lines 29-37 to import a list of common contractions and their expanded forms, converts it to a dictionary, and in Line 41, does the conversion. However, for contractions not in the list, see Line 44 in that spell checker now skips situations where \"'s\" appears as a token. I should note that this is only a real issue if the original transcription avoids contractions altogether, but this is a somewhat unrealistic expectation.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ALEX** Should lines 29-37 be handled in main function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize and apply spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Tokenize(text,nwords):\n",
    "    \"\"\"\n",
    "    Given list of text to be processed and a list \n",
    "    of known words, return a list of edited and \n",
    "    tokenized words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # internal function: identify possible spelling errors for a given word\n",
    "    def edits1(word): \n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = [a + b[1:] for a, b in splits if b]\n",
    "        transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "        replaces   = [a + c + b[1:] for a, b in splits for c in string.lowercase if b]\n",
    "        inserts    = [a + c + b     for a, b in splits for c in string.lowercase]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    # internal function: identify known edits\n",
    "    def known_edits2(word,nwords):\n",
    "        return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in nwords)\n",
    "\n",
    "    # internal function: identify known words\n",
    "    def known(words,nwords): return set(w for w in words if w in nwords)\n",
    "\n",
    "    # internal function: correct spelling\n",
    "    def correct(word,nwords):\n",
    "        candidates = known([word],nwords) or known(edits1(word),nwords) or known_edits2(word,nwords) or [word]\n",
    "        return max(candidates, key=nwords.get)\n",
    "\n",
    "    # expand out based on a fixed list of common contractions \n",
    "    contract_list=INPUT_PATH+'contractions.txt'\n",
    "    contract_dict = ast.literal_eval(file(contract_list).read().lower())\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contract_dict.keys()))      \n",
    "    # internal function:    \n",
    "    def expand_contractions(text, contractions_re=contractions_re):\n",
    "        def replace(match):\n",
    "            return contract_dict[match.group(0)]\n",
    "        return contractions_re.sub(replace, text.lower())\n",
    "\n",
    "    # process all words in the text\n",
    "    cleantoken = []\n",
    "    text = expand_contractions(text)\n",
    "    token = word_tokenize(text)\n",
    "    for word in token:        \n",
    "        if \"'\" not in word:\n",
    "            cleantoken.append(correct(word,nwords))\n",
    "        else:\n",
    "            cleantoken.append(word) \n",
    "\n",
    "#     cleantoken = []\n",
    "#     token = word_tokenize(text)    \n",
    "#     for word in token:\n",
    "#         cleantoken.append(correct(word,nwords))\n",
    "\n",
    "    return cleantoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pos_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert NLTK default tagger output into a format that Wordnet\n",
    "    can use in order to properly lemmatize the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create some inner functions for simplicity\n",
    "    def is_noun(tag):\n",
    "        return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "    def is_verb(tag):\n",
    "        return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    def is_adverb(tag):\n",
    "        return tag in ['RB', 'RBR', 'RBS']\n",
    "    def is_adjective(tag):\n",
    "        return tag in ['JJ', 'JJR', 'JJS']\n",
    "    \n",
    "    # check each tag against possible categories\n",
    "    if is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    else:\n",
    "        return wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Lemmatize(tokenlist):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    defaultPos = nltk.pos_tag(tokenlist) # get the POS tags from NLTK default tagger\n",
    "    words_lemma = []\n",
    "    for item in defaultPos:  \n",
    "        words_lemma.append(lemmatizer.lemmatize(item[0],pos_to_wn(item[1]))) # need to convert POS tags to a format (NOUN, VERB, ADV, ADJ) that wordnet uses to lemmatize\n",
    "    return words_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**PROBLEM** In original version, problem with ApplyTokenLemmatize at line 7. When you run Line 6, which shows the output of the Tokenize function, everything looks correct. However, when appending to the dataframe within the loop (Line 7), it either fails to save or saves as a null list. This failure can be seen in the output of Line 8. However, for reasons I do not know, this problem only occurs when dropping utterances when they do not meet minimium word requirements. That is why we did not see any issues when MINWORDS was set to 1 (in toy dataset, there were no instances of single words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def ApplyTokenLemmatize(df,nwords):\n",
    "#     df['token'] = \"\"\n",
    "#     df['lemma'] = \"\"\n",
    "#     for i in range(0,len(df)):\n",
    "        \n",
    "#         print(\"What SHOULD be returned from tokenization function:\",Tokenize(df['content'].iloc[i],nwords))\n",
    "#         df['token'].iloc[i]=Tokenize(df['content'].iloc[i],nwords) ### <<<<<< PROBLEM IS HERE! \n",
    "#         print(\"What IS actually being saved to the updated df:\",df['token'].iloc[i])\n",
    "        \n",
    "        \n",
    "#         df['lemma'].iloc[i]=Lemmatize(df['token'].iloc[i])  \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ND** Corrected version of ApplyTokenLemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ApplyTokenLemmatize(df,nwords):\n",
    "    token = []\n",
    "    lemma = []\n",
    "    for i in range(0,len(df)):\n",
    "        token.append(Tokenize(df['content'].iloc[i],nwords))        \n",
    "        lemma.append(Lemmatize(Tokenize(df['content'].iloc[i],nwords)))\n",
    "    df['token'] = token\n",
    "    df['lemma'] = lemma     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ALEX** Is this the best way of handling ADD_STANFORD_TAGS by putting it in the function? If I want to change to run analyses with or without Stanford I have to update it across multiple functions. It would be much easier to just declare it **once** as \"0\" or \"1\" early (as done in the \"Analysis Settings\" section) and it ensures that it is consistent across all functions that rely on it. Would help avoid mistakes, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**ND** Note that dropping \"Penn\" in tagger name when saving to file because it turns out this is the wrong conceptualization. What \"Penn\" really corresponds to is the default NLTK tagger, which happens to be `averaged_perceptron_tagger` for the updated NLTK version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ApplyPOSTagging(df,filename,ADD_STANFORD_TAGS=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Apply part-of-speech tagging to a dataframe of conversation turns \n",
    "    (df). Pass filename as a string to create create a new df variable. \n",
    "    By default, return only tags from the NLTK default POS tagger. Optionally,\n",
    "    also return Stanford POS tagger results by setting  \n",
    "    `ADD_STANFORD_TAGS=1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create new columns in our dataframe\n",
    "    df['tagged_token'] = \"\"\n",
    "    df['tagged_lemma'] = \"\"\n",
    "    df['file'] = \"\"\n",
    "    if ADD_STANFORD_TAGS == 1:\n",
    "        df['tagged_stan_token'] = \"\"\n",
    "        df['tagged_stan_lemma'] = \"\"\n",
    "        \n",
    "    # if desired, import Stanford tagger\n",
    "    if ADD_STANFORD_TAGS == 1:\n",
    "        stanford_tagger = StanfordPOSTagger(INPUT_PATH + 'stanford-postagger-2015-04-20/models/english-bidirectional-distsim.tagger',\n",
    "                                            INPUT_PATH + 'stanford-postagger-2015-04-20/stanford-postagger.jar')\n",
    "    \n",
    "    # cycle through each line in the dataframe\n",
    "    for i in range(0,len(df)):\n",
    "        df['file'].iloc[i]=filename\n",
    "\n",
    "        # by default, tag with ... <<<<<<<<<\n",
    "        pos_token=nltk.pos_tag(df['token'].iloc[i])\n",
    "        df['tagged_token'].iloc[i]=pos_token \n",
    "        pos_lemma=nltk.pos_tag(df['lemma'].iloc[i])\n",
    "        df['tagged_lemma'].iloc[i]=pos_lemma \n",
    "\n",
    "        # if desired, also tag with Stanford tagger\n",
    "        if ADD_STANFORD_TAGS == 1:\n",
    "            pos_stan_token=stanford_tagger.tag(df['token'].iloc[i])\n",
    "            df['tagged_stan_token'].iloc[i]=pos_stan_token    \n",
    "            pos_stan_lemma=stanford_tagger.tag(df['lemma'].iloc[i])\n",
    "            df['tagged_stan_lemma'].iloc[i]=pos_stan_lemma  \n",
    "\n",
    "    # return finished dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RUN Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* For each original transcript file, saves new file with columns for:\n",
    "    * \"Clean\" text\n",
    "    * Tokenized words\n",
    "    * Tokenized lemmatized-words\n",
    "    * NLTK default POS-tagging on tokenized words\n",
    "    * NLTK default POS-tagging on lemmatized words\n",
    "    * Stanford POS-tagging on tokenized words\n",
    "    * Stanford POS-tagging on lemmatized-words\n",
    "* Also saves a single datasheet with all tokenized lemmatized utterances from all transcripts as individual rows\n",
    "    * called align_concatenated_dataframe.txt\n",
    "    * to be used in building semantic space for Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def PHASE1RUN(input_file_directory, \n",
    "                   output_file_directory,\n",
    "                   training_dictionary=INPUT_PATH+'big.txt',\n",
    "                   ADD_STANFORD_TAGS=0):   \n",
    "\n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files, \n",
    "    return a completely prepared dataframe of transcribed \n",
    "    conversations for later ALIGN analysis, including: text \n",
    "    cleaning, merging adjacent turns, spell-checking, \n",
    "    tokenization, lemmatization, and part-of-speech tagging. \n",
    "    By default, return only the NLTK default \n",
    "    POS tagger values; optionally, also return Stanford POS tagger\n",
    "    values with `add_standford_tagger=1`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create an internal function to train the model\n",
    "    def train(features): \n",
    "        model = defaultdict(lambda: 1)\n",
    "        for f in features:\n",
    "            model[f] += 1\n",
    "        return model\n",
    "        \n",
    "    # train our spell-checking model\n",
    "    nwords = train(re.findall('[a-z]+',(file(training_dictionary).read().lower())))\n",
    "    \n",
    "    # cycle through all files \n",
    "#     import glob\n",
    "    file_list = glob.glob(input_file_directory+\"*.txt\")\n",
    "#     file_list = glob.glob(input_file_directory+\"dyad_10-condition_2.txt\")\n",
    "    \n",
    "    main = []\n",
    "    for fileName in file_list:      \n",
    "        \n",
    "        # let us know which file we're processing\n",
    "        dataframe = pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        print \"Processing: \"+fileName\n",
    "\n",
    "        # clean up, merge, spellcheck, tokenize, lemmatize, and POS-tag\n",
    "        dataframe = InitialCleanup(dataframe)\n",
    "        dataframe = AdjacentMerge(dataframe)\n",
    "        dataframe = ApplyTokenLemmatize(dataframe,nwords)\n",
    "        \n",
    "        dataframe = ApplyPOSTagging(dataframe, \n",
    "                                    os.path.basename(fileName), \n",
    "                                    ADD_STANFORD_TAGS)\n",
    "        \n",
    "        # export the conversation's dataframe as a CSV\n",
    "        dataframe.to_csv(output_file_directory + os.path.basename(fileName), \n",
    "                         encoding='utf-8',index=False,sep='\\t')\n",
    "        main.append(dataframe)\n",
    "\n",
    "    # save the concatenated dataframe\n",
    "    main = pd.concat(main, axis=0)\n",
    "    main.to_csv(output_file_directory +'../' + \"align_concatenated_dataframe.txt\",encoding='utf-8',\n",
    "                index=False, sep='\\t')\n",
    "    \n",
    "    # return the dataframe\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PHASE1RUN(input_file_directory=INPUT_PATH+TRANSCRIPTS,\n",
    "#                       output_file_directory=INPUT_PATH+PREPPED_TRANSCRIPTS,\n",
    "#                       training_dictionary=INPUT_PATH+'big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Phase 2: Generate alignment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [**Create helper functions**](#Create-helper-functions) for processing turn- and conversation-level data.\n",
    "* **[Build semantic space](#Build-semantic-space)** from the `forSemantic.txt` generated in Phase 1 and return a `word2vec` semantic space and vocabulary list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[To top.](#ALIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_pos(sequence1,sequence2,\n",
    "                   ignore_duplicates=True,\n",
    "                   ngramsize=2):\n",
    "    \"\"\"\n",
    "    Remove mimicked lexical sequences from two interlocutors'\n",
    "    sequences and return a dictionary of counts of ngrams\n",
    "    of the desired size for each sequence.\n",
    "    \n",
    "    By default, consider bigrams. If desired, this may be \n",
    "    changed by setting `ngramsize` to the appropriate \n",
    "    value.\n",
    "    \n",
    "    By default, ignore duplicate lexical n-grams when\n",
    "    processing these sequences. If desired, this may\n",
    "    be changed with `ignore_duplicates=False`.\n",
    "    \"\"\"     \n",
    "\n",
    "    # remove duplicates and recreate sequences\n",
    "    sequence1 = set(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = set(ngrams(sequence2,ngramsize))\n",
    " \n",
    "    # if desired, remove duplicates from sequences\n",
    "    if ignore_duplicates==True:\n",
    "        new_sequence1 = [tuple([' '.join(pair) for pair in tup]) for tup in list(sequence1 - sequence2)]\n",
    "        new_sequence2 = [tuple([' '.join(pair) for pair in tup]) for tup in list(sequence2 - sequence1)]\n",
    "    else:\n",
    "        new_sequence1 = [tuple([' '.join(pair) for pair in tup]) for tup in sequence1]\n",
    "        new_sequence2 = [tuple([' '.join(pair) for pair in tup]) for tup in sequence2]\n",
    "        \n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ngram_lexical(sequence1,sequence2,ngramsize=2):\n",
    "    \"\"\"\n",
    "    Create ngrams of the desired size for each of two\n",
    "    interlocutors' sequences and return a dictionary \n",
    "    of counts of ngrams for each sequence.\n",
    "    \n",
    "    By default, consider bigrams. If desired, this may be \n",
    "    changed by setting `ngramsize` to the appropriate \n",
    "    value.\n",
    "    \"\"\"   \n",
    "    \n",
    "    # generate ngrams\n",
    "    sequence1 = list(ngrams(sequence1,ngramsize))\n",
    "    sequence2 = list(ngrams(sequence2,ngramsize)) \n",
    "\n",
    "    # join for counters\n",
    "    new_sequence1 = [' '.join(pair) for pair in sequence1]\n",
    "    new_sequence2 = [' '.join(pair) for pair in sequence2]\n",
    "    \n",
    "    # return counters\n",
    "    return Counter(new_sequence1), Counter(new_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2): \n",
    "    \"\"\"\n",
    "    Derive cosine similarity metric, standard measure.\n",
    "    Adapted from <https://stackoverflow.com/a/33129724>.\n",
    "    \"\"\"     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_composite_semantic_vector(lemma_seq,vocablist,highDimModel):\n",
    "    \"\"\"\n",
    "    Function for producing vocablist and model is called in the main loop\n",
    "    \"\"\"\n",
    "    getComposite = [0] * len(highDimModel[vocablist[1]])    \n",
    "    for w1 in lemma_seq:\n",
    "        if w1 in vocablist:\n",
    "            semvector = highDimModel[w1]\n",
    "            getComposite = getComposite + semvector    \n",
    "    return getComposite\n",
    "\n",
    "# what we want to do here is find the union of the vocablist within the HDM and then sum over all of the columns.\n",
    "# should be faster/easier than current instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build semantic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BuildSemanticModel(semantic_model_input_file,\n",
    "                            high_sd_cutoff=3,\n",
    "                            low_n_cutoff=1):\n",
    "    \"\"\"\n",
    "    Given an input file produced by the ALIGN Phase 1 functions, \n",
    "    build a semantic model from all transcripts in all conversations\n",
    "    in target corpus after removing high- and low-frequency words.\n",
    "    High-frequency words are determined by a user-defined number of\n",
    "    SDs over the mean (by default, `high_sd_cutoff=3`). Low-frequency\n",
    "    words must appear over a specified number of raw occurrences \n",
    "    (by default, `low_n_cutoff=1`).\n",
    "    \n",
    "    Frequency cutoffs can be removed by `high_sd_cutoff=None` and/or\n",
    "    `low_n_cutoff=0`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read in the file\n",
    "    data1 = pd.read_csv(semantic_model_input_file, sep='\\t',encoding='utf-8')\n",
    "    \n",
    "    # get frequency count of all included words\n",
    "    all_words = filter(str.isalpha,[word.strip() for word in str(data1['lemma']).split(',')])\n",
    "    frequency = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        frequency[word] += 1\n",
    "        \n",
    "    # remove words that only occur more frequently than our cutoff (defined in occurrences)\n",
    "    frequency = {word: freq for word, freq in frequency.iteritems() if freq > low_n_cutoff}\n",
    "\n",
    "    # if desired, remove high-frequency words (over user-defined SDs above mean) \n",
    "    if high_sd_cutoff == None:\n",
    "        contentWords = [word for word in frequency.keys()] \n",
    "    else:\n",
    "        getOut = np.mean(frequency.values())+(np.std(frequency.values())*(high_sd_cutoff))\n",
    "        contentWords = {word: freq for word, freq in frequency.iteritems() if freq < getOut}.keys()\n",
    "    \n",
    "    # identify the sentences in the file, stripping out words we won't keep\n",
    "    getSentences = [re.sub('[^\\w\\s]+','',str(row)).split(' ') for row in list(data1['lemma'])]\n",
    "    keepSentences = [[word for word in row if word in contentWords] for row in getSentences]\n",
    "    \n",
    "    # build actual semantic space\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    semantic_model = word2vec.Word2Vec(keepSentences, min_count=low_n_cutoff)\n",
    "\n",
    "    # return all the content words and the word2vec model space\n",
    "    return contentWords, semantic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BuildSemanticModel(semantic_model_input_file=INPUT_PATH + \"align_concatenated_dataframe.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate lexical and POS alignment scores for each n-gram length across two comparison vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def LexicalPOSAlignment(tok1,lem1,penn_tok1,penn_lem1,\n",
    "                             tok2,lem2,penn_tok2,penn_lem2,\n",
    "                             stan_tok1=None,stan_lem1=None,\n",
    "                             stan_tok2=None,stan_lem2=None,\n",
    "                             ngramsLength=3,\n",
    "                             ignore_duplicates=True,\n",
    "                             ADD_STANFORD_TAGS = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Derive lexical and part-of-speech alignment scores\n",
    "    between interlocutors (suffix `1` and `2` in arguments\n",
    "    passed to function). \n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `ADD_STANFORD_TAGS=1` and by providing appropriate \n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and \n",
    "    `stan_lem2`.\n",
    "    \n",
    "    By default, consider only bigrams when calculating\n",
    "    similarity. If desired, this window may be expanded \n",
    "    by changing the `ngramsLength` argument value.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty dictionaries for syntactic similarity\n",
    "    cosine_syntax_penn_tok = {}\n",
    "    cosine_syntax_penn_lex = {}\n",
    "    \n",
    "    # if desired, generate Stanford-based scores\n",
    "    if ADD_STANFORD_TAGS == 1:\n",
    "        cosine_syntax_stan_tok = {}\n",
    "        cosine_syntax_stan_lem = {}\n",
    "    \n",
    "    # create empty dictionaries for lexical similarity\n",
    "    cosine_lexical_tok = {}\n",
    "    cosine_lexical_lem = {}\n",
    "    \n",
    "    # cycle through all desired ngram lengths\n",
    "    for ngram in range(2,ngramsLength+1):\n",
    "         \n",
    "        # calculate similarity for lexical ngrams (tokens and lemmas)\n",
    "        [vectorT1, vectorT2] = ngram_lexical(tok1,tok2,ngramsize=ngram)\n",
    "        [vectorL1, vectorL2] = ngram_lexical(lem1,lem2,ngramsize=ngram)\n",
    "        cosine_lexical_tok['cosine_lexical_tok{0}'.format(ngram)] = get_cosine(vectorT1,vectorT2)\n",
    "        cosine_lexical_lem['cosine_lexical_lem{0}'.format(ngram)] = get_cosine(vectorL1, vectorL2)\n",
    "\n",
    "        # calculate similarity for Penn POS ngrams (tokens)\n",
    "        [vector_penn_tok1, vector_penn_tok2] = ngram_pos(penn_tok1,penn_tok2,\n",
    "                                                ngramsize=ngram,\n",
    "                                                ignore_duplicates=ignore_duplicates) \n",
    "        cosine_syntax_penn_tok['cosine_syntax_penn_tok{0}'.format(ngram)] = get_cosine(vector_penn_tok1, \n",
    "                                                                                            vector_penn_tok2)\n",
    "        \n",
    "        # calculate similarity for Penn POS ngrams (lemmas)\n",
    "        [vector_penn_lem1, vector_penn_lem2] = ngram_pos(penn_lem1,penn_lem2,\n",
    "                                                              ngramsize=ngram,\n",
    "                                                              ignore_duplicates=ignore_duplicates) \n",
    "        cosine_syntax_penn_lex['cosine_syntax_penn_lex{0}'.format(ngram)] = get_cosine(vector_penn_lem1, \n",
    "                                                                                            vector_penn_lem2) \n",
    "\n",
    "        # if desired, also calculate using Stanford POS\n",
    "        if ADD_STANFORD_TAGS == 1:         \n",
    "          \n",
    "            # calculate similarity for Stanford POS ngrams (tokens)\n",
    "            [vector_stan_tok1, vector_stan_tok2] = ngram_pos(stan_tok1,stan_tok2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates) \n",
    "            cosine_syntax_stan_tok['cosine_syntax_stan_tok{0}'.format(ngram)] = get_cosine(vector_stan_tok1,\n",
    "                                                                                                vector_stan_tok2)\n",
    "\n",
    "            # calculate similarity for Stanford POS ngrams (lemmas)\n",
    "            [vector_stan_lem1, vector_stan_lem2] = ngram_pos(stan_lem1,stan_lem2,\n",
    "                                                                  ngramsize=ngram,\n",
    "                                                                  ignore_duplicates=ignore_duplicates) \n",
    "            cosine_syntax_stan_lem['cosine_syntax_stan_lem{0}'.format(ngram)] = get_cosine(vector_stan_lem1,\n",
    "                                                                                                vector_stan_lem2)\n",
    "        \n",
    "    # return requested information\n",
    "    if ADD_STANFORD_TAGS==1:\n",
    "        dictionaries_list = [cosine_syntax_penn_tok, cosine_syntax_penn_lex,\n",
    "                             cosine_syntax_stan_tok, cosine_syntax_stan_lem, \n",
    "                             cosine_lexical_tok, cosine_lexical_lem]      \n",
    "    else:\n",
    "        dictionaries_list = [cosine_syntax_penn_tok, cosine_syntax_penn_lex,\n",
    "                             cosine_lexical_tok, cosine_lexical_lem]      \n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate turn-level analysis of alignment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conceptualAlignment(lem1, lem2, vocablist, highDimModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate conceptual alignment scores from list of lemmas\n",
    "    from between two interocutors (suffix `1` and `2` in arguments\n",
    "    passed to function) using `word2vec`.\n",
    "    \"\"\"\n",
    "\n",
    "    # aggregate composite high-dimensional vectors of all words in utterance\n",
    "    W2Vec1 = build_composite_semantic_vector(lem1,vocablist,highDimModel)\n",
    "    W2Vec2 = build_composite_semantic_vector(lem2,vocablist,highDimModel)\n",
    "\n",
    "    # return cosine distance alignment score\n",
    "    return 1 - spatial.distance.cosine(W2Vec1, W2Vec2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def returnMultilevelAlignment(cond_info,\n",
    "                                   partnerA,tok1,lem1,penn_tok1,penn_lem1,\n",
    "                                   partnerB,tok2,lem2,penn_tok2,penn_lem2,\n",
    "                                   vocablist, highDimModel, \n",
    "                                   stan_tok1=None,stan_lem1=None,\n",
    "                                   stan_tok2=None,stan_lem2=None,\n",
    "                                   ADD_STANFORD_TAGS=0,\n",
    "                                   ngramsLength=3, \n",
    "                                   ignore_duplicates=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between a pair of turns by individual interlocutors \n",
    "    (suffix `1` and `2` in arguments passed to function), \n",
    "    including leading/following comparison directionality.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `ADD_STANFORD_TAGS=1` and by providing appropriate \n",
    "    values for `stan_tok1`, `stan_lem1`, `stan_tok2`, and \n",
    "    `stan_lem2`.\n",
    "    \n",
    "    By default, consider only bigrams when calculating\n",
    "    similarity. If desired, this window may be expanded \n",
    "    by changing the `ngramsLength` argument value.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create empty dictionaries \n",
    "    partner_direction = {}\n",
    "    condition_info = {}\n",
    "    cosine_semanticL = {}\n",
    "    \n",
    "    # calculate lexical and syntactic alignment\n",
    "    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 ngramsLength=ngramsLength,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 ADD_STANFORD_TAGS=ADD_STANFORD_TAGS)\n",
    "    \n",
    "    # calculate conceptual alignment\n",
    "    cosine_semanticL['cosine_semanticL'] = conceptualAlignment(lem1,lem2,vocablist,highDimModel)\n",
    "    dictionaries_list.append(cosine_semanticL.copy())\n",
    "    \n",
    "    # determine directionality of leading/following comparison\n",
    "    partner_direction['partner_direction'] = str(partnerA) + \">\" + str(partnerB)\n",
    "    dictionaries_list.append(partner_direction.copy())\n",
    "\n",
    "    # add condition information\n",
    "    condition_info['condition_info'] = cond_info    \n",
    "    dictionaries_list.append(condition_info.copy())\n",
    "\n",
    "    # return alignment scores\n",
    "    return dictionaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def TurnByTurnAnalysis(dataframe,\n",
    "                            vocablist,\n",
    "                            highDimModel, \n",
    "                            delay=1,\n",
    "                            maxngram = 3,\n",
    "                            ADD_STANFORD_TAGS=0,\n",
    "                            ignore_duplicates=True):    \n",
    "\n",
    "    \"\"\"\n",
    "    Calculate lexical, syntactic, and conceptual alignment\n",
    "    between interlocutors over an entire conversation.\n",
    "    Automatically detect individual speakers by unique\n",
    "    speaker codes.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 4. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `ADD_STANFORD_TAGS=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if we don't want the Stanford tagger data, set defaults\n",
    "    if ADD_STANFORD_TAGS == 0:\n",
    "        stan_tok1=None\n",
    "        stan_lem1=None\n",
    "        stan_tok2=None\n",
    "        stan_lem2=None\n",
    "    \n",
    "    # prepare the data to the appropriate type\n",
    "    \n",
    "    print dataframe['token']\n",
    "    \n",
    "    dataframe['token'] = dataframe['token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))    \n",
    "    dataframe['lemma'] = dataframe['lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_token'] = dataframe['tagged_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "    dataframe['tagged_lemma'] = dataframe['tagged_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "    \n",
    "    # if desired, prepare the Stanford tagger data\n",
    "    if ADD_STANFORD_TAGS == 1:           \n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_token'] = dataframe['tagged_stan_token'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: re.sub('[^\\w\\s]+','',x).split(' '))\n",
    "        dataframe['tagged_stan_lemma'] = dataframe['tagged_stan_lemma'].apply(lambda x: zip(x[0::2],x[1::2])) # thanks to https://stackoverflow.com/a/4647086\n",
    "\n",
    "    # create lagged version of the dataframe\n",
    "    df_original = dataframe.drop(dataframe.tail(delay).index,inplace=False)\n",
    "    df_lagged = dataframe.shift(-delay).drop(dataframe.tail(delay).index,inplace=False)\n",
    "    \n",
    "    # cycle through each pair of turns\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    for i in range(0,df_original.shape[0]):\n",
    "\n",
    "        # identify the condition for this dataframe\n",
    "        cond_info = dataframe['file'].unique()\n",
    "        if len(cond_info)==1: \n",
    "            cond_info = str(cond_info[0])\n",
    "        \n",
    "        # break and flag error if we have more than 1 condition per dataframe\n",
    "        else: \n",
    "            raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "\n",
    "        # grab all of first participant's data\n",
    "        first_row = df_original.iloc[i]\n",
    "        first_partner = first_row['participant']\n",
    "        # text1=first_row['content']\n",
    "        tok1=first_row['token']\n",
    "        lem1=first_row['lemma']\n",
    "        penn_tok1=first_row['tagged_token']\n",
    "        penn_lem1=first_row['tagged_lemma']\n",
    "\n",
    "        # grab all of lagged participant's data\n",
    "        lagged_row = df_lagged.iloc[i]\n",
    "        lagged_partner = lagged_row['participant']\n",
    "        # text2=lagged_row['content']\n",
    "        tok2=lagged_row['token']\n",
    "        lem2=lagged_row['lemma']\n",
    "        penn_tok2=lagged_row['tagged_token']\n",
    "        penn_lem2=lagged_row['tagged_lemma']\n",
    "        \n",
    "        # if desired, grab the Stanford tagger data for both participants\n",
    "        if ADD_STANFORD_TAGS == 1:           \n",
    "            stan_tok1=first_row['tagged_stan_token']\n",
    "            stan_lem1=first_row['tagged_stan_lemma']\n",
    "            stan_tok2=lagged_row['tagged_stan_token']\n",
    "            stan_lem2=lagged_row['tagged_stan_lemma']\n",
    "                \n",
    "        # process multilevel alignment\n",
    "        dictionaries_list=returnMultilevelAlignment(cond_info=cond_info,\n",
    "                                                         partnerA=first_partner,\n",
    "                                                         # text1=text1,\n",
    "                                                         tok1=tok1,lem1=lem1,\n",
    "                                                         penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                         partnerB=lagged_partner,\n",
    "                                                         # text2=text2,\n",
    "                                                         tok2=tok2,lem2=lem2,\n",
    "                                                         penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                         vocablist=vocablist,\n",
    "                                                         highDimModel=highDimModel,\n",
    "                                                         stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                         stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                         ngramsLength = maxngram,\n",
    "                                                         ignore_duplicates = ignore_duplicates,\n",
    "                                                         ADD_STANFORD_TAGS = ADD_STANFORD_TAGS) \n",
    "        \n",
    "        # append data to existing structures\n",
    "        next_df_line = pd.DataFrame.from_dict(dict(j for i in dictionaries_list for j in i.items()),\n",
    "                               orient='index').transpose()\n",
    "        aggregated_df = aggregated_df.append(next_df_line)\n",
    "            \n",
    "    # reformat turn information and add index\n",
    "    aggregated_df = aggregated_df.reset_index(drop=True).reset_index().rename(columns={\"index\":\"time\"})\n",
    "\n",
    "    # give us our finished dataframe\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate conversation-level analysis of alignment scores\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ConvoByConvoAnalysis(dataframe,\n",
    "                          ngramsLength = 3,\n",
    "                          ignore_duplicates=True,\n",
    "                          ADD_STANFORD_TAGS = 0):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate analysis of multilevel similarity over\n",
    "    a conversation between two interlocutors from a \n",
    "    transcript dataframe prepared by Phase 1\n",
    "    of ALIGN. Automatically detect speakers by unique\n",
    "    speaker codes.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 2. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `ADD_STANFORD_TAGS=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "\n",
    "    # identify the condition for this dataframe\n",
    "    cond_info = dataframe['file'].unique()\n",
    "    if len(cond_info)==1: \n",
    "        cond_info = str(cond_info[0])\n",
    "    \n",
    "    # break and flag error if we have more than 1 condition per dataframe\n",
    "    else: \n",
    "        raise ValueError('Error! Dataframe contains multiple conditions. Split dataframe into multiple dataframes, one per condition: '+cond_info)\n",
    "   \n",
    "    # if we don't want the Stanford info, set defaults \n",
    "    if ADD_STANFORD_TAGS==0:\n",
    "        stan_tok1 = None\n",
    "        stan_lem1 = None\n",
    "        stan_tok2 = None\n",
    "        stan_lem2 = None\n",
    "\n",
    "    # identify individual interlocutors\n",
    "    df_A = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[0]]\n",
    "    df_B = dataframe.loc[dataframe['participant'] == dataframe['participant'].unique()[1]]\n",
    "   \n",
    "    # concatenate the token, lemma, and POS information for participant A\n",
    "    tok1 = [word for turn in df_A['token'] for word in turn]\n",
    "    lem1 = [word for turn in df_A['lemma'] for word in turn]\n",
    "    penn_tok1 = [POS for turn in df_A['tagged_token'] for POS in turn]    \n",
    "    penn_lem1 = [POS for turn in df_A['tagged_token'] for POS in turn] \n",
    "    if ADD_STANFORD_TAGS == 1:\n",
    "        stan_tok1 = [POS for turn in df_A['tagged_stan_token'] for POS in turn]    \n",
    "        stan_lem21 = [POS for turn in df_A['tagged_stan_lemma'] for POS in turn] \n",
    "\n",
    "    # concatenate the token, lemma, and POS information for participant B\n",
    "    tok2 = [word for turn in df_B['token'] for word in turn]\n",
    "    lem2 = [word for turn in df_B['lemma'] for word in turn]\n",
    "    penn_tok2 = [POS for turn in df_B['tagged_token'] for POS in turn]    \n",
    "    penn_lem2 = [POS for turn in df_B['tagged_token'] for POS in turn] \n",
    "    if ADD_STANFORD_TAGS == 1:\n",
    "        stan_tok2 = [POS for turn in df_B['tagged_stan_token'] for POS in turn]    \n",
    "        stan_lem2 = [POS for turn in df_B['tagged_stan_lemma'] for POS in turn] \n",
    "        \n",
    "    # process multilevel alignment\n",
    "    dictionaries_list = LexicalPOSAlignment(tok1=tok1,lem1=lem1,\n",
    "                                                 penn_tok1=penn_tok1,penn_lem1=penn_lem1,\n",
    "                                                 tok2=tok2,lem2=lem2,\n",
    "                                                 penn_tok2=penn_tok2,penn_lem2=penn_lem2,\n",
    "                                                 stan_tok1=stan_tok1,stan_lem1=stan_lem1,\n",
    "                                                 stan_tok2=stan_tok2,stan_lem2=stan_lem2,\n",
    "                                                 ngramsLength=ngramsLength,\n",
    "                                                 ignore_duplicates=ignore_duplicates,\n",
    "                                                 ADD_STANFORD_TAGS=ADD_STANFORD_TAGS)\n",
    "    \n",
    "    # append data to existing structures\n",
    "    dictionary_df = pd.DataFrame.from_dict(dict(j for i in dictionaries_list for j in i.items()),\n",
    "                           orient='index').transpose()\n",
    "    dictionary_df['condition_info'] = cond_info\n",
    "            \n",
    "    # return the dataframe\n",
    "    return dictionary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RUN Phase 2: Actual Partners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* For each prepped transcript file, runs turn-level and conversational-level alignment scores\n",
    "* Saves output into single datasheet to be used in statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def PHASE2RUN_REAL(input_file_directory, \n",
    "                        output_file_directory,\n",
    "                        semantic_model_input_file,\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1,\n",
    "                        delay=1,\n",
    "                        maxngram=3,\n",
    "                        ignore_duplicates=True,\n",
    "                        ADD_STANFORD_TAGS=0):   \n",
    "\n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `PHASE1RUN` \n",
    "    preparation stage, return multi-level alignment \n",
    "    scores with turn-by-turn and conversation-level metrics.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    high-frequency cutoff of 3 SD over the mean. If \n",
    "    desired, this can be changed with the \n",
    "    `high_sd_cutoff` argument and can be removed with\n",
    "    `high_sd_cutoff=None`.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    low-frequency cutoff in which a word will be \n",
    "    removed if they occur 1 or fewer times. if\n",
    "    desired, this can be changed with the \n",
    "    `low_n_cutoff` argument and can be removed with\n",
    "    `low_n_cutoff=0`.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 4. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `ADD_STANFORD_TAGS=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the files in the list\n",
    "    file_list = glob.glob(input_file_directory+\"*.txt\")\n",
    "    \n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                        high_sd_cutoff=high_sd_cutoff,\n",
    "                                                        low_n_cutoff=low_n_cutoff)\n",
    "\n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "    \n",
    "    # cycle through each prepared file\n",
    "    for fileName in file_list:\n",
    "        \n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 0:\n",
    "            \n",
    "            # let us know which filename we're processing\n",
    "            print \"Processing: \"+fileName   \n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "            \n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             ngramsLength = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             ADD_STANFORD_TAGS = ADD_STANFORD_TAGS)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "        \n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print \"Invalid file: \"+fileName   \n",
    "            \n",
    "    # update final dataframes\n",
    "    FINAL_TURN = AlignmentT2T.reset_index(drop=True)\n",
    "    FINAL_CONVO = AlignmentC2C.reset_index(drop=True)\n",
    "    \n",
    "    # export the final files\n",
    "    FINAL_TURN.to_csv(output_file_directory+\"AlignmentT2T.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')   \n",
    "    FINAL_CONVO.to_csv(output_file_directory+\"AlignmentC2C.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t') \n",
    "\n",
    "    # display the info, too\n",
    "    return FINAL_TURN, FINAL_CONVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PHASE2RUN_REAL(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "#                         output_file_directory = INPUT_PATH+ANALYSIS_READY,\n",
    "#                         semantic_model_input_file = INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "#                         high_sd_cutoff=3,\n",
    "#                         low_n_cutoff=1,\n",
    "#                         delay=1,\n",
    "#                         maxngram=3,\n",
    "#                         ignore_duplicates=True,\n",
    "#                         ADD_STANFORD_TAGS=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Generate surrogate pairings\n",
    "-------------------------\n",
    "* Collects all possible pairs of participants across the dyads in each condition and creates surrogate pairings by combining their conversational turns, preserving turn order. Output saved as new separate conversational transcripts. \n",
    "* Main Function:\n",
    "    * GenerateSurrogate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def GenerateSurrogate(original_conversation_list,\n",
    "                           surrogate_file_directory,\n",
    "                           all_surrogates = False,\n",
    "                           id_separator = '\\-',\n",
    "                           dyad_label='dyad',\n",
    "                           condition_label='cond',\n",
    "                           keep_original_turn_order = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create transcripts for surrogate pairs of \n",
    "    participants (i.e., participants who did not \n",
    "    genuinely interact in the experiment), which\n",
    "    will later be used to generate baseline levels \n",
    "    of alignment. Store surrogate files in a new\n",
    "    folder each time the surrogate generation is run.\n",
    "    \n",
    "    Returns a list of all surrogate files created.\n",
    "\n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID is a hyphen ('\\-'). If desired,\n",
    "    this may be changed in the `id_separator` \n",
    "    argument.\n",
    "\n",
    "    By default, condition IDs will be identified as \n",
    "    any characters following `cond`. If desired,\n",
    "    this may be changed with the `condition_label`\n",
    "    argument.\n",
    "    \n",
    "    By default, dyad IDs will be identified as \n",
    "    any characters following `dyad`. If desired,\n",
    "    this may be changed with the `dyad_label`\n",
    "    argument.\n",
    "    \n",
    "    By default, generate surrogates only from a subset\n",
    "    of all possible pairings. If desired, instead \n",
    "    generate surrogates from all possible pairings\n",
    "    with `all_surrogates=True`.\n",
    "    \n",
    "    By default, create surrogates by shuffling all\n",
    "    turns within each surrogate partner's data. If \n",
    "    desired, retain the original ordering of each\n",
    "    surrogate partner's data with \n",
    "    `keep_original_turn_order = True`.\n",
    "    \"\"\"\n",
    "        \n",
    "    # create a subfolder for the new set of surrogates\n",
    "    import time\n",
    "    new_surrogate_path = surrogate_file_directory + 'surrogate_run-' + str(time.time()) +'/'\n",
    "    if not os.path.exists(new_surrogate_path):\n",
    "        os.makedirs(new_surrogate_path)\n",
    "        \n",
    "    # grab condition types from each file name\n",
    "    file_info = [re.sub('\\.txt','',os.path.basename(file_name)) for file_name in original_conversation_list]\n",
    "    condition_ids = list(set([re.findall('[^'+id_separator+']*'+condition_label+'.*',metadata)[0] for metadata in file_info]))\n",
    "    files_conditions = {}\n",
    "    for unique_condition in condition_ids:\n",
    "        next_condition_files = [add_file for add_file in original_conversation_list if unique_condition in add_file]\n",
    "        files_conditions[unique_condition] = next_condition_files\n",
    "    \n",
    "    # cycle through conditions\n",
    "    for condition in files_conditions.keys():\n",
    "        \n",
    "        # grab all possible pairs of conversations of this condition\n",
    "        paired_surrogates = [pair for pair in combinations(files_conditions[condition],2)]\n",
    "        \n",
    "        # default: randomly pull from all pairs to get target surrogate sample\n",
    "        if all_surrogates == False:\n",
    "            import math\n",
    "            paired_surrogates = random.sample(paired_surrogates, \n",
    "                                              int(math.ceil(len(files_conditions[condition])/2)))\n",
    "            \n",
    "        # cycle through surrogate pairings\n",
    "        for next_surrogate in paired_surrogates:\n",
    "            \n",
    "            # read in the files\n",
    "            original_file1 = os.path.basename(next_surrogate[0])\n",
    "            original_file2 = os.path.basename(next_surrogate[1])\n",
    "            original_df1=pd.read_csv(next_surrogate[0], sep='\\t',encoding='utf-8')\n",
    "            original_df2=pd.read_csv(next_surrogate[1], sep='\\t',encoding='utf-8')\n",
    "            \n",
    "            # get participants A and B from df1\n",
    "            participantA_1_code = min(original_df1['participant'].unique())\n",
    "            participantB_1_code = max(original_df1['participant'].unique())\n",
    "            participantA_1 = original_df1[original_df1['participant'] == participantA_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_1 = original_df1[original_df1['participant'] == participantB_1_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            \n",
    "            # get participants A and B from df2\n",
    "            participantA_2_code = min(original_df2['participant'].unique())\n",
    "            participantB_2_code = max(original_df2['participant'].unique())\n",
    "            participantA_2 = original_df2[original_df2['participant'] == participantA_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            participantB_2 = original_df2[original_df2['participant'] == participantB_2_code].reset_index().rename(columns={'file': 'original_file'})\n",
    "            \n",
    "            # identify truncation point for both surrogates (to have even number of turns)\n",
    "            surrogateX_turns=min([participantA_1.shape[0],\n",
    "                                  participantB_2.shape[0]])\n",
    "            surrogateY_turns=min([participantA_2.shape[0],\n",
    "                                  participantB_1.shape[0]])\n",
    "            \n",
    "            # if desired, preserve original turn order for surrogate pairs\n",
    "            if keep_original_turn_order == True:\n",
    "                surrogateX = participantA_1.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "                                participantB_2.truncate(after=surrogateX_turns-1,copy=False)).sort(\n",
    "                                ['index']).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "                surrogateY = participantA_2.truncate(after=surrogateX_turns-1,copy=False).append(\n",
    "                                participantB_1.truncate(after=surrogateX_turns-1,copy=False)).sort(\n",
    "                                ['index']).reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "            \n",
    "            # otherwise, just shuffle all turns within participants\n",
    "            else:\n",
    "                \n",
    "                # shuffle for first surrogate pairing\n",
    "                surrogateX_A1 = participantA_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX_B2 = participantB_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateX = surrogateX_A1.append(surrogateX_B2).sort_index().reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "                \n",
    "                # and for second surrogate pairing\n",
    "                surrogateY_A2 = participantA_2.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY_B1 = participantB_1.truncate(after=surrogateX_turns-1,copy=False).sample(frac=1).reset_index(drop=True)\n",
    "                surrogateY = surrogateY_A2.append(surrogateY_B1).sort_index().reset_index(drop=True).rename(columns={'index': 'original_index'})\n",
    "\n",
    "            # create filename for our surrogate file\n",
    "            original_dyad1 = re.findall(dyad_label+'[^'+id_separator+']*',original_file1)[0]\n",
    "            original_dyad2 = re.findall(dyad_label+'[^'+id_separator+']*',original_file2)[0]\n",
    "            surrogateX['file'] = condition + '-' + original_dyad1 + '-' + original_dyad2\n",
    "            surrogateY['file'] = condition + '-' + original_dyad1 + '-' + original_dyad2\n",
    "            nameX='SurrogatePair-'+original_dyad1+'A'+'-'+original_dyad2+'B'+'-'+condition+'.txt'\n",
    "            nameY='SurrogatePair-'+original_dyad2+'A'+'-'+original_dyad1+'B'+'-'+condition+'.txt'\n",
    "            \n",
    "            # save to file\n",
    "            surrogateX.to_csv(new_surrogate_path + nameX, encoding='utf-8',index=False,sep='\\t')\n",
    "            surrogateY.to_csv(new_surrogate_path + nameY, encoding='utf-8',index=False,sep='\\t')\n",
    "            \n",
    "    # return list of all surrogate files\n",
    "    return glob.glob(new_surrogate_path+\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "RUN Phase 2: Surrogate Partners\n",
    "-------------------------------\n",
    "* Runs function to generate new surrogate transcript conversations (separate files)\n",
    "* For each surrogate transcript file, runs turn-level and conversational-level alignment scores\n",
    "* Saves output into single datasheet to be used in statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def PHASE2RUN_SURROGATE(input_file_directory, \n",
    "                             surrogate_file_directory,\n",
    "                             output_file_directory,\n",
    "                             semantic_model_input_file,\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=False,\n",
    "                             keep_original_turn_order = False,\n",
    "                             delay=1,\n",
    "                             maxngram=3,\n",
    "                             ignore_duplicates=True,\n",
    "                             ADD_STANFORD_TAGS=0):   \n",
    "    \"\"\"\n",
    "    Given a directory of individual .txt files and the\n",
    "    vocab list that have been generated by the `PHASE1RUN` \n",
    "    preparation stage, return multi-level alignment \n",
    "    scores with turn-by-turn and conversation-level metrics\n",
    "    for surrogate baseline conversations.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    high-frequency cutoff of 3 SD over the mean. If \n",
    "    desired, this can be changed with the \n",
    "    `high_sd_cutoff` argument and can be removed with\n",
    "    `high_sd_cutoff=None`.\n",
    "    \n",
    "    By default, create the semantic model with a \n",
    "    low-frequency cutoff in which a word will be \n",
    "    removed if they occur 1 or fewer times. if\n",
    "    desired, this can be changed with the \n",
    "    `low_n_cutoff` argument and can be removed with\n",
    "    `low_n_cutoff=0`.\n",
    "    \n",
    "    By default, compare only adjacent turns. If desired,\n",
    "    the comparison distance may be changed by increasing\n",
    "    the `delay` argument.\n",
    "    \n",
    "    By default, include maximum n-gram comparison of 4. If\n",
    "    desired, this may be changed by passing the appropriate\n",
    "    value to the the `maxngram` argument.\n",
    "    \n",
    "    By default, return scores based only on Penn POS taggers. \n",
    "    If desired, also return scores using Stanford tagger with \n",
    "    `ADD_STANFORD_TAGS=1`.\n",
    "    \n",
    "    By default, remove exact duplicates when calculating POS\n",
    "    similarity scores (i.e., does not consider perfectly\n",
    "    mimicked lexical items between speakers). If desired, \n",
    "    duplicates may be included when calculating scores by \n",
    "    passing `ignore_duplicates=False`.\n",
    "    \n",
    "    By default, the separator between dyad ID and\n",
    "    condition ID in each file name is a hyphen ('\\-'). \n",
    "    If desired, this may be changed with the \n",
    "    `id_separator` argument.\n",
    "\n",
    "    By default, condition IDs in each file name\n",
    "    will be identified as any characters following \n",
    "    `cond`. If desired, this may be changed with the \n",
    "    `condition_label` argument.\n",
    "    \n",
    "    By default, dyad IDs in each file name\n",
    "    will be identified as any characters following \n",
    "    `dyad`. If desired, this may be changed with the \n",
    "    `dyad_label` argument.\n",
    "    \n",
    "    By default, generate surrogates only from a subset\n",
    "    of all possible pairings. If desired, instead \n",
    "    generate surrogates from all possible pairings\n",
    "    with `all_surrogates=True`\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the files in the input list\n",
    "    file_list = glob.glob(input_file_directory+\"*.txt\")\n",
    "    surrogate_file_list = GenerateSurrogate(original_conversation_list = file_list,\n",
    "                                                   surrogate_file_directory = surrogate_file_directory,\n",
    "                                                   all_surrogates = all_surrogates,\n",
    "                                                   id_separator = id_separator,\n",
    "                                                   condition_label = condition_label,\n",
    "                                                   dyad_label = dyad_label,\n",
    "                                                   keep_original_turn_order = keep_original_turn_order) \n",
    "    \n",
    "    # build the semantic model to be used for all conversations\n",
    "    [vocablist, highDimModel] = BuildSemanticModel(semantic_model_input_file=semantic_model_input_file,\n",
    "                                                        high_sd_cutoff=high_sd_cutoff,\n",
    "                                                        low_n_cutoff=low_n_cutoff)\n",
    "    \n",
    "    # create containers for alignment values\n",
    "    AlignmentT2T = pd.DataFrame()\n",
    "    AlignmentC2C = pd.DataFrame()\n",
    "    \n",
    "    # cycle through the files\n",
    "    for fileName in surrogate_file_list:\n",
    "        \n",
    "        # process the file if it's got a valid conversation\n",
    "        dataframe=pd.read_csv(fileName, sep='\\t',encoding='utf-8')\n",
    "        if len(dataframe) > 0:\n",
    "            \n",
    "            # let us know which filename we're processing\n",
    "            print \"Processing: \"+fileName   \n",
    "\n",
    "            # calculate turn-by-turn alignment scores\n",
    "            xT2T=TurnByTurnAnalysis(dataframe=dataframe,\n",
    "                                         delay=delay,\n",
    "                                         maxngram=maxngram,\n",
    "                                         vocablist=vocablist,\n",
    "                                         highDimModel=highDimModel)\n",
    "            AlignmentT2T=AlignmentT2T.append(xT2T)\n",
    "            \n",
    "            # calculate conversation-level alignment scores\n",
    "            xC2C = ConvoByConvoAnalysis(dataframe=dataframe,\n",
    "                                             ngramsLength = maxngram,\n",
    "                                             ignore_duplicates=ignore_duplicates,\n",
    "                                             ADD_STANFORD_TAGS = ADD_STANFORD_TAGS)\n",
    "            AlignmentC2C=AlignmentC2C.append(xC2C)\n",
    "        \n",
    "        # if it's invalid, let us know\n",
    "        else:\n",
    "            print \"Invalid file: \"+fileName   \n",
    "            \n",
    "    # update final dataframes\n",
    "    FINAL_TURN_SURROGATE = AlignmentT2T.reset_index(drop=True)\n",
    "    FINAL_CONVO_SURROGATE = AlignmentC2C.reset_index(drop=True)\n",
    "    \n",
    "    # export the final files\n",
    "    FINAL_TURN_SURROGATE.to_csv(output_file_directory+\"AlignmentT2T_Surrogate.txt\",\n",
    "                      encoding='utf-8',index=False,sep='\\t')   \n",
    "    FINAL_CONVO_SURROGATE.to_csv(output_file_directory+\"AlignmentC2C_Surrogate.txt\",\n",
    "                       encoding='utf-8',index=False,sep='\\t') \n",
    "\n",
    "    # display the info, too\n",
    "    return FINAL_TURN_SURROGATE, FINAL_CONVO_SURROGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ANALYSIS_READY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-26064bc7dce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m [turn_surrogate,convo_surrogate] = PHASE2RUN_SURROGATE(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n\u001b[1;32m      2\u001b[0m                              \u001b[0msurrogate_file_directory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mINPUT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mSURROGATE_TRANSCRIPTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                              \u001b[0moutput_file_directory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mINPUT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mANALYSIS_READY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                              \u001b[0msemantic_model_input_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINPUT_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'align_concatenated_dataframe.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mhigh_sd_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ANALYSIS_READY' is not defined"
     ]
    }
   ],
   "source": [
    "[turn_surrogate,convo_surrogate] = PHASE2RUN_SURROGATE(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                             surrogate_file_directory= INPUT_PATH+SURROGATE_TRANSCRIPTS,\n",
    "                             output_file_directory= INPUT_PATH+ANALYSIS_READY,\n",
    "                             semantic_model_input_file=INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=False,\n",
    "                             keep_original_turn_order = False,\n",
    "                             delay=1,\n",
    "                             maxngram=3,\n",
    "                             ignore_duplicates=True,\n",
    "                             ADD_STANFORD_TAGS=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Run everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 1: Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_phase1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model_store = PHASE1RUN(input_file_directory=INPUT_PATH+TRANSCRIPTS,\n",
    "#                       output_file_directory=INPUT_PATH+PREPPED_TRANSCRIPTS,\n",
    "#                       training_dictionary=INPUT_PATH+'big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 2: Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/41_1-1-3.txt\n",
      "0                                [that, it, should, be]\n",
      "1     [that, it, should, be, okay, why, did, you, sa...\n",
      "2     [because, the, reason, why, the, politicians, ...\n",
      "3     [okay, i, though, it, was, more, of, a, someth...\n",
      "4     [so, your, stance, is, that, it, should, not, ...\n",
      "5     [they, should, not, be, taxed, as, much, becau...\n",
      "6      [that, is, a, small, popularity, most, of, them]\n",
      "7     [but, say, if, they, open, their, business, or...\n",
      "8     [but, a, lot, of, the, rich, are, not, being, ...\n",
      "9     [i, have, not, heard, it, so, much, but, if, t...\n",
      "10    [that, is, basically, like, the, working, clas...\n",
      "11    [i, think, also, we, can, look, at, people, li...\n",
      "12    [that, is, not, the, the, rich, people, like, ...\n",
      "13    [i, mean, i, am, just, high, lighting, some, e...\n",
      "14    [well, i, understand, people, who, are, rich, ...\n",
      "15    [but, then, you, can, also, think, of, in, the...\n",
      "16    [i, am, trying, to, incorporate, a, movie, dra...\n",
      "17    [i, mean, yeah, that, is, always, a, big, deba...\n",
      "18    [because, the, republicans, they, were, dealin...\n",
      "19    [no, i, did, not, watch, it, that, day, which,...\n",
      "20    [topic, and, it, was, just, a, bit, complex, f...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/16_2-1-2.txt\n",
      "0     [ok, i, am, really, against, abortion, complet...\n",
      "1     [so, an, argument, could, be, like, one, no, m...\n",
      "2     [it, is, still, alive, i, can, not, think, of,...\n",
      "3     [like, even, even, after, like, a, few, weeks,...\n",
      "4      [it, is, still, a, person, it, is, not, just, a]\n",
      "5     [like, yeah, it, is, not, there, is, not, like...\n",
      "6     [there, is, the, chances, that, most, do, need...\n",
      "7     [i, think, that, it, can, like, hurt, it, can,...\n",
      "8     [i, also, think, because, it, is, a, blind, pr...\n",
      "9     [we, have, it, is, live, no, matter, what, it,...\n",
      "10    [well, i, mean, they, could, be, the, great, p...\n",
      "11    [well, we, could, think, about, like, what, wo...\n",
      "12    [that, is, true, a, counterargument, is, a, wo...\n",
      "13    [so, it, is, a, woman, 's, choice, but, it, is...\n",
      "14                [they, do, not, have, a, say, in, it]\n",
      "15        [yeah, the, baby, does, not, have, a, choice]\n",
      "16    [it, up, i, know, people, get, abortion, becau...\n",
      "17    [there, is, been, cases, where, they, think, t...\n",
      "18    [so, they, adopted, a, perfectly, healthy, bab...\n",
      "19    [an, argument, is, that, like, it, is, not, li...\n",
      "20    [well, but, it, but, it, yeah, and, i, think, ...\n",
      "21                                  [the, genetic, one]\n",
      "22           [the, genetic, place, that, we, were, the]\n",
      "23    [what, was, the, first, one, the, choice, yeah...\n",
      "24    [yeah, would, that, be, a, counter, the, to, a...\n",
      "25    [but, at, the, same, time, i, would, think, i,...\n",
      "26    [yeah, i, get, what, you, are, saying, it, is,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/9_1-0-3.txt\n",
      "0                                     [i, will, start]\n",
      "1                                    [yeah, go, ahead]\n",
      "2    [i, do, not, i, do, not, think, i, think, the,...\n",
      "3    [well, we, have, been, in, the, middle, east, ...\n",
      "4    [yeah, now, i, think, it, was, the, right, thi...\n",
      "5    [well, when, whenever, you, just, go, into, a,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/23_2-1-3.txt\n",
      "0           [so, what, is, your, opinion, on, abortion]\n",
      "1     [abortion, i, think, it, is, wrong, because, o...\n",
      "2     [yeah, i, think, that, is, true, because, it, ...\n",
      "3     [to, help, yeah, another, yeah, to, take, care...\n",
      "4     [yeah, so, even, if, something, unfortunate, h...\n",
      "5     [yeah, yeah, then, you, do, notwant, yeah, yea...\n",
      "6     [yeah, yeah, that, is, true, yeah, so, you, ca...\n",
      "7     [is, there, states, that, are, legal, like, le...\n",
      "8     [i, am, not, sure, is, california, legalized, ...\n",
      "9     [i, do, not, know, religions, is, very, agains...\n",
      "10                                        [i, thin, so]\n",
      "11           [yeah, i, am, not, sure, i, do, not, know]\n",
      "12    [because, you, know, okay, so, people, been, a...\n",
      "13    [yeah, that, is, true, yeah, it, is, like, yea...\n",
      "14    [really, i, thought, i, thought, i, thought, l...\n",
      "15    [but, then, yeah, after, hut, yeah, yeah, yeah...\n",
      "16    [no, no, it, is, not, actually, i, think, it, ...\n",
      "17    [oh, okay, so, it, is, not, considered, as, ab...\n",
      "18    [no, no, yeah, i, know, my, aunt, had, acciden...\n",
      "19    [yeah, that, is, happened, to, my, cousin, i, ...\n",
      "20    [so, i, think, it, is, it, is, pretty, bad, to...\n",
      "21    [there, is, like, a, lot, of, people, believe,...\n",
      "22    [actually, a, lot, of, people, do, not, think,...\n",
      "23    [okay, until, like, it, happens, to, them, or,...\n",
      "24    [maybe, maybe, those, people, who, are, agains...\n",
      "25    [abortion, is, bad, yeah, ha, yeah, i, do, not...\n",
      "26    [i, know, my, culture, does, not, accept, abor...\n",
      "27    [yeah, mine, either, i, do, not, think, it, do...\n",
      "28    [there, are, people, who, try, to, do, like, u...\n",
      "29    [mint, really, that, is, weird, i, did, not, k...\n",
      "30    [yeah, it, is, very, dangerous, but, i, guess,...\n",
      "31           [instead, of, like, having, a, baby, yeah]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/5_2-0-2.txt\n",
      "0                                 [so, are, we, on, na]\n",
      "1      [so, what, are, our, opinions, on, this, should]\n",
      "2     [i, feel, as, if, they, are, on, na, come, at,...\n",
      "3     [taxed, the, same, yea, in, all, honesty, i, d...\n",
      "4                                    [i, firmly, agree]\n",
      "5     [i, i, despise, all, notions, of, socialism, e...\n",
      "6     [so, far, for, the, i, think, we, should, have...\n",
      "7     [it, is, not, mine, either, i, do, not, know, ...\n",
      "8     [ok, i, can, i, can, you, on, that, i, did, a,...\n",
      "9     [i, would, not, call, it, a, right, but, i, wo...\n",
      "10    [right, i, agree, and, then, up, pharmacy, ok,...\n",
      "11    [a, third, reason, to, vote, to, sort, of, try...\n",
      "12    [not, for, socialism, but, for, well, if, you,...\n",
      "13    [or, a, the, the, rate, of, rich, people, gett...\n",
      "14    [that, realm, of, i, understand, yeah, it, is,...\n",
      "15    [it, is, not, it, is, got, to, do, with, fair,...\n",
      "16                                 [yeah, take, action]\n",
      "17    [but, but, now, but, now, i, hear, we, have, t...\n",
      "18    [taking, action, is, probably, the, more, impo...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/17_2-0-2.txt\n",
      "0     [ok, so, i, think, that, marijuana, should, be...\n",
      "1     [i, agree, like, i, feel, like, marijuana, sho...\n",
      "2     [and, it, is, not, as, adjective, either, like...\n",
      "3     [i, feel, like, they, just, will, not, legaliz...\n",
      "4     [and, then, if, they, do, end, up, like, selli...\n",
      "5     [cause, people, if, you, think, about, it, lik...\n",
      "6     [and, then, there, is, the, whole, thing, like...\n",
      "7     [like, ibuprofen, 's, a, drug, there, is, a, b...\n",
      "8     [yeah, there, is, a, bunch, of, other, drugs, ...\n",
      "9                                    [i, do, not, know]\n",
      "10    [maybe, it, is, not, cause, my, yeah, it, is, ...\n",
      "11    [i, do, not, yeah, sorry, all, of, the, side, ...\n",
      "12    [i, do, not, know, or, it, could, also, be, li...\n",
      "13    [yeah, i, feel, like, it, is, like, it, is, a,...\n",
      "14    [yeah, it, does, not, bother, you, or, anythin...\n",
      "15    [yeah, i, mean, i, know, like, i, know, they, ...\n",
      "16    [i, was, not, aware, and, i, remember, i, was,...\n",
      "17    [yeah, and, then, you, just, fall, asleep, or,...\n",
      "18    [yeah, so, there, is, like, more, positive, th...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/40_1-0-3.txt\n",
      "0     [that, is, true, i, think, i, think, well, i, ...\n",
      "1     [well, was, it, knit, was, in, colorado, it, was]\n",
      "2     [colorado, i, think, was, the, one, of, the, o...\n",
      "3     [me, neither, but, i, am, like, if, they, let,...\n",
      "4     [just, because, i, am, thinking, right, now, a...\n",
      "5                                 [second, hand, smoke]\n",
      "6     [i, mean, they, are, just, going, to, get, int...\n",
      "7              [this, is, going, to, bring, more, laws]\n",
      "8     [there, will, be, are, going, to, have, a, har...\n",
      "9     [i, got, to, fight, feel, like, i, could, just...\n",
      "10    [i, could, be, like, walk, on, a, park, and, d...\n",
      "11    [yeah, because, a, lot, of, people, would, com...\n",
      "12    [i, know, one, thing, it, will, stop, i, guess...\n",
      "13             [it, will, be, more, open, to, everyone]\n",
      "14    [but, i, feel, like, it, is, already, prescrip...\n",
      "15    [anyone, can, just, go, op, to, a, random, sto...\n",
      "16    [yeah, right, now, and, everyone, or, as, long...\n",
      "17    [there, are, like, local, stores, here, and, t...\n",
      "18    [yeah, i, actually, do, not, know, too, much, ...\n",
      "19    [yeah, it, goes, both, ways, it, depends, on, ...\n",
      "20               [i, see, your, side, too, i, see, how]\n",
      "21    [you, got, to, choose, for, drinking, like, yo...\n",
      "22         [as, long, as, people, it, is, your, choice]\n",
      "23       [it, is, your, choice, really, bigger, prison]\n",
      "24    [what, else, i, just, feel, like, california, ...\n",
      "25    [they, are, trying, to, be, like, the, role, m...\n",
      "26    [because, i, mean, i, feel, like, we, have, a,...\n",
      "27    [and, there, will, not, be, like, a, big, diff...\n",
      "28    [it, will, not, be, a, huge, yeah, it, will, n...\n",
      "29    [they, got, to, do, their, research, on, it, i...\n",
      "30    [i, just, think, right, know, that, california...\n",
      "31    [that, is, one, of, the, reasons, why, i, had,...\n",
      "32    [yeah, there, are, other, issues, that, califo...\n",
      "33    [i, think, if, it, was, passed, for, legalizin...\n",
      "34    [it, will, because, after, you, heard, that, t...\n",
      "35    [yeah, i, do, i, was, like, colorado, are, you...\n",
      "36    [yeah, and, like, everyone, 's, is, just, goin...\n",
      "37    [right, just, for, that, that, is, all, they, ...\n",
      "38    [like, let, us, go, over, to, this, state, and...\n",
      "39    [i, will, prevent, from, lots, of, big, things...\n",
      "40    [from, happening, too, that, is, why, i, think...\n",
      "41    [yeah, it, is, just, what, we, hear, and, what...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/8_1-1-3.txt\n",
      "0              [what, did, you, think, about, the, war]\n",
      "1                               [i, am, for, the, wars]\n",
      "2     [are, you, for, it, i, was, against, it, why, ...\n",
      "3     [well, because, they, obviously, combed, us, o...\n",
      "4     [ok, and, i, think, i, was, against, it, becau...\n",
      "5     [well, because, we, were, they, are, fighting,...\n",
      "6     [and, yet, the, combed, the, the, train, after...\n",
      "7                     [yeah, like, i, think, they, did]\n",
      "8     [they, did, that, and, then, i, think, they, d...\n",
      "9          [see, exactly, so, we, need, to, stop, them]\n",
      "10    [they, actually, did, not, learn, from, the, r...\n",
      "11                        [i, would, not, kill, anyone]\n",
      "12    [ok, then, what, makes, you, think, about, we,...\n",
      "13    [well, why, did, they, kill, us, they, were, k...\n",
      "14    [but, did, not, we, kill, people, over, there,...\n",
      "15    [our, intentions, are, not, to, kill, everyone...\n",
      "16    [but, the, method, that, george, bush, yeah, i...\n",
      "17    [however, that, is, within, their, own, countr...\n",
      "18    [but, are, not, we, part, of, the, country, ar...\n",
      "19    [no, i, mean, in, mexico, that, is, within, th...\n",
      "20    [because, i, know, like, the, border, patrol, ...\n",
      "21    [well, that, is, not, like, what, we, want, to...\n",
      "22    [yeah, but, how, do, we, how, do, we, regulate...\n",
      "23    [we, have, no, control, over, it, i, mean, the...\n",
      "24    [how, are, we, strong, our, economy, is, reall...\n",
      "25    [we, have, a, we, are, our, economy, is, down,...\n",
      "26    [our, our, economy, most, of, our, economy, is...\n",
      "27    [that, is, what, i, meant, like, a, strong, mi...\n",
      "28    [like, we, are, we, are, doing, globalization,...\n",
      "29    [i, was, talking, like, militarywise, that, is...\n",
      "30    [what, makes, you, think, that, but, what, did...\n",
      "31    [yeah, but, they, are, still, coming, but, lik...\n",
      "32    [like, alqaeda, they, could, have, done, it, t...\n",
      "33    [they, do, some, cruel, stuff, to, to, their, ...\n",
      "34    [if, they, if, they, would, have, done, it, to...\n",
      "35    [so, what, you, are, saying, is, we, should, n...\n",
      "36    [i, think, we, should, have, i, think, should,...\n",
      "37    [so, you, believe, like, nine, eleven, was, fa...\n",
      "38    [like, i, do, not, think, it, is, face, well, ...\n",
      "39    [but, it, is, still, providing, protection, fo...\n",
      "40    [well, when, when, we, need, protection, like,...\n",
      "41    [exactly, that, is, why, we, have, the, nation...\n",
      "42    [no, we, need, like, more, you, know, we, need...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/4_2-1-3.txt\n",
      "0     [i, am, formal, against, because, i, am, for, ...\n",
      "1       [you, are, both, for, and, against, all, right]\n",
      "2     [cause, human, rights, and, another, one, 's, ...\n",
      "3     [well, i, think, i, am, i, am, for, death, pen...\n",
      "4     [that, is, true, life, then, like, you, should...\n",
      "5     [yeah, but, then, i, still, believe, when, you...\n",
      "6     [yeah, if, it, is, an, accident, that, is, ano...\n",
      "7     [if, it, is, an, accident, yeah, but, i, mean,...\n",
      "8     [is, that, a, chance, you, are, willing, to, t...\n",
      "9     [yeah, they, could, be, accused, of, doing, it...\n",
      "10    [yeah, cause, the, phrenology, and, stuff, was...\n",
      "11    [no, it, was, just, a, little, like, he, was, ...\n",
      "12    [was, it, really, i, am, on, the, fence, that,...\n",
      "13    [well, he, had, be, old, and, when, he, came, ...\n",
      "14    [what, if, they, are, like, when, they, went, ...\n",
      "15    [they, will, be, missing, out, so, much, already]\n",
      "16                           [he, is, willing, to, die]\n",
      "17                 [yeah, why, do, not, you, just, die]\n",
      "18    [so, people, might, say, that, human, rights, ...\n",
      "19                      [what, is, it, what, about, it]\n",
      "20    [how, we, argue, that, they, just, should, die...\n",
      "21    [well, i, mean, i, think, that, they, do, not,...\n",
      "22    [so, what, you, believe, is, if, you, are, for...\n",
      "23    [alright, if, the, person, okay, well, i, mean...\n",
      "24    [or, if, you, let, him, go, if, he, commits, m...\n",
      "25    [yeah, you, do, not, want, to, like, a, aerial...\n",
      "26    [i, just, like, but, if, he, is, not, the, mur...\n",
      "27      [the, the, thing, is, you, can, not, prove, it]\n",
      "28    [there, are, ways, like, there, was, this, cas...\n",
      "29               [so, he, was, being, killed, for, the]\n",
      "30          [he, was, he, was, like, in, jail, for, it]\n",
      "31                                [was, he, killed, or]\n",
      "32    [he, was, not, killed, but, he, was, in, jail,...\n",
      "33    [just, in, jail, for, a, long, time, okay, tha...\n",
      "34    [the, serious, thing, is, like, incident, ther...\n",
      "35                    [well, if, they, win, then, yeah]\n",
      "36    [yeah, that, i, am, not, sure, how, i, feel, l...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/36_1-1-2.txt\n",
      "0                [i, think, i, believe, it, should, be]\n",
      "1       [ok, i, think, it, should, be, legalized, yeah]\n",
      "2                            [be, legalized, legalized]\n",
      "3     [yeah, yeah, yeah, like, illegal, like, they, ...\n",
      "4     [so, why, do, you, think, it, should, be, ille...\n",
      "5                      [because, up, lots, of, reasons]\n",
      "6     [hata, though, i, think, i, mean, for, me, i, ...\n",
      "7                          [and, oregon, and, colorado]\n",
      "8     [colorado, so, i, think, that, would, be, inte...\n",
      "9     [like, yeah, i, totally, understand, it, is, l...\n",
      "10    [yeah, and, i, know, it, has, been, i, mean, i...\n",
      "11    [that, did, not, work, i, think, it, still, wo...\n",
      "12    [and, that, is, some, yeah, i, guess, it, is, ...\n",
      "13    [it, has, got, the, negative, yeah, but, have,...\n",
      "14    [just, so, yeah, how, about, like, medical, in...\n",
      "15    [well, medical, medical, system, is, like, rea...\n",
      "16    [yeah, it, is, not, hard, at, all, to, get, th...\n",
      "17    [and, it, is, not, yeah, these, doctors, like,...\n",
      "18    [yeah, and, i, think, i, mean, that, i, say, t...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/3_2-0-2.txt\n",
      "0     [okay, so, what, do, you, think, about, gay, a...\n",
      "1     [i, think, they, have, the, right, to, get, ma...\n",
      "2     [i, feel, the, same, way, as, you, do, yeah, o...\n",
      "3     [not, really, i, mean, they, are, just, expres...\n",
      "4     [yeah, i, think, the, same, way, do, you, thin...\n",
      "5     [yeah, because, in, the, future, i, think, peo...\n",
      "6     [yeah, like, how, our, generation, is, totally...\n",
      "7     [cause, they, did, not, see, that, in, their, ...\n",
      "8     [me, too, and, i, have, gay, friends, too, the...\n",
      "9     [the, parents, yeah, all, they, want, is, thei...\n",
      "10    [that, is, very, sad, but, definitely, they, t...\n",
      "11                                     [yeah, they, do]\n",
      "12           [do, you, know, any, same, sex, marriages]\n",
      "13    [not, personally, but, i, have, a, couple, of,...\n",
      "14    [yeah, better, yeah, that, is, true, i, know, ...\n",
      "15    [i, do, not, think, there, is, that, much, jea...\n",
      "16    [there, probably, is, there, probably, still, ...\n",
      "17         [they, seem, to, overcome, everything, well]\n",
      "18    [definitely, i, agree, what, else, what, do, y...\n",
      "19    [well, i, asked, my, what, she, would, think, ...\n",
      "20    [so, is, my, what, about, your, dad, i, think,...\n",
      "21    [yeah, that, helps, i, think, like, over, time...\n",
      "22    [because, it, is, not, like, their, kid, 's, g...\n",
      "23    [you, love, someone, parents, not, as, they, h...\n",
      "24    [definitely, i, definitely, agree, there, is, ...\n",
      "25    [they, are, not, important, they, can, find, a...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/31_1-0-2.txt\n",
      "0     [well, i, personally, do, not, believe, in, ga...\n",
      "1     [is, that, is, it, because, like, you, say, be...\n",
      "2     [i, think, it, is, more, because, well, first,...\n",
      "3     [yeah, so, do, you, see, the, bible, as, like,...\n",
      "4     [yeah, i, like, i, said, i, do, not, i, do, no...\n",
      "5     [so, is, your, only, reason, because, the, bib...\n",
      "6     [it, is, also, because, you, know, my, customs...\n",
      "7     [yeah, yeah, well, i, do, not, know, i, guess,...\n",
      "8     [yeah, i, i, do, understand, your, position, a...\n",
      "9     [yeah, though, yeah, i, understand, we, have, ...\n",
      "10    [yeah, i, think, it, is, also, you, know, and,...\n",
      "11    [yeah, yeah, another, reason, why, i, do, supp...\n",
      "12    [i, i, totally, agree, with, you, yeah, no, an...\n",
      "13    [yeah, yeah, yeah, no, i, agree, i, mean, like...\n",
      "14    [and, you, are, right, like, there, have, been...\n",
      "15    [do, you, think, there, is, like, a, differenc...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/25_2-1-3.txt\n",
      "0                       [yeah, i, am, against, it, too]\n",
      "1               [yeah, how, come, you, feel, that, way]\n",
      "2       [well, well, i, am, a, christian, for, one, so]\n",
      "3                                     [i, am, catholic]\n",
      "4     [yeah, okay, so, for, yeah, i, do, not, believ...\n",
      "5     [yeah, so, i, get, you, yeah, but, however, ev...\n",
      "6     [i, feet, like, they, do, not, allow, them, to...\n",
      "7     [they, do, not, the, reason, why, the, main, r...\n",
      "8     [and, that, is, what, we, are, talking, about,...\n",
      "9     [they, have, the, rights, that, that, we, have...\n",
      "10    [but, i, think, that, is, a, little, stupid, b...\n",
      "11    [ha, ha, so, and, then, i, also, well, this, c...\n",
      "12    [yeah, i, do, not, think, they, do, that, either]\n",
      "13                                [yeah, i, feel, like]\n",
      "14    [those, things, you, think, that, they, should...\n",
      "15    [yeah, they, should, get, that, but, you, know...\n",
      "16    [but, then, the, thing, is, then, they, will, ...\n",
      "17                 [and, you, can, not, have, children]\n",
      "18    [yeah, so, in, the, bible, it, is, not, natura...\n",
      "19    [yeah, i, feel, like, it, should, not, be, cal...\n",
      "20    [inside, the, church, hata, and, you, violatin...\n",
      "21    [what, we, believe, in, it, is, yeah, separati...\n",
      "22    [for, a, reason, hata, i, think, some, people,...\n",
      "23    [let, us, see, we, can, just, read, the, other...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/25_1-0-3.txt\n",
      "0                [your, views, on, the, death, penalty]\n",
      "1     [i, was, dealing, on, this, also, but, i, said...\n",
      "2                                       [why, is, that]\n",
      "3     [because, due, i, look, i, like, i, have, like...\n",
      "4     [well, well, i, think, the, death, penalty, wo...\n",
      "5                                [what, do, you, think]\n",
      "6     [what, i, what, would, i, say, death, the, death]\n",
      "7     [because, i, am, going, for, a, yes, right, no...\n",
      "8     [death, penalty, as, i, see, it, is, i, see, a...\n",
      "9                   [like, killing, people, like, okay]\n",
      "10    [like, killing, people, like, like, like, or, ...\n",
      "11    [that, is, that, is, only, wait, it, is, not, ...\n",
      "12    [death, penalty, they, they, sit, there, like,...\n",
      "13    [well, no, no, i, meant, like, like, if, they,...\n",
      "14    [well, yea, but, at, the, same, time, you, hav...\n",
      "15                    [food, costs, like, water, costs]\n",
      "16    [all, of, that, when, they, they, should, not,...\n",
      "17    [probably, even, more, people, like, yeah, oka...\n",
      "18    [okay, like, and, in, like, in, plenty, of, pl...\n",
      "19    [yeah, i, know, they, get, like, food, free, h...\n",
      "20    [why, like, they, do, not, deserve, that, they...\n",
      "21    [so, like, do, you, like, think, it, is, misse...\n",
      "22    [honestly, death, i, would, see, as, rather, p...\n",
      "23    [have, you, seen, like, okay, like, electric, ...\n",
      "24    [yeah, but, that, is, like, pretty, much, gone...\n",
      "25      [like, where, would, they, shoot, them, though]\n",
      "26    [like, they, line, up, a, row, of, people, and...\n",
      "27                [have, you, researches, this, before]\n",
      "28                  [i, just, know, about, that, topic]\n",
      "29       [now, i, do, not, know, anything, about, this]\n",
      "30    [there, is, an, electric, chair, there, is, le...\n",
      "31    [but, how, many, states, do, they, like, do, t...\n",
      "32     [most, of, the, time, it, is, lethal, injection]\n",
      "33    [okay, and, like, the, states, that, do, allow...\n",
      "34    [some, of, them, still, do, the, shooting, but...\n",
      "35    [life, away, yeah, like, if, they, are, on, a,...\n",
      "36    [yeah, like, if, they, are, dying, say, they, ...\n",
      "37    [like, i, think, doctors, get, that, consent, ...\n",
      "38    [they, are, not, allowed, to, do, that, it, is...\n",
      "39                       [see, i, did, not, know, that]\n",
      "40    [yeah, it, is, illegal, to, do, because, peopl...\n",
      "41    [like, no, that, because, the, people, said, t...\n",
      "42    [yeah, but, people, can, get, in, trouble, for...\n",
      "43    [did, not, know, that, are, there, like, times...\n",
      "44    [well, i, mean, like, pulling, pulling, the, p...\n",
      "45                       [wait, so, like, mercy, kills]\n",
      "46                           [that, is, a, mercy, kill]\n",
      "47    [like, they, oh, inject, them, with, something...\n",
      "48    [they, can, inject, them, pulling, the, plug, ...\n",
      "49                [so, pulling, the, plug, would, that]\n",
      "50    [that, would, not, that, would, not, i, do, no...\n",
      "51    [but, pulling, the, plug, can, not, that, is, ...\n",
      "52    [yeah, well, like, i, said, like, say, they, a...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/29_2-0-3.txt\n",
      "0     [okay, so, for, me, well, since, i, am, a, chr...\n",
      "1     [i, agree, to, an, extent, because, i, mean, l...\n",
      "2     [yeah, that, is, what, like, that, is, how, i,...\n",
      "3     [like, it, is, your, choice, and, then, like, ...\n",
      "4     [but, i, feel, like, i, do, not, know, i, feel...\n",
      "5     [that, is, true, yeah, so, like, that, is, tru...\n",
      "6     [yeah, but, like, teenager, having, kiss, they...\n",
      "7     [exactly, so, just, in, a, general, sense, rig...\n",
      "8     [why, let, it, suffer, or, watch, it, suffer, ...\n",
      "9     [like, it, is, right, like, if, it, gets, hit,...\n",
      "10    [i, feel, like, in, that, sense, abortion, lik...\n",
      "11    [yeah, yeah, in, most, cases, like, abortion, ...\n",
      "12    [which, is, like, i, feel, like, most, abortio...\n",
      "13                          [they, make, it, too, easy]\n",
      "14    [right, i, feel, i, do, not, know, but, yeah, ...\n",
      "15    [i, do, not, know, it, seems, like, we, are, k...\n",
      "16    [with, the, opposing, what, is, that, say, the...\n",
      "17    [i, mean, i, would, say, well, i, mean, yeah, ...\n",
      "18    [yeah, but, if, they, said, like, all, abortio...\n",
      "19    [like, a, regulation, or, something, yeah, ver...\n",
      "20    [because, i, have, actually, never, said, like...\n",
      "21    [yeah, because, then, it, is, it, is, kind, of...\n",
      "22    [yeah, in, a, sense, it, does, but, then, it, ...\n",
      "23    [yeah, yeah, because, then, it, is, like, if, ...\n",
      "24    [yeah, if, it, wa, n't, meant, to, me, then, i...\n",
      "25                                       [it, is, cool]\n",
      "26    [i, did, not, i, did, not, really, notice, tha...\n",
      "27    [i, would, only, imagine, if, the, body, only,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/24_2-0-2.txt\n",
      "0     [okay, so, i, just, said, i, was, against, it,...\n",
      "1     [i, personally, i, am, against, it, but, i, am...\n",
      "2     [yeah, and, then, i, have, also, considered, l...\n",
      "3     [okay, so, we, can, decide, that, we, are, for...\n",
      "4         [we, are, against, legally, we, are, against]\n",
      "5     [alright, i, am, just, trying, to, think, of, ...\n",
      "6     [if, people, were, if, people, were, opposing,...\n",
      "7     [okay, so, your, saying, that, it, would, caus...\n",
      "8     [yeah, liberals, would, agree, but, conservati...\n",
      "9     [yeah, i, am, under, impression, that, the, ma...\n",
      "10                [yeah, that, is, what, i, think, too]\n",
      "11    [i, kind, of, thought, i, kind, of, think, arg...\n",
      "12    [everyone, one, of, them, yeah, and, i, think,...\n",
      "13    [yeah, i, i, could, see, that, being, an, issu...\n",
      "14    [so, also, yeah, that, is, makes, me, think, t...\n",
      "15    [that, is, a, good, point, i, feel, like, shou...\n",
      "16    [like, a, perfect, example, i, think, about, i...\n",
      "17    [well, i, do, not, know, the, details, on, tha...\n",
      "18    [that, is, another, reason, that, they, would,...\n",
      "19    [maybe, the, us, that, i, do, not, if, the, us...\n",
      "20    [yeah, like, if, you, were, in, like, the, us,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/28_2-1-2.txt\n",
      "0                  [okay, do, you, want, to, go, first]\n",
      "1     [sure, okay, so, i, think, that, gay, and, les...\n",
      "2     [yeah, i, agree, just, because, i, think, that...\n",
      "3     [yeah, it, is, that, is, how, they, are, like,...\n",
      "4     [yeah, and, i, also, think, that, things, do, ...\n",
      "5     [society, yeah, and, i, think, i, could, see, ...\n",
      "6     [right, so, it, is, more, like, i, think, in, ...\n",
      "7     [and, i, feel, like, it, is, also, a, lot, of,...\n",
      "8     [yeah, because, i, think, that, change, is, mo...\n",
      "9     [so, basically, i, feel, like, the, main, argu...\n",
      "10    [i, mean, i, am, not, really, religious, so, i...\n",
      "11    [yeah, see, i, am, not, religious, either, but...\n",
      "12                        [we, are, more, open, minded]\n",
      "13    [yeah, but, also, like, we, do, not, have, up,...\n",
      "14    [so, maybe, our, our, part, of, our, counter, ...\n",
      "15    [yeah, and, i, guess, i, feel, like, i, talk, ...\n",
      "16    [yeah, and, then, i, do, agree, like, it, is, ...\n",
      "17    [yeah, i, agree, and, like, if, it, is, tradit...\n",
      "18    [i, think, so, i, think, so, i, think, so, tha...\n",
      "19    [when, the, when, the, law, first, got, put, i...\n",
      "20    [yeah, i, agree, and, just, because, it, is, h...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/2_2-1-2.txt\n",
      "0     [ok, i, i, i, think, we, should, change, the, ...\n",
      "1                [i, think, we, should, change, it, to]\n",
      "2     [the, issue, ok, just, because, like, in, all,...\n",
      "3     [wait, you, think, it, should, be, dropped, to...\n",
      "4     [just, like, a, yeah, that, is, true, yeah, i,...\n",
      "5     [no, i, totally, agree, i, think, it, is, just...\n",
      "6     [i, think, just, like, the, leading, cause, is...\n",
      "7     [well, even, especially, if, it, was, lower, t...\n",
      "8     [yeah, from, like, your, parents, your, not, r...\n",
      "9     [and, right, now, people, are, preaching, you,...\n",
      "10    [because, kiss, can, not, really, do, anything...\n",
      "11    [right, i, agree, i, think, and, then, like, i...\n",
      "12    [yea, that, one, i, feel, like, those, are, ju...\n",
      "13    [i, feel, like, there, is, like, right, the, n...\n",
      "14    [i, think, it, is, like, the, adrenalin, rush,...\n",
      "15    [and, plus, a, lot, of, people, will, wait, ti...\n",
      "16    [yeah, no, yeah, that, is, when, you, need, sl...\n",
      "17    [yeah, no, i, agree, no, i, totally, agree, i,...\n",
      "18    [and, that, is, like, when, you, are, about, t...\n",
      "19    [yea, and, it, is, like, at, when, your, you, ...\n",
      "20    [you, can, not, drink, yea, like, a, lot, of, ...\n",
      "21    [right, it, is, true, it, is, true, it, is, tr...\n",
      "22                        [with, guns, and, explosives]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/11_1-1-3.txt\n",
      "0     [but, do, you, agree, that, marijuana, should,...\n",
      "1     [i, do, not, think, that, marijuana, should, b...\n",
      "2     [but, they, can, still, get, it, and, they, wo...\n",
      "3     [well, i, guess, if, it, is, legalized, then, ...\n",
      "4     [yes, but, let, us, say, and, i, am, assuming,...\n",
      "5     [well, what, do, you, do, if, you, have, a, si...\n",
      "6     [because, even, if, it, is, illegal, they, wou...\n",
      "7     [so, you, are, saying, that, it, is, okay, for...\n",
      "8     [i, mean, if, it, is, legal, it, is, better, b...\n",
      "9     [but, what, safety, is, available, if, they, s...\n",
      "10    [then, their, parents, can, take, care, of, th...\n",
      "11    [so, you, are, saying, that, if, the, children...\n",
      "12    [no, it, is, their, responsibility, it, is, ju...\n",
      "13    [but, i, would, be, more, concerned, like, abo...\n",
      "14    [because, they, are, still, going, to, do, it,...\n",
      "15    [but, legalizing, is, just, going, to, make, i...\n",
      "16    [eitherway, the, marijuana, they, are, going, ...\n",
      "17    [how, would, you, feel, if, your, child, was, ...\n",
      "18    [as, long, as, it, tells, me, but, it, is, wel...\n",
      "19    [what, if, they, are, a, teenager, what, if, t...\n",
      "20    [yeah, once, they, start, driving, or, once, t...\n",
      "21    [so, you, would, feel, totally, comfortable, w...\n",
      "22    [yeah, i, mean, i, can, stop, him, but, i, can...\n",
      "23    [but, would, not, it, be, better, if, there, w...\n",
      "24    [but, they, are, not, going, to, follow, that,...\n",
      "25    [so, you, think, it, should, be, legalized, be...\n",
      "26    [yeah, and, i, think, it, would, open, busines...\n",
      "27    [but, would, not, if, everyone, smoked, mariju...\n",
      "28    [but, people, who, are, but, the, thing, is, t...\n",
      "29    [but, would, not, that, be, increased, if, mar...\n",
      "30    [just, because, they, can, do, it, in, front, ...\n",
      "31    [just, the, people, that, are, doing, it, that...\n",
      "32    [i, think, there, yeah, i, think, they, would,...\n",
      "33                [what, if, what, if, it, hurts, them]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/30_2-0-2.txt\n",
      "0     [alright, so, the, topic, we, were, given, is,...\n",
      "1     [and, i, think, that, when, your, gay, you, do...\n",
      "2     [well, yeah, just, by, being, yourself, i, do,...\n",
      "3     [i, think, that, the, church, has, a, big, inf...\n",
      "4     [yeah, so, yeah, it, is, just, really, sad, th...\n",
      "5     [you, can, marry, a, horse, how, is, that, bet...\n",
      "6     [right, and, in, this, case, you, know, hypoth...\n",
      "7     [beneath, an, animal, a, scavenger, i, wish, t...\n",
      "8     [i, feel, like, it, is, for, a, lot, of, peopl...\n",
      "9     [i, agree, like, relationships, should, be, be...\n",
      "10    [and, that, is, what, the, definition, really,...\n",
      "11                   [yeah, ha, ha, we, definitely, do]\n",
      "12    [yeah, and, i, think, that, for, other, countr...\n",
      "13    [you, have, your, own, mind, you, have, your, ...\n",
      "14    [yeah, and, i, think, that, up, in, the, same,...\n",
      "15    [i, hear, that, everyday, yeah, you, know, the...\n",
      "16    [yeah, it, does, not, say, that, right, it, is...\n",
      "17    [and, being, a, literature, major, you, can, i...\n",
      "18    [yeah, it, is, been, it, is, been, years, sinc...\n",
      "19                                         [no, way, a]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/30_1-1-3.txt\n",
      "0     [alright, you, start, this, one, you, started,...\n",
      "1     [okay, well, i, believe, that, a, women, has, ...\n",
      "2     [so, okay, so, you, say, the, a, woman, has, t...\n",
      "3     [i, do, not, think, that, a, little, bundle, o...\n",
      "4     [so, you, do, not, think, you, do, not, think,...\n",
      "5     [i, think, after, the, three, after, the, thre...\n",
      "6     [but, you, do, not, i, mean, like, so, you, ca...\n",
      "7     [well, my, issue, is, not, is, not, when, a, b...\n",
      "8     [but, why, could, why, is, not, adoption, an, ...\n",
      "9                            [adoption, is, an, option]\n",
      "10    [okay, well, then, so, what, is, wrong, with, ...\n",
      "11    [because, some, some, girls, like, they, get, ...\n",
      "12    [but, is, it, not, their, responsibility, to, ...\n",
      "13    [sometimes, it, is, it, it, is, honestly, an, ...\n",
      "14     [so, you, would, rather, just, kill, the, child]\n",
      "15    [well, if, the, well, if, the, child, is, not,...\n",
      "16    [but, it, is, still, it, is, still, killing, i...\n",
      "17    [i, i, do, not, think, it, is, murder, because...\n",
      "18    [you, are, intentionally, killing, a, human, b...\n",
      "19    [i, think, at, three, i, think, at, three, mon...\n",
      "20    [i, know, i, know, what, you, are, talking, ab...\n",
      "21    [well, we, already, do, that, with, death, pen...\n",
      "22           [but, you, do, not, know, that, for, sure]\n",
      "23    [i, know, plenty, of, girls, who, have, though...\n",
      "24    [but, you, do, not, know, for, sure, i, mean, ...\n",
      "25    [what, do, you, mean, that, is, not, what, all...\n",
      "26    [no, of, course, not, i, mean, that, is, why, ...\n",
      "27    [yeah, but, the, well, i, have, you, been, pre...\n",
      "28                    [and, that, is, a, great, choice]\n",
      "29    [that, was, yeah, well, that, is, a, good, cho...\n",
      "30                [but, why, could, not, you, do, that]\n",
      "31    [because, some, people, have, to, work, for, a...\n",
      "32    [i, know, plenty, of, people, who, work, while...\n",
      "33    [well, i, i, actually, do, not, know, many, pe...\n",
      "34    [well, i, mean, i, work, i, work, at, a, fast,...\n",
      "35    [no, that, is, not, and, you, know, they, they...\n",
      "36    [and, if, and, if, that, person, does, end, up...\n",
      "37    [well, it, it, can, it, can, not, even, think,...\n",
      "38    [yeah, it, it, can, not, literally, consciousl...\n",
      "39    [yeah, and, sometimes, that, is, enough, to, g...\n",
      "40    [and, that, that, brings, up, to, another, poi...\n",
      "41    [that, is, incredibly, rare, that, is, that, i...\n",
      "42    [but, yeah, that, that, is, the, stuff, that, ...\n",
      "43    [i, i, personally, do, not, think, so, i, i, w...\n",
      "44    [well, yeah, safe, sex, but, if, they, have, a...\n",
      "45    [well, do, you, know, do, you, know, painful, ...\n",
      "46    [well, yeah, but, i, am, pretty, sure, that, p...\n",
      "47    [well, i, am, sure, that, is, the, case, but, ...\n",
      "48                  [it, is, free, now, actually, hata]\n",
      "49    [yeah, up, well, with, health, insurance, but,...\n",
      "50    [but, a, lot, of, people, do, not, know, that,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/24_1-1-3.txt\n",
      "0                      [so, i, am, for, the, ran, war]\n",
      "1    [i, am, not, i, just, think, it, should, have,...\n",
      "2    [i, feel, like, if, we, were, to, leave, that,...\n",
      "3    [i, i, am, not, saying, we, should, leave, one...\n",
      "4    [well, i, feel, that, we, are, using, our, fun...\n",
      "5    [yeah, but, i, mean, we, have, problems, too, ...\n",
      "6      [what, do, you, feel, the, hidden, agenda, are]\n",
      "7    [i, do, not, know, what, the, hidden, agenda, ...\n",
      "8    [outside, the, troops, and, money, issue, do, ...\n",
      "9    [i, mean, there, is, our, safety, and, i, am, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/36_1-0-3.txt\n",
      "0           [well, i, am, against, the, death, penalty]\n",
      "1     [ok, i, am, like, not, too, sure, i, am, somet...\n",
      "2                                 [what, do, you, mean]\n",
      "3     [i, mean, i, just, i, hear, like, because, i, ...\n",
      "4     [millions, or, something, like, that, yeah, ye...\n",
      "5     [so, so, it, is, more, expensive, to, kill, so...\n",
      "6     [yeah, yeah, it, is, like, considerably, more,...\n",
      "7     [ok, well, see, then, that, like, because, i, ...\n",
      "8     [then, it, is, like, yeah, i, remember, seeing...\n",
      "9     [yeah, and, then, definitely, also, with, like...\n",
      "10    [i, just, feel, like, america, should, be, a, ...\n",
      "11    [i, was, going, to, say, there, is, a, lot, of...\n",
      "12    [found, innocent, yeah, which, is, even, worse...\n",
      "13    [yeah, because, i, know, there, is, people, th...\n",
      "14    [yeah, exactly, then, you, can, not, do, anyth...\n",
      "15    [yeah, it, is, like, very, very, low, and, i, ...\n",
      "16    [yeah, i, guess, they, will, say, case, to, ca...\n",
      "17    [yeah, well, if, if, it, from, that, perspecti...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/40_1-1-2.txt\n",
      "0                                [should, i, go, first]\n",
      "1                                   [yeah, go, for, it]\n",
      "2     [okay, so, i, think, abortion, is, alright, i,...\n",
      "3     [sorry, the, ones, that, are, not, kind, of, r...\n",
      "4     [yes, ready, for, that, and, so, it, helps, th...\n",
      "5     [i, kind, of, feel, like, it, is, like, it, is...\n",
      "6     [what, else, about, abortion, i, remember, som...\n",
      "7     [i, feel, like, it, has, to, be, one, both, si...\n",
      "8     [exactly, because, she, had, no, choice, of, w...\n",
      "9                     [put, yourself, in, their, shoes]\n",
      "10    [so, that, is, why, i, think, abortion, is, fi...\n",
      "11             [totally, be, an, option, for, everyone]\n",
      "12    [it, should, be, an, option, because, everyone...\n",
      "13    [it, is, like, you, are, preventing, for, some...\n",
      "14    [exactly, because, i, do, not, thing, somethin...\n",
      "15    [it, is, the, first, time, i, am, hearing, tha...\n",
      "16    [i, heard, this, within, the, last, six, month...\n",
      "17    [yeah, who, would, not, want, like, what, is, ...\n",
      "18    [especially, if, it, is, done, in, an, earlier...\n",
      "19         [it, is, a, big, deal, it, is, a, big, deal]\n",
      "20    [start, thinking, about, the, right, choice, y...\n",
      "21    [yeah, if, you, are, joint, to, do, both, it, ...\n",
      "22    [what, else, i, can, not, think, if, anything,...\n",
      "23    [there, is, a, lot, of, big, fumes, about, lik...\n",
      "24                [yeah, you, are, looking, past, that]\n",
      "25                                    [that, woman, or]\n",
      "26    [yeah, it, is, because, i, mean, first, of, th...\n",
      "27                [to, have, it, or, not, to, have, it]\n",
      "28                               [cause, if, they, can]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/8_1-0-2.txt\n",
      "0                           [what, what, did, you, say]\n",
      "1                             [i, would, legalized, it]\n",
      "2     [you, would, legalized, it, i, said, no, but, ...\n",
      "3     [i, think, we, can, just, legalized, it, to, h...\n",
      "4     [i, get, you, i, i, said, no, because, like, y...\n",
      "5     [well, i, compared, it, to, drinking, because,...\n",
      "6     [well, true, too, i, think, like, if, you, hav...\n",
      "7     [well, by, legalizing, marijuana, not, only, w...\n",
      "8     [you, think, when, it, is, legalized, they, ar...\n",
      "9     [like, under, the, table, like, people, still,...\n",
      "10        [yeah, like, on, the, streets, or, something]\n",
      "11    [i, am, pretty, sure, a, possibility, i, mean,...\n",
      "12    [for, their, friends, and, stuff, yeah, true, ...\n",
      "13    [yeah, i, think, so, not, like, dramatically, ...\n",
      "14    [because, i, think, like, there, is, still, on...\n",
      "15    [of, course, yeah, there, has, to, be, an, age...\n",
      "16                      [i, think, like, even, if, you]\n",
      "17    [it, probably, could, be, like, the, same, age...\n",
      "18    [twenty, one, why, you, think, it, is, twenty,...\n",
      "19    [because, eighteen, is, too, young, like, or, ...\n",
      "20    [eighteen, so, because, like, yeah, because, l...\n",
      "21                  [the, age, you, do, not, think, it]\n",
      "22    [yeah, like, the, age, is, not, really, accura...\n",
      "23    [k, now, you, are, legal, what, age, would, yo...\n",
      "24    [i, do, not, think, it, should, be, legal, all...\n",
      "25                                    [alright, it, is]\n",
      "26                     [you, strongly, agree, with, it]\n",
      "27    [i, am, like, somewhat, close, to, no, like, t...\n",
      "28    [well, i, kind, get, what, you, are, saying, t...\n",
      "29    [why, i, was, saying, i, guess, marijuana, is,...\n",
      "30    [well, we, could, regulate, who, gets, it, or,...\n",
      "31          [why, i, was, saying, like, they, regulate]\n",
      "32    [like, there, is, prescriptions, in, the, drug...\n",
      "33    [there, has, to, be, like, a, certain, record,...\n",
      "34    [but, then, again, like, the, cough, medicines...\n",
      "35    [yeah, but, i, mean, we, can, not, stop, all, ...\n",
      "36                          [i, think, the, access, to]\n",
      "37       [so, might, as, well, just, like, tax, on, it]\n",
      "38    [yeah, the, access, is, on, na, be, the, same,...\n",
      "39                                [i, am, pretty, sure]\n",
      "40                              [is, meditation, taxed]\n",
      "41                                   [i, am, not, sure]\n",
      "42    [well, yeah, and, then, like, you, will, have,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/4_2-0-2.txt\n",
      "0                                          [i, am, for]\n",
      "1                                    [i, was, for, too]\n",
      "2                        [okay, why, do, you, say, for]\n",
      "3     [i, said, that, you, can, love, whoever, you, ...\n",
      "4     [cause, yeah, because, you, are, going, to, be...\n",
      "5                 [it, is, whatever, makes, you, happy]\n",
      "6     [what, makes, you, happy, and, then, everybody...\n",
      "7     [yeah, it, is, more, of, like, years, ago, was...\n",
      "8     [and, it, is, also, about, like, happiness, yo...\n",
      "9     [it, is, like, if, anything, you, are, not, th...\n",
      "10    [but, then, they, will, be, like, you, know, f...\n",
      "11                   [yeah, time, changes, like, years]\n",
      "12    [and, we, can, not, take, literally, everythin...\n",
      "13    [some, people, say, that, it, is, not, right, ...\n",
      "14    [god, does, not, like, it, or, they, might, be...\n",
      "15    [like, if, you, want, to, eat, penza, eat, pen...\n",
      "16    [yeah, it, is, human, rights, we, all, have, t...\n",
      "17    [that, is, just, our, fathers, caught, for, this]\n",
      "18    [yeah, mainly, because, you, love, somebody, s...\n",
      "19    [yeah, it, is, your, own, choice, no, one, is,...\n",
      "20    [and, besides, there, would, be, less, populat...\n",
      "21                        [yes, we, are, overpopulated]\n",
      "22    [except, that, maybe, in, a, country, there, m...\n",
      "23    [well, if, you, are, protected, are, we, all, ...\n",
      "24    [yeah, kind, of, i, mean, yeah, like, we, are,...\n",
      "25    [yeah, well, if, you, are, protected, and, kno...\n",
      "26    [and, not, to, be, yeah, stuck, in, traditiona...\n",
      "27    [yeah, you, should, always, be, open, minded, ...\n",
      "28             [i, do, not, know, what, else, to, talk]\n",
      "29    [yeah, and, the, thing, about, the, bible, you...\n",
      "30    [i, do, not, know, what, else, yeah, marriage,...\n",
      "31    [yeah, well, i, think, some, gay, people, are,...\n",
      "32        [i, never, really, read, about, the, divorce]\n",
      "33    [yeah, so, maybe, they, are, happier, they, kn...\n",
      "34    [they, are, looking, more, from, the, inside, ...\n",
      "35    [yeah, they, understand, each, other, more, a,...\n",
      "36    [yeah, but, like, for, i, guess, like, general...\n",
      "37                              [done, with, that, too]\n",
      "38    [but, yeah, if, you, want, to, get, married, i...\n",
      "39    [and, if, you, just, live, together, you, do, ...\n",
      "40    [yeah, i, mean, that, is, the, same, way, with...\n",
      "41        [yeah, i, feel, like, just, living, together]\n",
      "42                         [so, i, do, not, know, yeah]\n",
      "43                           [especially, for, divorce]\n",
      "44             [what, else, is, there, to, talk, about]\n",
      "45    [okay, some, people, think, if, gay, people, m...\n",
      "46    [yeah, normal, and, just, people, and, just, y...\n",
      "47    [yeah, they, are, just, more, open, minded, i,...\n",
      "48    [i, think, taking, care, of, kiss, would, prob...\n",
      "49             [yeah, cause, the, kiss, they, want, so]\n",
      "50    [and, the, kiss, will, be, more, open, minded,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/5_2-1-3.txt\n",
      "0     [what, is, our, stance, i, personally, am, for...\n",
      "1     [yeah, it, is, a, really, no, real, it, is, it...\n",
      "2     [is, a, word, to, get, but, i, do, not, know, ...\n",
      "3     [no, one, is, ever, on, na, have, he, same, vi...\n",
      "4     [but, it, would, be, treating, them, less, tha...\n",
      "5     [if, they, if, they, are, if, they, are, comin...\n",
      "6     [that, is, mostly, true, but, our, founding, f...\n",
      "7     [in, the, end, and, in, them, getting, what, t...\n",
      "8                              [what, sounds, outdated]\n",
      "9     [outdated, appropriate, to, the, current, cult...\n",
      "10           [so, what, if, it, has, mold, or, a, mole]\n",
      "11                    [some, type, of, mold, substance]\n",
      "12                                [a, madly, substance]\n",
      "13    [yeah, yeah, not, like, a, birth, mark, yeah, ...\n",
      "14    [i, suppose, that, i, suppose, that, one, argu...\n",
      "15    [when, i, say, recently, i, mean, like, within...\n",
      "16    [they, would, know, they, would, know, so, tha...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/41_1-0-2.txt\n",
      "0     [i, think, that, death, penalty, should, not, ...\n",
      "1                                 [i, said, it, should]\n",
      "2                                        [why, do, you]\n",
      "3                  [i, said, it, should, only, because]\n",
      "4          [you, should, not, just, like, go, to, jail]\n",
      "5     [that, is, true, but, i, mean, like, if, you, ...\n",
      "6     [and, it, is, more, expensive, for, the, state...\n",
      "7     [that, is, true, and, it, does, take, forever,...\n",
      "8     [you, just, die, it, is, like, you, are, murde...\n",
      "9     [what, if, it, was, like, your, parents, or, s...\n",
      "10    [i, would, want, the, person, to, suffer, more...\n",
      "11    [but, do, not, you, think, that, is, also, moral]\n",
      "12    [killing, someone, that, is, more, moral, if, ...\n",
      "13    [yeah, since, it, is, so, long, the, death, pe...\n",
      "14                                   [by, killing, him]\n",
      "15    [but, what, if, it, is, from, that, view, of, ...\n",
      "16    [why, would, you, want, to, help, him, out, wh...\n",
      "17    [i, do, not, know, i, guess, it, is, life, for...\n",
      "18    [but, yeah, i, understand, your, perspective, ...\n",
      "19    [if, it, is, that, route, then, i, feel, like,...\n",
      "20    [so, the, do, tells, he, was, in, the, they, u...\n",
      "21                                  [do, not, remember]\n",
      "22           [cost, lot, of, tax, dollars, so, i, mean]\n",
      "23    [i, think, in, the, point, of, view, tax, doll...\n",
      "24    [like, other, countries, still, use, death, pe...\n",
      "25    [this, is, more, i, guess, it, is, more, human...\n",
      "26    [the, movie, i, do, not, know, what, it, is, c...\n",
      "27        [this, is, an, old, movie, like, green, mile]\n",
      "28    [yeah, and, he, did, not, want, to, kill, him,...\n",
      "29    [yeah, i, think, it, is, a, good, one, and, th...\n",
      "30                       [he, killed, him, either, way]\n",
      "31    [that, is, more, torture, still, with, electri...\n",
      "32    [it, is, on, my, i, would, i, do, not, approve...\n",
      "33    [case, by, case, basis, i, think, so, i, mean,...\n",
      "34                    [okay, then, you, death, penalty]\n",
      "35    [not, really, i, do, not, have, a, to, so, i, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/37_1-1-2.txt\n",
      "0     [okay, so, health, care, should, not, be, univ...\n",
      "1     [well, i, just, feel, that, we, should, have, ...\n",
      "2     [okay, so, you, then, you, feel, that, we, sho...\n",
      "3     [well, i, think, that, obviously, it, would, c...\n",
      "4     [so, then, what, happens, to, the, the, lawsui...\n",
      "5     [i, think, universal, health, care, and, the, ...\n",
      "6     [so, you, mean, to, say, that, well, yeah, tha...\n",
      "7     [i, think, that, what, whether, there, is, uni...\n",
      "8     [but, then, if, being, free, would, not, where...\n",
      "9     [well, i, mean, universal, health, care, free,...\n",
      "10    [i, mean, i, feel, that, no, one, or, people, ...\n",
      "11    [i, do, not, think, that, it, is, much, that, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/16_2-0-3.txt\n",
      "0     [in, a, supporter, of, gay, marriage, i, am, g...\n",
      "1     [i, agree, i, think, that, you, are, not, born...\n",
      "2     [i, have, i, am, best, friends, with, trans, p...\n",
      "3     [yeah, i, remember, when, prop, was, passed, a...\n",
      "4        [i, think, in, california, it, is, the, worst]\n",
      "5            [and, all, i, am, in, the, united, states]\n",
      "6     [m, yeah, thank, god, california, is, the, but...\n",
      "7     [i, think, that, by, saying, you, can, not, ge...\n",
      "8     [who, they, see, as, a, even, before, years, l...\n",
      "9     [but, what, would, like, be, some, the, counte...\n",
      "10    [the, only, one, i, can, obviously, think, of,...\n",
      "11    [i, would, hear, that, homosexuality, is, not,...\n",
      "12    [but, i, was, on, na, say, that, it, is, prese...\n",
      "13    [i, can, not, imagine, like, being, being, tol...\n",
      "14    [it, is, a, know, and, then, somebody, throws,...\n",
      "15    [and, i, think, that, it, tells, gay, people, ...\n",
      "16    [and, just, the, law, it, sounds, it, is, ridi...\n",
      "17    [i, do, not, know, what, another, counter, arg...\n",
      "18    [i, wan, na, say, that, like, when, i, heard, ...\n",
      "19    [yeah, that, is, it, i, guess, another, argume...\n",
      "20                          [i, do, not, have, a, clue]\n",
      "21    [people, think, that, like, it, it, takes, awa...\n",
      "22                        [i, i, do, not, know, either]\n",
      "23    [some, people, think, like, it, takes, it, mak...\n",
      "24                             [i, can, not, see, that]\n",
      "25    [yeah, i, do, not, either, i, do, not, think, ...\n",
      "26    [i, guess, maybe, it, is, an, argument, that, ...\n",
      "27    [yeah, i, think, that, it, is, kind, of, silly...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/23_2-0-2.txt\n",
      "0     [alright, so, what, is, your, opinion, about, ...\n",
      "1        [i, do, not, think, it, should, be, legalized]\n",
      "2                                       [why, is, that]\n",
      "3     [why, because, i, do, not, know, for, example,...\n",
      "4     [yeah, i, feel, the, same, way, you, know, how...\n",
      "5     [oh, that, is, true, yeah, yeah, but, then, ag...\n",
      "6     [yeah, that, is, true, yeah, and, i, think, es...\n",
      "7     [that, is, true, that, is, true, hata, yeah, y...\n",
      "8     [so, we, do, not, want, to, we, would, rather,...\n",
      "9                          [yeah, yeah, that, is, true]\n",
      "10    [yeah, i, do, not, know, i, heard, i, heard, p...\n",
      "11    [i, am, pretty, sure, it, has, like, negative,...\n",
      "12           [does, it, give, you, mental, retaliation]\n",
      "13    [i, do, not, know, i, heard, like, people, lik...\n",
      "14    [that, is, true, yeah, hata, okay, yeah, so, s...\n",
      "15    [i, know, that, ha, ha, ha, is, not, there, li...\n",
      "16    [yeah, there, are, states, that, legalized, ma...\n",
      "17    [and, then, see, yeah, but, you, do, not, thin...\n",
      "18    [i, do, not, know, if, it, is, legalized, then...\n",
      "19    [but, i, still, think, like, it, is, the, same...\n",
      "20    [yeah, that, is, true, because, now, we, are, ...\n",
      "21    [it, is, not, going, to, be, a, big, differenc...\n",
      "22    [so, i, think, we, should, just, keep, it, rig...\n",
      "23    [yeah, yeah, i, think, so, i, think, that, too...\n",
      "24    [so, do, you, know, the, effect, of, marijuana...\n",
      "25        [what, do, you, mean, like, negative, things]\n",
      "26    [no, the, fact, that, it, why, is, it, used, i...\n",
      "27    [i, am, not, sure, i, no, honestly, i, do, not...\n",
      "28    [is, it, like, temporarily, reducing, the, sym...\n",
      "29                                   [probably, it, is]\n",
      "30                              [i, feel, like, it, is]\n",
      "31    [yeah, because, you, get, high, so, you, like,...\n",
      "32    [yeah, because, i, know, my, cousin, he, has, ...\n",
      "33                                       [he, uses, it]\n",
      "34                   [he, use, the, medical, marijuana]\n",
      "35                [like, people, with, asthma, use, it]\n",
      "36    [i, am, not, sure, if, there, is, any, other, ...\n",
      "37    [medially, i, do, not, know, that, is, true, b...\n",
      "38    [yeah, so, i, wonder, if, like, doctors, presc...\n",
      "39    [illegally, that, is, true, and, then, they, g...\n",
      "40    [you, need, a, doctor, 's, prescription, yeah,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/9_1-1-2.txt\n",
      "0     [okay, well, i, do, not, know, i, it, is, kind...\n",
      "1          [why, do, you, think, it, should, be, legal]\n",
      "2     [well, because, compared, to, other, drugs, li...\n",
      "3     [well, i, think, it, should, be, illegal, beca...\n",
      "4     [i, think, that, is, yea, that, there, is, a, ...\n",
      "5     [the, effects, it, has, to, choose, yeah, is, ...\n",
      "6         [yeah, that, is, that, is, that, is, another]\n",
      "7               [so, you, have, to, think, about, that]\n",
      "8     [like, i, just, think, the, fact, that, it, is...\n",
      "9     [i, think, that, they, also, use, it, for, i, ...\n",
      "10    [yeah, something, something, like, yeah, but, ...\n",
      "11    [and, then, another, thing, is, say, we, do, m...\n",
      "12    [yeah, well, like, i, i, i, get, that, like, t...\n",
      "13    [yeah, so, that, is, my, concern, with, alcoho...\n",
      "14    [yeah, there, is, yeah, i, i, do, not, i, i, d...\n",
      "15    [because, i, think, it, is, because, it, is, a...\n",
      "16                          [it, has, it, has, a, yeah]\n",
      "17               [that, is, my, thing, it, is, a, drug]\n",
      "18    [it, is, a, it, has, got, a, it, has, got, a, ...\n",
      "19    [yeah, because, i, know, in, i, learned, this,...\n",
      "20    [yeah, i, think, it, is, i, think, it, is, kin...\n",
      "21    [and, think, i, think, is, still, is, like, ju...\n",
      "22    [yeah, that, is, there, is, there, yeah, it, i...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/30_1-0-2.txt\n",
      "0     [okay, so, what, i, think, is, that, i, think,...\n",
      "1     [i, personally, think, that, they, should, jus...\n",
      "2     [yeah, i, do, not, know, it, is, just, i, feel...\n",
      "3     [well, i, am, sure, they, would, still, have, ...\n",
      "4     [yeah, i, know, but, i, mean, like, the, cerem...\n",
      "5     [i, i, doubt, they, the, extremists, would, be...\n",
      "6     [probably, not, hata, yeah, so, then, why, do,...\n",
      "7     [well, i, think, that, that, is, ah, that, tha...\n",
      "8     [i, dont, know, though, because, the, word, it...\n",
      "9     [essentially, they, would, be, doing, the, sam...\n",
      "10    [yeah, see, that, others, me, yeah, a, differe...\n",
      "11    [well, the, rings, are, just, like, everything...\n",
      "12    [yeah, i, do, not, know, it, just, really, dis...\n",
      "13    [it, there, is, just, too, many, connections, ...\n",
      "14    [i, do, not, know, that, that, was, interestin...\n",
      "15    [no, but, there, is, constant, arguments, agai...\n",
      "16    [i, do, not, know, not, really, arguing, but, ...\n",
      "17    [it, has, to, yeah, it, has, to, hit, close, t...\n",
      "18          [is, your, family, like, really, religious]\n",
      "19    [you, know, what, not, religious, but, they, a...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/24_1-0-2.txt\n",
      "0           [i, think, it, should, just, be, legalized]\n",
      "1                                    [yeah, i, say, no]\n",
      "2                                        [you, say, no]\n",
      "3     [yeah, because, my, house, mates, do, it, and,...\n",
      "4     [it, has, on, them, well, i, just, think, it, ...\n",
      "5     [yeah, i, feel, like, because, it, is, not, fe...\n",
      "6     [yeah, but, i, mean, that, is, the, way, it, w...\n",
      "7     [well, looking, at, it, you, see, perspective,...\n",
      "8     [they, spend, their, money, on, ha, but, that,...\n",
      "9     [i, guess, if, i, had, nothing, much, to, do, ...\n",
      "10    [no, have, you, ever, tried, a, cigarette, do,...\n",
      "11                                        [yeah, i, am]\n",
      "12    [okay, up, i, do, not, know, if, i, would, try...\n",
      "13    [well, do, you, like, it, if, you, were, walki...\n",
      "14    [no, i, hate, no, because, i, mean, there, is,...\n",
      "15                               [fair, game, anywhere]\n",
      "16    [yeah, but, i, think, it, should, be, able, fo...\n",
      "17    [how, would, you, think, of, someone, smoking,...\n",
      "18    [well, no, you, can, not, do, that, that, is, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/24_2-1-3.txt\n",
      "0     [well, i, have, always, been, oppose, to, abor...\n",
      "1     [yeah, i, feel, it, is, murder, which, i, do, ...\n",
      "2     [yeah, and, that, is, totally, understandable,...\n",
      "3     [are, are, you, in, the, do, you, consider, al...\n",
      "4     [okay, yeah, because, i, mean, in, even, when,...\n",
      "5     [yeah, and, actually, there, in, later, years,...\n",
      "6     [and, i, think, about, things, like, i, am, re...\n",
      "7     [i, think, for, many, women, who, decide, to, ...\n",
      "8     [yeah, i, think, that, the, government, could,...\n",
      "9     [do, you, consider, with, birth, control, to, ...\n",
      "10    [like, i, am, i, consider, abortion, to, be, w...\n",
      "11    [so, your, okay, with, the, idea, of, birth, c...\n",
      "12    [i, think, it, is, okay, if, it, is, for, like...\n",
      "13    [so, in, that, regard, you, are, you, are, oka...\n",
      "14               [i, think, that, i, agree, with, that]\n",
      "15    [okay, okay, i, am, just, trying, to, figure, ...\n",
      "16    [i, though, considering, like, birth, control,...\n",
      "17    [okay, yeah, i, think, with, the, availability...\n",
      "18    [yeah, i, agree, with, that, too, because, the...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/2_2-0-3.txt\n",
      "0     [i, said, yes, that, i, think, you, should, be...\n",
      "1     [i, am, i, am, kind, of, if, because, like, ju...\n",
      "2     [it, is, ok, if, it, is, different, depends, o...\n",
      "3     [no, i, completely, agree, cause, i, get, like...\n",
      "4     [yeah, i, definitely, yeah, i, think, that, bu...\n",
      "5     [it, is, just, greed, a, lot, of, people, yeah...\n",
      "6     [yea, i, definitely, i, agree, and, i, think, ...\n",
      "7     [buy, their, way, into, life, yeah, so, then, ...\n",
      "8     [and, a, lot, of, the, times, the, people, who...\n",
      "9     [yeah, and, i, feel, like, people, who, have, ...\n",
      "10    [yeah, no, i, agree, i, think, but, i, definit...\n",
      "11    [it, is, really, hard, to, find, those, terms,...\n",
      "12                           [yeah, i, agree, i, agree]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/28_2-0-3.txt\n",
      "0                         [do, we, disagree, or, agree]\n",
      "1               [okay, okay, well, what, did, you, say]\n",
      "2                       [with, abortion, i, agree, yes]\n",
      "3     [that, you, should, be, able, to, okay, me, to...\n",
      "4     [well, i, mean, understand, the, other, side, ...\n",
      "5     [yeah, no, i, agree, i, do, not, know, the, sp...\n",
      "6     [yeah, i, agree, and, plus, also, for, me, i, ...\n",
      "7     [yeah, there, is, has, to, be, like, when, you...\n",
      "8     [yeah, so, i, do, agree, with, that, regulatio...\n",
      "9     [yeah, and, i, feel, like, an, opposing, argum...\n",
      "10    [yeah, plus, it, is, not, like, it, is, super,...\n",
      "11    [yeah, and, she, yeah, like, there, is, always...\n",
      "12    [yeah, i, do, agree, with, that, actually, bec...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/11_1-0-2.txt\n",
      "0     [i, think, no, because, i, think, us, should, ...\n",
      "1     [i, think, that, it, should, be, universal, be...\n",
      "2     [yes, but, they, can, like, help, people, who,...\n",
      "3     [but, what, if, they, can, not, afford, it, wh...\n",
      "4     [if, american, citizens, can, not, afford, it,...\n",
      "5     [but, what, if, it, is, a, situation, where, t...\n",
      "6     [yeah, then, i, think, they, should, they, sho...\n",
      "7     [so, you, so, there, should, be, universal, he...\n",
      "8     [in, different, cases, in, different, scenario...\n",
      "9     [so, you, think, only, certain, individuals, s...\n",
      "10    [yeah, their, cases, should, be, read, or, int...\n",
      "11    [you, you, are, more, for, towards, no, health...\n",
      "12    [no, they, no, no, no, that, they, should, be,...\n",
      "13                     [only, to, those, who, need, it]\n",
      "14    [yeah, and, if, they, are, if, they, are, comi...\n",
      "15               [they, should, be, helped, too, right]\n",
      "16    [do, you, think, everyone, should, get, health...\n",
      "17    [i, think, though, i, think, to, a, certain, e...\n",
      "18    [does, do, we, when, we, say, health, care, do...\n",
      "19    [are, you, saying, that, it, should, be, inclu...\n",
      "20    [i, am, asking, when, we, when, you, say, heal...\n",
      "21    [i, i, feel, a, little, bit, differently, abou...\n",
      "22    [i, think, younger, adults, especially, younge...\n",
      "23                      [it, should, not, be, included]\n",
      "24                     [yeah, included, in, healthcare]\n",
      "25    [with, that, i, agree, so, how, else, do, you,...\n",
      "26    [healthcare, up, and, i, think, people, who, a...\n",
      "27    [but, how, do, you, determine, who, actually, ...\n",
      "28    [you, just, simply, ask, them, and, trust, the...\n",
      "29    [but, what, if, they, do, lie, then, what, do,...\n",
      "30    [i, guess, then, then, you, can, still, go, ba...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/30_2-1-3.txt\n",
      "0     [so, our, topic, was, whether, or, not, the, d...\n",
      "1     [right, i, personally, feel, that, the, drinki...\n",
      "2     [i, think, from, considering, my, experience, ...\n",
      "3     [so, if, we, were, yeah, if, there, was, not, ...\n",
      "4     [they, will, probably, go, over, board, yeah, ...\n",
      "5     [to, handle, alcohol, yeah, and, the, use, of,...\n",
      "6     [you, know, sometimes, i, think, that, the, ad...\n",
      "7     [yeah, that, is, one, way, i, and, that, is, w...\n",
      "8     [it, make, you, look, bad, and, a, part, from,...\n",
      "9     [right, you, know, and, if, you, also, want, t...\n",
      "10    [and, again, because, they, can, not, handle, it]\n",
      "11    [exactly, your, not, responsible, for, it, you...\n",
      "12    [yeah, and, i, understand, like, sometimes, no...\n",
      "13    [yeah, you, never, you, do, not, want, to, pro...\n",
      "14    [yeah, i, thin, alcohol, is, a, thing, in, the...\n",
      "15    [it, is, not, really, worth, it, in, the, end,...\n",
      "16    [it, is, not, and, it, takes, an, effect, on, ...\n",
      "17    [right, so, like, yeah, it, does, not, really,...\n",
      "18    [and, again, from, the, irresponsibility, fact...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/25_1-1-2.txt\n",
      "0     [do, we, begin, so, what, do, you, say, on, re...\n",
      "1                                          [i, say, no]\n",
      "2                                       [why, is, that]\n",
      "3     [why, is, that, it, is, because, people, do, n...\n",
      "4     [that, is, so, true, okay, i, say, yes, becaus...\n",
      "5     [so, you, are, saying, since, it, is, not, leg...\n",
      "6     [if, it, is, because, like, i, do, not, know, ...\n",
      "7     [it, is, not, just, that, though, its, like, t...\n",
      "8     [i, guess, it, is, kind, of, like, okay, never...\n",
      "9     [well, because, ok, if, it, were, legalized, i...\n",
      "10    [is, that, is, true, is, so, true, i, do, not,...\n",
      "11    [i, mean, there, are, not, there, are, not, li...\n",
      "12    [yeah, i, guess, it, depends, on, the, person,...\n",
      "13    [well, some, not, all, but, the, people, that,...\n",
      "14    [okay, i, guess, it, depends, see, it, depends...\n",
      "15    [it, is, not, like, one, person, i, have, talk...\n",
      "16                [they, want, to, get, high, all, day]\n",
      "17    [pretty, much, they, they, enjoy, that, feelin...\n",
      "18    [people, always, tell, me, that, too, but, whe...\n",
      "19    [well, if, it, is, legalized, then, it, is, li...\n",
      "20    [i, think, like, if, it, does, get, legalized,...\n",
      "21    [or, if, you, think, about, it, it, is, if, th...\n",
      "22                     [what, do, you, mean, undreamed]\n",
      "23                           [for, example, cigarettes]\n",
      "24                                  [you, have, to, be]\n",
      "25    [like, you, have, to, be, but, people, that, a...\n",
      "26                  [i, have, not, seen, that, yea, ok]\n",
      "27    [i, have, seen, quite, young, children, smokin...\n",
      "28    [now, it, like, no, if, you, are, saying, like...\n",
      "29    [yeah, but, if, you, think, about, it, like, n...\n",
      "30    [unless, like, the, kid, wants, some, i, guess...\n",
      "31    [about, realization, and, so, no, it, should, ...\n",
      "32    [so, do, you, have, any, like, sons, against, ...\n",
      "33    [any, arguments, for, there, is, one, argument...\n",
      "34    [that, would, be, for, government, spending, t...\n",
      "35    [exactly, but, then, that, that, but, then, th...\n",
      "36    [that, is, that, is, the, only, like, good, af...\n",
      "37    [if, you, think, about, it, like, how, plenty,...\n",
      "38                            [because, like, no, like]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/25_2-0-2.txt\n",
      "0     [yeah, i, think, we, can, start, okay, so, uni...\n",
      "1     [i, totally, agree, because, in, that, way, li...\n",
      "2         [i, guess, we, debated, on, the, other, side]\n",
      "3     [so, the, opposing, view, i, guess, the, i, ha...\n",
      "4     [and, then, yeah, also, the, opposing, view, i...\n",
      "5     [because, then, if, it, happens, to, them, the...\n",
      "6     [and, it, is, like, we, should, look, at, cana...\n",
      "7     [well, you, yeah, and, another, thing, i, have...\n",
      "8     [yeah, i, guess, dont, like, that, its, collec...\n",
      "9                                     [i, know, it, is]\n",
      "10    [just, like, now, help, the, little, people, b...\n",
      "11    [yeah, it, is, yeah, i, just, yeah, i, think, ...\n",
      "12    [i, guess, would, it, effect, the, medical, pe...\n",
      "13    [i, think, it, does, i, think, i, remember, wa...\n",
      "14    [doctor, 's, will, not, like, that, true, that...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/29_2-1-2.txt\n",
      "0     [yeah, i, mean, legalized, marijuana, right, y...\n",
      "1     [i, think, they, should, legalized, marijuana,...\n",
      "2     [yeah, and, then, you, can, make, money, off, ...\n",
      "3     [exactly, i, think, it, would, be, a, little, ...\n",
      "4     [yeah, i, mean, it, helps, to, like, medially,...\n",
      "5           [i, mean, why, make, it, harder, for, them]\n",
      "6     [yeah, yeah, right, it, is, on, na, be, on, th...\n",
      "7     [yeah, i, mean, i, feel, like, people, are, ju...\n",
      "8     [anyway, we, get, the, most, out, of, it, for,...\n",
      "9     [so, i, mean, make, high, tax, on, it, everybo...\n",
      "10                                     [that, is, true]\n",
      "11    [i, know, i, am, thinking, compromise, already...\n",
      "12    [and, then, it, is, different, from, the, othe...\n",
      "13    [it, does, not, kill, you, i, mean, exactly, s...\n",
      "14                               [yeah, that, is, true]\n",
      "15              [i, do, not, know, what, else, to, say]\n",
      "16    [yeah, how, are, we, supposed, to, oppose, thi...\n",
      "17    [so, other, people, would, probably, say, well...\n",
      "18    [yeah, too, and, i, mean, then, if, they, lega...\n",
      "19    [again, free, will, people, should, do, what, ...\n",
      "20    [and, then, people, have, their, own, tastes, ...\n",
      "21    [yeah, there, is, still, people, who, are, not...\n",
      "22    [on, na, be, there, like, the, high, risk, fac...\n",
      "23          [i, do, not, know, what, would, corps, say]\n",
      "24                                   [i, do, not, know]\n",
      "25    [i, do, not, know, i, feel, like, there, is, s...\n",
      "26    [that, even, do, it, themselves, probably, get...\n",
      "27    [yeah, i, mean, hello, am, pretty, sure, they,...\n",
      "28    [i, know, right, it, is, not, your, responsibi...\n",
      "29    [yeah, it, is, your, responsibility, we, shoul...\n",
      "30    [yeah, same, thing, with, medicine, like, how,...\n",
      "31    [exactly, like, just, put, a, warning, on, it,...\n",
      "32                            [it, kills, brain, cells]\n",
      "33         [i, guess, yeah, but, again, that, is, like]\n",
      "34    [but, then, that, is, controversial, too, beca...\n",
      "35    [and, then, again, it, is, still, those, peopl...\n",
      "36    [that, is, true, that, is, true, it, is, not, ...\n",
      "37    [exactly, unless, you, like, poured, some, che...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/3_2-1-3.txt\n",
      "0                                [what, do, you, think]\n",
      "1     [i, definitely, think, that, well, i, am, not,...\n",
      "2                                [i, think, that, well]\n",
      "3     [do, you, think, it, should, be, lowered, to, ...\n",
      "4     [i, think, it, should, i, do, not, know, like,...\n",
      "5     [it, should, not, be, okay, is, that, what, yo...\n",
      "6     [it, is, like, when, my, friend, and, i, went,...\n",
      "7     [but, do, you, think, that, would, make, more,...\n",
      "8     [you, can, not, drink, well, everyone, has, th...\n",
      "9     [cause, i, feel, like, it, is, not, like, when...\n",
      "10    [you, have, been, drinking, i, also, think, th...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/31_1-1-3.txt\n",
      "0    [i, put, yes, but, i, was, kind, of, indiffere...\n",
      "1    [i, think, they, they, should, not, be, becaus...\n",
      "2    [i, do, not, i, just, thought, it, was, like, ...\n",
      "3    [ah, yeah, i, do, not, know, i, guess, i, am, ...\n",
      "4    [yeah, even, though, like, i, i, do, understan...\n",
      "5    [yeah, i, think, there, is, a, lot, of, loopho...\n",
      "6    [yeah, and, then, also, i, think, like, when, ...\n",
      "7    [i, think, it, also, depends, on, what, the, g...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/21_2-1-2.txt\n",
      "0     [okay, so, oh, marijuana, what, is, your, opin...\n",
      "1     [if, they, were, to, actually, legalized, it, ...\n",
      "2     [exactly, like, everyone, would, want, to, use...\n",
      "3     [i, think, if, it, is, for, medical, purposes,...\n",
      "4     [it, is, still, like, to, like, you, just, do,...\n",
      "5     [yeah, i, think, well, from, where, i, am, fro...\n",
      "6     [yeah, it, is, it, is, more, like, people, are...\n",
      "7     [okay, to, convince, someone, to, legalized, i...\n",
      "8                                 [there, is, like, no]\n",
      "9     [well, for, me, that, in, fact, in, the, persp...\n",
      "10    [like, why, like, it, is, not, that, important...\n",
      "11    [yeah, and, if, it, it, is, more, than, they, ...\n",
      "12    [but, it, is, not, necessary, to, actually, ha...\n",
      "13       [yeah, for, like, everybody, for, the, public]\n",
      "14    [like, there, is, other, meditations, you, can...\n",
      "15    [and, like, just, in, heart, you, meant, that,...\n",
      "16    [it, sounds, a, lot, like, an, excuse, to, get...\n",
      "17    [i, guess, an, argument, could, be, it, is, re...\n",
      "18                         [and, it, comes, with, that]\n",
      "19                              [it, comes, with, that]\n",
      "20          [it, comes, with, easing, i, do, not, know]\n",
      "21                      [easing, stress, or, something]\n",
      "22    [do, you, think, it, would, get, like, kind, o...\n",
      "23    [i, think, so, like, at, first, i, think, ther...\n",
      "24                        [may, be, i, should, try, it]\n",
      "25    [yeah, maybe, i, should, try, it, it, is, not,...\n",
      "26    [do, think, it, would, like, oh, yeah, well, w...\n",
      "27    [yeah, it, is, just, like, peer, pressure, so,...\n",
      "28    [so, what, would, you, like, let, us, say, lik...\n",
      "29    [yeah, because, they, already, though, of, eve...\n",
      "30    [and, then, there, is, like, other, stuff, you...\n",
      "31    [i, just, though, of, one, some, people, say, ...\n",
      "32    [and, then, for, like, example, let, us, just,...\n",
      "33    [i, am, not, sure, i, think, its, i, think, it...\n",
      "34                [well, that, can, be, a, problem, ha]\n",
      "35    [kind, of, like, another, one, if, people, gro...\n",
      "36    [like, that, would, be, like, a, up, for, bein...\n",
      "37    [so, smaller, business, that, are, like, just,...\n",
      "38    [more, business, hata, let, us, just, say, lik...\n",
      "39    [i, think, in, colorado, or, somewhere, i, thi...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/35_1-0-2.txt\n",
      "0     [ok, well, i, am, kind, of, like, i, could, un...\n",
      "1     [well, i, think, that, it, should, be, more, a...\n",
      "2     [yeah, i, mean, i, get, what, you, are, saying...\n",
      "3     [but, i, mean, like, how, would, you, like, go...\n",
      "4     [yeah, that, is, true, hata, that, is, why, i,...\n",
      "5     [yeah, but, they, do, give, out, like, the, li...\n",
      "6     [yeah, but, it, is, like, it, is, like, a, lon...\n",
      "7     [but, yeah, i, still, think, it, should, be, l...\n",
      "8     [yeah, like, once, every, like, nine, years, h...\n",
      "9     [because, i, think, if, you, are, like, if, yo...\n",
      "10    [there, are, i, mean, i, am, not, trying, to, ...\n",
      "11               [can, you, repeat, that, i, am, sorry]\n",
      "12    [ok, hata, like, if, you, if, there, is, someo...\n",
      "13    [but, like, what, you, were, saying, about, th...\n",
      "14                    [hata, like, what, do, you, mean]\n",
      "15    [no, because, it, is, like, because, then, lik...\n",
      "16    [yeah, i, mean, i, understand, you, are, not, ...\n",
      "17    [yeah, but, i, mean, that, just, puts, more, o...\n",
      "18    [yeah, but, whatever, they, are, here, to, hel...\n",
      "19    [because, it, is, easier, like, true, well, th...\n",
      "20    [no, i, know, but, the, school, whole, choosin...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/18_1-1-2.txt\n",
      "0                       [well, what, is, your, opinion]\n",
      "1                                    [i, am, pro, life]\n",
      "2     [i, am, fairly, neutral, but, i, think, if, i,...\n",
      "3     [okay, that, makes, sense, but, like, i, do, n...\n",
      "4     [so, you, said, like, you, said, something, ab...\n",
      "5     [but, but, at, the, same, time, like, they, do...\n",
      "6     [i, guess, that, goes, to, the, question, of, ...\n",
      "7     [but, it, is, still, taking, away, a, life, be...\n",
      "8     [do, you, think, it, is, necessarily, right, t...\n",
      "9     [you, could, say, the, same, thing, about, ado...\n",
      "10    [would, you, can, not, you, see, or, would, yo...\n",
      "11                                [what, do, you, mean]\n",
      "12    [so, like, if, you, take, away, a, child, from...\n",
      "13    [i, mean, yes, but, she, should, have, i, mean...\n",
      "14    [i, mean, there, are, like, what, is, it, ther...\n",
      "15    [i, mean, let, us, say, it, is, like, an, acci...\n",
      "16                   [it, is, not, the, car, 's, fault]\n",
      "17    [i, am, trying, to, think, of, a, good, analog...\n",
      "18    [i, think, that, just, goes, back, to, what, i...\n",
      "19    [well, okay, well, once, the, zygoma, forms, l...\n",
      "20                                        [yes, it, is]\n",
      "21    [but, have, you, ever, watched, the, movie, june]\n",
      "22                               [no, i, have, not, no]\n",
      "23    [you, have, not, that, is, such, a, good, movi...\n",
      "24    [i, guess, when, you, take, it, apart, from, t...\n",
      "25    [we, are, talking, about, two, different, thin...\n",
      "26                       [i, know, but, that, is, true]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/14_2-1-2.txt\n",
      "0     [well, i, personally, think, that, from, every...\n",
      "1     [yeah, i, i, completely, agree, with, you, yea...\n",
      "2     [also, another, thing, that, i, along, the, li...\n",
      "3     [yeah, i, also, think, like, like, by, legaliz...\n",
      "4     [most, definitely, one, of, the, things, also,...\n",
      "5                                      [that, is, true]\n",
      "6     [well, like, it, all, like, just, i, do, not, ...\n",
      "7     [yeah, you, do, not, know, the, damage, that, ...\n",
      "8     [and, i, think, that, is, on, na, be, one, of,...\n",
      "9     [yea, it, probably, would, cause, more, damage...\n",
      "10    [and, i, think, it, is, just, it, is, a, drug,...\n",
      "11    [yeah, i, agree, yeah, i, think, will, just, l...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/7_1-1-3.txt\n",
      "0     [are, you, first, or, what, are, you, against,...\n",
      "1                    [for, realization, i, am, for, it]\n",
      "2          [so, you, are, saying, yes, to, realization]\n",
      "3          [yes, and, you, are, saying, no, i, presume]\n",
      "4                         [okay, okay, you, start, now]\n",
      "5     [you, can, go, ok, well, i, think, that, marij...\n",
      "6     [but, then, there, is, still, i, mean, i, am, ...\n",
      "7     [how, old, do, you, have, to, be, to, smoke, c...\n",
      "8     [eighteen, right, yeah, yea, but, then, there,...\n",
      "9     [then, why, is, alcohol, and, tobacco, legaliz...\n",
      "10    [not, automatically, but, they, can, die, off,...\n",
      "11    [but, that, is, that, is, their, choice, they,...\n",
      "12    [even, if, like, and, in, class, like, if, a, ...\n",
      "13    [but, but, i, mean, i, dont, i, do, not, do, i...\n",
      "14              [they, will, keep, going, but, i, mean]\n",
      "15    [but, it, will, only, be, the, people, who, wa...\n",
      "16    [but, then, there, is, people, who, have, the,...\n",
      "17                [i, do, not, believe, that, is, true]\n",
      "18                    [some, people, would, like, some]\n",
      "19    [i, do, not, think, that, like, affects, their...\n",
      "20    [well, some, people, might, again, like, it, i...\n",
      "21    [but, is, that, the, only, reason, because, it...\n",
      "22    [because, when, you, are, education, the, youn...\n",
      "23    [then, what, happened, with, i, think, it, was...\n",
      "24    [but, then, if, you, look, at, it, like, i, am...\n",
      "25    [do, you, do, you, do, you, know, people, in, ...\n",
      "26    [i, i, mean, i, never, expected, people, to, d...\n",
      "27    [but, you, know, so, it, being, legal, or, ill...\n",
      "28                   [not, everything, not, everything]\n",
      "29    [yeah, not, everything, but, you, know, there,...\n",
      "30    [well, the, real, damage, is, to, the, whole, ...\n",
      "31    [is, tobacco, a, drug, so, what, difference, w...\n",
      "32    [i, mean, okay, you, know, how, like, now, lik...\n",
      "33                                    [i, i, think, so]\n",
      "34    [okay, so, like, they, put, it, into, like, wh...\n",
      "35    [so, like, everyone, is, going, to, eat, marij...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/21_1-0-3.txt\n",
      "0     [i, feel, that, the, death, penalty, that, the...\n",
      "1     [i, kind, of, oppose, that, i, do, not, think,...\n",
      "2     [i, am, not, saying, death, penalty, right, aw...\n",
      "3     [that, if, you, are, saying, that, you, know, ...\n",
      "4     [are, very, rare, rare, but, i, do, not, think...\n",
      "5     [yeah, that, that, is, what, i, am, saying, bu...\n",
      "6     [and, how, would, they, like, you, said, they,...\n",
      "7     [no, cure, for, killing, but, if, they, have, ...\n",
      "8     [i, mean, like, how, people, they, people, get...\n",
      "9     [that, is, what, i, am, saying, that, life, if...\n",
      "10    [but, then, that, would, be, more, money, that...\n",
      "11    [yeah, i, mean, they, spend, more, money, on, ...\n",
      "12    [i, do, not, know, i, just, think, that, the, ...\n",
      "13    [i, mean, yeah, i, mean, if, if, your, state, ...\n",
      "14    [i, am, not, saying, it, even, it, out, but, i...\n",
      "15    [well, compared, to, that, they, do, not, i, d...\n",
      "16    [i, just, feel, in, my, heart, that, the, deat...\n",
      "17    [yeah, yeah, again, i, am, i, am, not, saying,...\n",
      "18    [but, you, say, you, are, saying, basically, y...\n",
      "19    [i, do, not, no, beating, no, i, would, i, wou...\n",
      "20    [that, would, make, them, insane, like, not, h...\n",
      "21    [that, is, kind, of, something, that, then, th...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/34_1-1-2.txt\n",
      "0    [i, think, we, s, we, should, not, lower, the,...\n",
      "1    [okay, the, first, part, that, you, said, abou...\n",
      "2    [i, think, when, people, grow, up, to, they, w...\n",
      "3    [i, guess, so, let, us, see, i, know, that, li...\n",
      "4    [i, think, we, just, do, like, what, we, are, ...\n",
      "5    [have, you, seen, any, of, the, news, about, i...\n",
      "6            [they, do, a, lot, of, news, about, that]\n",
      "7    [especially, when, friends, act, like, that, i...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/34_2-0-2.txt\n",
      "0     [well, i, feel, i, mean, i, feel, like, mariju...\n",
      "1     [lung, cancer, i, mean, that, is, genetic, tho...\n",
      "2     [yeah, but, i, mean, for, i, mean, what, i, am...\n",
      "3     [that, is, true, right, i, totally, agree, wit...\n",
      "4     [well, i, mean, i, i, think, it, is, an, inter...\n",
      "5     [right, right, and, i, am, thinking, if, we, l...\n",
      "6     [yeah, definitely, i, was, thinking, that, act...\n",
      "7              [so, decriminalize, marijuana, now, now]\n",
      "8     [yeah, yeah, most, people, i, know, that, i, m...\n",
      "9     [s, right, suppressed, that, violent, or, that...\n",
      "10    [it, definitely, has, it, is, benefits, i, i, ...\n",
      "11    [right, right, that, is, true, wells, there, a...\n",
      "12    [well, i, mean, as, a, counter, argument, peop...\n",
      "13    [other, drugs, do, not, believe, it, that, is,...\n",
      "14    [yeah, that, is, all, that, is, a, personal, c...\n",
      "15    [yeah, yeah, that, is, true, yeah, yeahwhat, o...\n",
      "16    [i, do, not, know, well, i, do, not, know, i, ...\n",
      "17    [so, what, is, wrong, yeah, me, too, yeah, i, ...\n",
      "18    [yeah, and, they, are, all, i, mean, they, are...\n",
      "19    [yeah, they, are, yeah, yeah, i, think, it, is...\n",
      "20                             [yeah, yeah, definitely]\n",
      "21    [what, else, oh, the, only, bad, thing, about,...\n",
      "22    [tobacco, tobacco, is, pretty, horrible, i, me...\n",
      "23    [yeah, and, just, think, about, a, lot, of, pe...\n",
      "24    [let, me, think, do, you, think, it, would, be...\n",
      "25    [maybe, sang, that, is, good, maybe, just, see...\n",
      "26    [yeah, and, that, is, a, that, is, another, hu...\n",
      "27    [right, and, also, those, people, that, like, ...\n",
      "28    [yeah, they, probably, come, out, of, into, so...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/38_1-0-2.txt\n",
      "0     [pro, choice, on, abortion, i, i, am, not, so,...\n",
      "1     [i, do, not, know, because, you, know, how, li...\n",
      "2     [again, what, was, i, just, saying, like, i, j...\n",
      "3     [yeah, because, well, like, i, i, think, it, i...\n",
      "4     [so, there, there, is, certain, aspects, of, t...\n",
      "5     [yeah, like, you, should, get, checked, like, ...\n",
      "6     [yeah, well, everything, again, everything, 's...\n",
      "7     [yeah, and, then, like, people, argue, like, t...\n",
      "8     [but, then, the, whole, religious, aspect, lik...\n",
      "9     [yeah, because, i, mean, but, like, when, you,...\n",
      "10    [okay, so, i, would, be, not, nationalizing, t...\n",
      "11    [well, a, religious, agenda, like, pretty, muc...\n",
      "12    [yeah, but, i, feel, like, it, is, really, har...\n",
      "13                                    [that, you, yeah]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/15_1-1-3.txt\n",
      "0                       [okay, so, what, did, you, say]\n",
      "1     [so, you, know, do, not, think, it, should, be...\n",
      "2     [yeah, i, can, see, that, yeah, well, like, i,...\n",
      "3     [yeah, i, know, and, it, is, sort, of, like, t...\n",
      "4     [may, well, i, do, not, think, so, yeah, well,...\n",
      "5     [yeah, definitely, yeah, definitely, i, think,...\n",
      "6     [they, are, so, hostile, yeah, but, i, think, ...\n",
      "7     [yeah, californians, people, from, california,...\n",
      "8     [the, south, yeah, definitely, yeah, i, do, no...\n",
      "9     [you, know, perhaps, these, you, know, perhaps...\n",
      "10    [yeah, but, well, yeah, if, it, is, a, very, h...\n",
      "11    [yeah, here, it, is, not, so, much, an, issue,...\n",
      "12    [yeah, they, are, very, they, are, very, very,...\n",
      "13    [eh, kid, does, not, really, have, a, choice, ...\n",
      "14    [i, know, but, like, i, just, said, like, you,...\n",
      "15    [but, it, is, just, economic, it, is, just, th...\n",
      "16    [okay, well, yeah, that, is, true, but, okay, ...\n",
      "17    [well, i, think, i, think, it, has, to, do, wi...\n",
      "18    [yeah, that, would, definitely, cause, yeah, i...\n",
      "19           [there, has, to, be, a, majority, i, feel]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/15_2-0-2.txt\n",
      "0     [i, do, not, think, it, should, be, lowered, h...\n",
      "1     [yeah, like, i, know, i, read, somewhere, that...\n",
      "2     [yeah, that, is, that, was, my, main, thing, l...\n",
      "3                       [and, then, opposing, argument]\n",
      "4     [now, what, would, what, would, i, guess, they...\n",
      "5     [i, guess, there, would, not, be, that, many, ...\n",
      "6     [okay, no, i, see, what, you, are, saying, tho...\n",
      "7     [and, what, is, the, point, of, lowering, it, ...\n",
      "8     [yeah, that, is, true, that, is, all, i, reall...\n",
      "9     [yeah, cause, i, know, they, actually, pay, le...\n",
      "10    [yeah, and, it, is, like, if, the, upper, clas...\n",
      "11    [like, it, is, not, on, na, hurt, them, to, pu...\n",
      "12    [yeah, no, exactly, yeah, but, opposing, views...\n",
      "13    [but, people, that, are, lower, class, are, al...\n",
      "14    [yeah, having, to, pay, more, i, guess, people...\n",
      "15    [but, the, yeah, they, are, already, paying, m...\n",
      "16    [i, am, just, trying, to, think, of, why, peop...\n",
      "17    [well, obviously, the, upper, class, people, w...\n",
      "18    [yeah, they, do, not, wan, na, spend, more, mo...\n",
      "19    [i, do, not, know, who, else, would, oppose, t...\n",
      "20    [yeah, i, would, all, see, them, opposing, to,...\n",
      "21    [it, is, like, it, is, been, that, way, for, s...\n",
      "22    [yeah, yeah, and, it, is, like, we, have, gott...\n",
      "23    [would, not, it, end, up, befitting, the, econ...\n",
      "24    [i, am, not, sure, i, do, not, know, i, wonder...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/19_2-1-2.txt\n",
      "0     [do, you, want, to, start, off, or, i, will, s...\n",
      "1     [okay, well, what, do, you, think, about, abor...\n",
      "2     [i, agree, that, abortion, is, wrong, just, be...\n",
      "3     [what, about, those, people, that, that, got, ...\n",
      "4     [well, then, at, the, same, time, if, somebody...\n",
      "5     [okay, but, do, not, you, think, like, what, a...\n",
      "6     [if, if, the, child, if, the, woman, is, sick,...\n",
      "7     [but, but, but, what, if, she, dies, while, gi...\n",
      "8     [while, giving, birth, i, believe, that, the, ...\n",
      "9     [what, about, if, the, mother, is, not, on, na...\n",
      "10    [there, there, is, a, lot, of, women, that, yo...\n",
      "11    [you, know, i, agree, with, you, i, i, also, d...\n",
      "12    [but, yet, life, is, already, suffereable, goi...\n",
      "13    [well, it, it, it, is, i, i, i, have, to, agre...\n",
      "14    [i, support, abortion, only, because, if, my, ...\n",
      "15    [in, a, normal, but, then, again, if, you, do,...\n",
      "16    [they, might, not, have, a, guidance, in, thei...\n",
      "17    [that, is, true, that, is, true, yeah, that, i...\n",
      "18    [we, are, all, living, organisms, anyway, so, ...\n",
      "19    [not, so, much, the, consequence, because, peo...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/19_1-0-2.txt\n",
      "0                   [what, is, your, opinion, on, this]\n",
      "1     [i, think, the, legal, drinking, age, should, ...\n",
      "2     [i, do, not, think, so, well, i, guess, when, ...\n",
      "3     [well, when, you, think, about, it, that, way,...\n",
      "4     [i, am, still, i, am, still, twenty, i, am, tw...\n",
      "5     [well, do, not, you, have, like, a, did, or, y...\n",
      "6     [yeah, some, people, do, not, though, that, is...\n",
      "7     [some, people, do, not, well, in, other, count...\n",
      "8     [yeah, that, is, true, yeah, i, think, that, i...\n",
      "9     [well, it, is, kind, of, my, idea, of, once, y...\n",
      "10    [i, really, think, if, if, they, are, going, t...\n",
      "11    [well, i, can, see, a, progressive, like, goin...\n",
      "12    [yeah, but, you, do, not, jump, it, straight, ...\n",
      "13    [well, i, can, see, a, progression, of, it, ra...\n",
      "14    [i, can, already, see, a, bunch, of, seniors, ...\n",
      "15                          [like, the, senior, picnic]\n",
      "16    [yeah, my, god, i, mean, if, they, are, going,...\n",
      "17    [yeah, well, there, has, always, been, laws, a...\n",
      "18    [yeah, you, can, not, have, alcohol, in, the, ...\n",
      "19    [like, not, instantly, but, progressively, it,...\n",
      "20    [this, generation, better, like, prove, themse...\n",
      "21    [well, that, or, it, could, be, a, huge, mista...\n",
      "22                                   [yeah, very, true]\n",
      "23    [lots, of, things, are, learned, from, that, g...\n",
      "24    [it, feels, really, weird, that, we, are, in, ...\n",
      "25    [well, it, depends, on, like, that, is, why, i...\n",
      "26    [yeah, that, is, why, i, am, saying, it, shoul...\n",
      "27    [one, funny, thing, there, is, worse, drugs, t...\n",
      "28    [yeah, there, is, worse, drugs, but, like, i, ...\n",
      "29    [it, kills, a, lot, more, than, all, the, othe...\n",
      "30    [it, kills, a, lot, more, yeah, and, now, ther...\n",
      "31    [good, combination, good, combination, yeah, l...\n",
      "32    [it, is, not, that, bad, of, a, choice, but, i...\n",
      "33    [it, just, needs, to, be, over, time, not, ins...\n",
      "34    [yeah, see, like, how, is, it, doing, if, you,...\n",
      "35    [the, only, thing, is, you, need, to, wait, fo...\n",
      "36    [yeah, it, is, that, is, a, topic, that, is, i...\n",
      "37    [it, is, the, law, like, the, whole, legalized...\n",
      "38                                   [yes, on, connais]\n",
      "39    [yeah, i, did, not, vote, at, all, because, i,...\n",
      "40    [yeah, i, do, that, too, when, i, like, do, no...\n",
      "41    [also, i, kind, of, do, not, want, to, have, t...\n",
      "42    [i, have, yet, to, be, summoned, and, we, are,...\n",
      "43    [because, that, was, one, of, the, other, topics]\n",
      "44    [that, was, one, of, the, other, topics, i, am...\n",
      "45    [i, am, guessing, well, i, think, it, should, ...\n",
      "46    [you, can, not, but, you, can, have, a, lot, o...\n",
      "47    [you, can, well, it, can, affect, the, memory,...\n",
      "48    [but, if, you, are, a, chronic, user, of, it, ...\n",
      "49    [no, it, is, not, going, to, affect, you, as, ...\n",
      "50    [that, is, nothing, alcohol, is, alcohol, is, ...\n",
      "51    [you, can, actually, get, a, due, from, drivin...\n",
      "52    [yeah, it, would, still, be, under, the, influ...\n",
      "53    [eh, some, day, we, will, find, out, what, hap...\n",
      "54    [honestly, knowing, that, this, generation, is...\n",
      "55    [well, is, not, it, already, in, california, t...\n",
      "56    [yeah, for, medical, needs, which, is, i, am, ...\n",
      "57    [well, have, you, ever, seen, the, southward, ...\n",
      "58                    [people, do, that, for, adderall]\n",
      "59                               [adderall, the, whole]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/20_2-0-3.txt\n",
      "0     [do, you, think, they, should, lower, the, age...\n",
      "1                                   [yes, they, should]\n",
      "2     [i, think, they, should, too, because, i, mean...\n",
      "3     [and, then, your, allowed, to, go, to, war, an...\n",
      "4     [yeah, your, allowed, to, get, a, gun, why, ca...\n",
      "5     [yeah, it, is, kind, of, like, marijuana, it, ...\n",
      "6     [it, is, illegal, they, still, do, it, because...\n",
      "7     [yeah, and, if, they, lower, the, age, then, m...\n",
      "8     [they, would, be, like, well, there, is, someb...\n",
      "9     [it, will, create, less, struggle, for, the, g...\n",
      "10    [and, then, college, students, are, those, one...\n",
      "11    [yeah, and, then, i, feel, there, is, less, re...\n",
      "12    [make, everything, twentyone, and, make, every...\n",
      "13                     [yeah, it, is, just, dumb, yeah]\n",
      "14    [and, what, do, you, think, like, the, opposin...\n",
      "15    [their, immature, well, obviously, we, got, ok...\n",
      "16    [because, we, are, if, we, are, mature, enough...\n",
      "17    [yeah, and, go, to, strip, clubs, do, not, go,...\n",
      "18    [like, a, lot, of, eighteen, year, old, do, it...\n",
      "19    [all, the, time, they, do, not, need, to, show...\n",
      "20    [the, 're, allowed, to, buy, a, house, can, no...\n",
      "21    [they, are, allowed, to, be, independent, if, ...\n",
      "22    [yeah, i, mean, it, is, it, is, dumb, it, is, ...\n",
      "23    [you, could, get, cancer, i, think, and, then,...\n",
      "24    [but, it, is, bad, for, your, liver, and, then...\n",
      "25    [i, do, not, know, but, you, can, not, drink, ...\n",
      "26    [yeah, or, if, your, a, reckless, driver, you,...\n",
      "27    [they, could, make, a, test, of, maturity, so,...\n",
      "28    [have, a, license, to, drink, well, they, do, ...\n",
      "29    [yeah, i, think, they, should, they, should, j...\n",
      "30    [yes, because, college, students, do, not, hav...\n",
      "31    [i, mean, that, is, the, problem, that, they, ...\n",
      "32     [put, the, prices, up, because, really, alcohol]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/12_1-0-2.txt\n",
      "0     [okay, so, do, you, think, marijuana, should, ...\n",
      "1                          [yeah, i, think, it, should]\n",
      "2      [why, do, you, think, it, should, be, legalized]\n",
      "3     [i, think, it, should, be, legalized, because,...\n",
      "4     [personally, i, think, it, should, not, be, le...\n",
      "5     [no, absolutely, not, honestly, that, no, i, f...\n",
      "6     [so, but, one, of, your, reasons, was, up, to,...\n",
      "7     [it, is, legal, yeah, that, is, why, i, am, sa...\n",
      "8     [so, basically, your, main, point, is, for, th...\n",
      "9              [yeah, that, would, be, my, main, point]\n",
      "10    [i, mean, that, is, always, a, good, thing, si...\n",
      "11                              [what, are, your, sons]\n",
      "12    [i, already, listed, them, yeah, and, i, yeah,...\n",
      "13    [i, think, some, i, think, it, just, really, d...\n",
      "14    [yeah, maybe, if, i, had, more, first, hand, e...\n",
      "15    [i, am, yeah, yeah, i, just, feel, like, like,...\n",
      "16    [so, if, you, were, to, legalized, it, how, do...\n",
      "17    [i, do, not, know, i, feel, like, it, would, i...\n",
      "18    [like, i, do, not, i, am, not, too, familiar, ...\n",
      "19    [okay, well, basically, like, if, you, are, if...\n",
      "20    [well, so, taking, into, perspective, a, perso...\n",
      "21    [it, just, depends, on, the, person, i, guess,...\n",
      "22    [well, although, marijuana, is, on, a, smaller...\n",
      "23           [yeah, i, agree, like, cocain, and, stuff]\n",
      "24    [but, so, i, do, not, think, that, is, a, viab...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/12_2-1-2.txt\n",
      "0    [i, think, well, i, am, prochoice, on, abortio...\n",
      "1    [that, is, true, i, agree, with, you, because,...\n",
      "2    [and, then, other, people, might, say, but, wh...\n",
      "3    [also, even, though, sometimes, people, might,...\n",
      "4    [and, i, feel, like, if, you, are, not, put, i...\n",
      "5    [what, i, also, like, like, i, have, friends, ...\n",
      "6    [also, to, us, and, the, one, of, the, argumen...\n",
      "7    [that, can, be, overwhelming, and, and, then, ...\n",
      "8    [they, just, take, him, right, off, and, then,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/33_1-0-3.txt\n",
      "0     [honestly, abortion, okay, that, i, believe, n...\n",
      "1     [because, i, heard, that, there, is, like, ser...\n",
      "2     [yeah, that, is, true, though, too, but, then,...\n",
      "3     [if, you, do, not, want, to, but, if, you, thi...\n",
      "4            [that, is, true, though, it, is, so, true]\n",
      "5     [if, you, want, to, like, you, are, having, a,...\n",
      "6     [that, is, true, that, is, but, then, then, ag...\n",
      "7     [because, there, are, like, services, to, stop...\n",
      "8     [yeah, that, is, true, also, there, are, servi...\n",
      "9     [a, lot, to, do, so, yeah, want, to, change, a...\n",
      "10    [that, is, good, better, than, the, last, one,...\n",
      "11    [i, put, yes, or, lower, the, age, yeah, yeah,...\n",
      "12    [you, really, it, should, be, lowered, why, do...\n",
      "13    [no, no, there, is, no, difference, yeah, they...\n",
      "14    [but, i, feel, like, it, is, in, an, asia, cul...\n",
      "15                          [yeah, yeah, asia, culture]\n",
      "16    [the, thing, is, asia, culture, has, more, lik...\n",
      "17    [because, there, are, like, examples, that, in...\n",
      "18    [yeah, that, is, true, though, that, is, a, go...\n",
      "19    [because, like, i, think, the, age, is, like, ...\n",
      "20    [that, is, true, also, but, yeah, i, feel, tha...\n",
      "21    [i, put, yes, because, they, have, more, incom...\n",
      "22    [yeah, i, believe, so, too, but, then, it, sho...\n",
      "23    [because, it, depends, on, how, much, you, ear...\n",
      "24    [yeah, it, does, balance, yeah, it, balances, ...\n",
      "25    [that, is, the, last, view, for, drinking, you...\n",
      "26    [yeah, that, is, true, that, is, what, you, be...\n",
      "27    [yeah, so, i, think, it, does, not, make, any,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/27_1-0-3.txt\n",
      "0             [okay, what, do, you, think, about, that]\n",
      "1     [well, i, feel, like, the, death, penalty, it,...\n",
      "2           [like, certain, cases, what, do, you, mean]\n",
      "3     [like, well, like, murder, that, kind, of, stu...\n",
      "4     [yes, because, i, think, like, everybody, dese...\n",
      "5     [yeah, like, i, have, also, heard, stories, ab...\n",
      "6     [like, but, the, cases, were, actually, they, ...\n",
      "7     [make, a, mistake, okay, clarify, on, that, li...\n",
      "8     [yeah, actually, i, would, i, would, rather, l...\n",
      "9     [okay, yeah, as, i, said, like, and, then, the...\n",
      "10    [but, like, are, you, looking, at, the, eviden...\n",
      "11    [i, think, it, should, be, combined, with, evi...\n",
      "12                [and, just, see, if, the, person, is]\n",
      "13    [yeah, and, i, mean, if, like, they, could, be...\n",
      "14    [but, i, actually, want, to, ask, you, a, ques...\n",
      "15                            [is, not, enough, it, is]\n",
      "16           [like, for, those, people, you, mentioned]\n",
      "17    [for, those, people, like, there, are, some, c...\n",
      "18    [but, do, you, think, like, when, a, murderer,...\n",
      "19    [do, they, think, about, the, penalty, sometim...\n",
      "20    [so, you, are, saying, that, like, actually, t...\n",
      "21    [it, is, you, know, it, is, probably, like, yo...\n",
      "22    [okay, so, so, like, let, me, clarify, what, y...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/1_2-0-2.txt\n",
      "0     [did, we, start, okay, i, do, believe, we, sho...\n",
      "1     [that, is, crazy, i, think, we, should, too, b...\n",
      "2     [yeah, i, think, examples, of, it, are, like, ...\n",
      "3     [live, with, taxes, high, it, benefits, everyb...\n",
      "4     [i, like, i, do, not, know, the, specific, cou...\n",
      "5      [wait, is, not, it, near, france, or, somewhere]\n",
      "6                                    [i, am, not, sure]\n",
      "7          [began, had, us, watch, a, video, about, it]\n",
      "8     [nice, i, just, i, do, not, know, it, just, se...\n",
      "9     [and, people, are, just, left, on, the, street...\n",
      "10    [i, think, that, is, just, like, bad, i, agree...\n",
      "11    [but, how, much, would, tax, rise, how, much, ...\n",
      "12    [that, is, definitely, like, the, main, issue,...\n",
      "13    [but, do, not, you, get, your, tax, back, like...\n",
      "14    [yeah, i, mean, pretty, much, yeah, if, you, a...\n",
      "15    [yeah, it, is, like, hospitals, can, only, hel...\n",
      "16    [exactly, yeah, so, it, is, like, it, helps, b...\n",
      "17    [about, your, brother, so, like, was, there, l...\n",
      "18    [you, just, can, not, get, health, insurance, ...\n",
      "19            [that, is, crazy, yeah, free, healthcare]\n",
      "20    [yeah, i, do, not, know, how, much, longer, ke...\n",
      "21    [rich, people, do, not, like, to, pay, taxes, ...\n",
      "22    [yeah, yeah, i, mean, yeah, that, is, pretty, ...\n",
      "23    [yeah, because, they, feel, like, they, are, m...\n",
      "24    [why, would, i, be, against, it, yeah, probabl...\n",
      "25    [but, there, are, people, that, work, hard, fo...\n",
      "26    [yeah, that, is, true, exactly, i, think, that...\n",
      "27    [well, my, worked, as, a, nurse, for, like, a,...\n",
      "28    [yeah, yeah, yeah, conservatives, seem, to, th...\n",
      "29    [i, think, when, you, become, like, a, college...\n",
      "30    [yeah, i, paid, two, health, insurance, i, pai...\n",
      "31    [you, have, to, have, to, schools, you, can, g...\n",
      "32    [are, you, yeah, i, do, not, know, i, do, not,...\n",
      "33    [that, is, why, it, should, just, be, free, be...\n",
      "34    [yeah, i, know, over, in, canada, it, is, not,...\n",
      "35    [they, just, go, to, the, hospital, okay, i, w...\n",
      "36    [yeah, that, crazy, yeah, that, is, a, pretty,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/33_2-1-2.txt\n",
      "0     [so, yeah, i, think, that, the, rich, should, ...\n",
      "1     [yeah, i, be, i, believe, that, as, well, at, ...\n",
      "2     [yeah, no, i, agree, i, think, that, well, tho...\n",
      "3     [and, until, we, get, out, of, the, debt, i, f...\n",
      "4     [yeah, i, know, i, mean, and, with, the, rich,...\n",
      "5     [their, house, or, something, yeah, and, yeah,...\n",
      "6     [exactly, yeah, the, middle, class, should, be...\n",
      "7     [exactly, i, feel, i, feel, like, that, as, we...\n",
      "8     [i, mean, i, would, say, that, increasing, the...\n",
      "9     [so, i, feel, like, if, we, were, presenting, ...\n",
      "10    [yeah, no, i, agree, yeah, absolutely, we, nee...\n",
      "11    [yeah, i, think, right, now, the, the, cap, li...\n",
      "12    [yeah, i, believe, it, is, something, i, think...\n",
      "13    [i, think, it, is, like, that, so, do, you, th...\n",
      "14    [i, would, say, that, it, should, probably, st...\n",
      "15    [you, think, it, should, stay, there, okay, ye...\n",
      "16    [that, is, i, mean, yeah, that, is, definitely...\n",
      "17    [yeah, i, get, you, i, get, what, you, are, tr...\n",
      "18    [no, yeah, i, agree, but, yeah, other, than, t...\n",
      "19                                     [yeah, i, agree]\n",
      "20    [so, i, do, not, know, i, mean, other, than, t...\n",
      "21    [anything, else, let, us, see, so, they, shoul...\n",
      "22    [yeah, definitely, you, know, raising, the, li...\n",
      "23    [and, just, until, until, we, get, out, of, ou...\n",
      "24    [yeah, you, are, right, yeah, and, lowering, t...\n",
      "25    [things, like, yeah, the, those, are, so, thos...\n",
      "26    [it, will, allow, for, financial, stability, o...\n",
      "27    [do, you, wait, do, you, know, how, the, break...\n",
      "28    [i, honestly, do, not, no, yeah, yeah, i, am, ...\n",
      "29    [from, what, i, think, i, think, from, what, i...\n",
      "30    [yeah, no, it, is, i, i, am, not, sure, what, ...\n",
      "31    [but, i, feel, like, i, feel, like, they, shou...\n",
      "32    [you, think, that, no, yeah, i, get, what, you...\n",
      "33    [i, i, i, can, guarantee, a, lot, of, people, ...\n",
      "34    [another, point, we, could, argue, is, that, a...\n",
      "35    [that, that, is, a, good, point, i, did, not, ...\n",
      "36                             [yeah, definitely, yeah]\n",
      "37    [well, what, else, do, we, have, so, we, need,...\n",
      "38                    [i, would, yeah, i, would, agree]\n",
      "39    [okay, to, increase, the, k, by, whatever, num...\n",
      "40    [just, that, it, would, allow, for, i, mean, a...\n",
      "41    [yeah, and, then, we, would, list, their, poin...\n",
      "42    [yeah, i, mean, just, if, you, have, like, a, ...\n",
      "43    [and, the, poor, and, the, poor, getting, poor...\n",
      "44                          [yeah, and, definitely, no]\n",
      "45                             [yeah, it, makes, sense]\n",
      "46                        [yeah, i, do, not, know, xxx]\n",
      "47    [i, am, not, sure, either, there, us, see, why...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/32_2-0-3.txt\n",
      "0                            [so, what, do, you, think]\n",
      "1                          [i, said, to, legalized, it]\n",
      "2                                   [to, legalized, it]\n",
      "3                            [what, did, you, said, no]\n",
      "4                                     [no, i, said, no]\n",
      "5                         [why, did, you, choose, that]\n",
      "6     [well, you, know, just, because, for, you, kno...\n",
      "7     [other, bad, things, i, know, what, you, mean,...\n",
      "8                         [which, is, legal, for, them]\n",
      "9     [yeah, but, somehow, people, will, just, get, ...\n",
      "10    [yeah, and, then, we, will, always, find, a, w...\n",
      "11                                       [i, get, that]\n",
      "12    [and, because, you, know, it, could, ruin, kis...\n",
      "13    [and, i, was, thinking, that, too, about, that...\n",
      "14    [right, but, like, let, us, say, you, know, th...\n",
      "15                               [but, not, as, easily]\n",
      "16    [not, as, easily, but, it, could, be, cheaper,...\n",
      "17    [i, am, on, the, spot, i, think, i, should, ch...\n",
      "18    [which, is, always, a, problem, for, kiss, to,...\n",
      "19    [yeah, that, is, true, do, you, think, it, sho...\n",
      "20    [the, xxx, and, yeah, and, the, media, stuff, ...\n",
      "21    [they, make, it, seem, so, like, not, a, big, ...\n",
      "22    [accountable, and, yeah, which, is, why, it, h...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/13_2-0-2.txt\n",
      "0     [alright, i, think, or, yea, i, so, i, what, i...\n",
      "1     [that, is, what, i, think, like, i, do, not, k...\n",
      "2     [so, i, think, a, lot, of, people, they, would...\n",
      "3     [yeah, like, a, baby, you, can, not, take, awa...\n",
      "4     [but, and, that, that, is, and, that, is, i, m...\n",
      "5                              [i, think, they, should]\n",
      "6     [yeah, definitely, they, should, be, able, to,...\n",
      "7     [its, like, they, did, not, even, want, it, li...\n",
      "8     [did, not, even, have, a, choice, even, and, t...\n",
      "9     [to, be, like, rapid, i, think, another, thing...\n",
      "10    [you, mean, like, when, they, are, when, they,...\n",
      "11             [yeah, like, really, really, big, flaws]\n",
      "12    [yeah, yeah, yeah, where, like, or, like, when...\n",
      "13    [yeah, i, feel, like, you, are, on, na, bring,...\n",
      "14    [exactly, and, i, think, that, is, that, is, a...\n",
      "15           [yeah, those, are, my, biggest, ones, too]\n",
      "16    [concerns, let, us, see, are, there, any, othe...\n",
      "17    [i, mean, i, just, think, that, when, it, is, ...\n",
      "18    [where, i, think, everybody, that, is, not, an...\n",
      "19    [yeah, and, that, well, everybody, is, entitle...\n",
      "20    [i, think, i, i, i, remember, i, am, trying, t...\n",
      "21    [i, think, i, think, it, might, be, like, if, ...\n",
      "22    [exactly, and, i, do, not, like, in, terms, of...\n",
      "23    [like, that, is, the, girl, that, is, carrying...\n",
      "24    [exactly, like, i, do, not, really, have, a, s...\n",
      "25    [let, us, see, what, else, could, we, talk, ab...\n",
      "26    [let, us, see, what, would, what, would, peopl...\n",
      "27    [they, are, just, on, na, say, how, like, you,...\n",
      "28    [that, is, true, that, is, true, and, like, wh...\n",
      "29    [yeah, they, are, not, cheap, and, obviously, ...\n",
      "30    [yeah, i, definitely, agree, with, that, any, ...\n",
      "31                     [i, know, like, the, main, ones]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/26_2-0-3.txt\n",
      "0     [what, do, you, think, about, same, sex, marri...\n",
      "1                                    [i, do, not, know]\n",
      "2     [gay, marriage, i, approve, of, it, just, beca...\n",
      "3     [okay, well, i, think, that, everybody, has, t...\n",
      "4     [i, think, like, you, said, it, is, religious,...\n",
      "5     [yeah, well, i, think, that, like, you, said, ...\n",
      "6     [that, is, true, but, then, again, in, this, d...\n",
      "7     [i, think, it, would, be, well, legally, marri...\n",
      "8     [so, then, you, do, not, oppose, gay, or, lesi...\n",
      "9     [i, oppose, it, in, the, church, i, think, tha...\n",
      "10    [but, why, does, it, why, does, it, matter, if...\n",
      "11    [i, think, well, in, the, bible, it, is, a, si...\n",
      "12    [i, do, not, know, to, much, about, religion, ...\n",
      "13                              [not, just, christians]\n",
      "14    [so, what, is, the, difference, between, that,...\n",
      "15    [okay, well, in, christianity, we, are, taught...\n",
      "16    [so, just, because, your, gay, does, not, been...\n",
      "17    [i, am, not, saying, that, i, am, saying, that...\n",
      "18    [so, between, an, individual, and, god, if, a,...\n",
      "19    [i, do, not, know, it, is, between, him, and, ...\n",
      "20    [so, then, again, like, you, said, it, is, bet...\n",
      "21    [but, if, their, christians, then, they, know,...\n",
      "22          [but, you, can, not, change, who, you, are]\n",
      "23           [i, am, not, saying, that, they, have, to]\n",
      "24    [so, your, going, to, withhold, the, right, of...\n",
      "25    [and, to, i, said, that, they, have, that, right]\n",
      "26    [but, it, is, like, it, is, like, during, like...\n",
      "27    [i, am, saying, that, they, have, that, right,...\n",
      "28    [to, get, married, right, that, is, what, your...\n",
      "29    [to, get, married, but, not, wholly, matrimony...\n",
      "30    [yeah, i, understand, that, but, like, i, forg...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/26_1-1-2.txt\n",
      "0                                  [how, do, you, feel]\n",
      "1     [well, i, feel, a, lot, like, like, it, should...\n",
      "2     [so, i, am, on, the, other, side, of, that, i,...\n",
      "3     [okay, i, mean, like, people, like, they, are,...\n",
      "4     [so, i, think, there, is, like, there, is, goi...\n",
      "5     [what, if, like, they, like, drank, with, like...\n",
      "6        [with, like, their, parents, or, with, family]\n",
      "7     [yeah, like, with, their, parents, like, they,...\n",
      "8     [yeah, i, do, not, know, how, much, drinking, ...\n",
      "9     [like, what, do, you, mean, like, like, at, a,...\n",
      "10    [no, like, how, would, you, say, okay, you, ca...\n",
      "11    [may, i, get, it, now, it, is, kind, of, impos...\n",
      "12    [longest, statement, that, is, ever, mad, it, ...\n",
      "13    [you, could, like, maybe, let, us, see, well, ...\n",
      "14    [because, they, have, to, learn, how, to, be, ...\n",
      "15                                 [this, is, so, hard]\n",
      "16     [well, alcohol, is, also, really, bad, for, you]\n",
      "17    [yeah, that, is, right, it, is, bad, for, the,...\n",
      "18    [i, do, not, know, how, it, affects, the, brai...\n",
      "19    [right, and, the, state, is, anyway, if, you, ...\n",
      "20    [yeah, and, if, you, asked, different, kinds, ...\n",
      "21              [yeah, i, do, not, know, what, to, say]\n",
      "22    [can, you, think, of, any, other, benefits, th...\n",
      "23    [benefits, of, drinking, well, like, there, ar...\n",
      "24    [drinking, at, that, age, no, that, is, why, t...\n",
      "25    [one, more, okay, would, you, like, would, you...\n",
      "26    [if, the, law, was, changed, to, eighteen, wou...\n",
      "27    [i, chose, the, other, topic, now, i, really, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/13_1-1-3.txt\n",
      "0     [i, like, strongly, feel, that, they, it, shou...\n",
      "1     [i, do, not, think, they, should, legal, well,...\n",
      "2     [so, is, it, not, right, because, like, god, s...\n",
      "3     [i, would, guess, so, but, i, think, society, ...\n",
      "4     [so, in, terms, of, society, you, think, that,...\n",
      "5     [well, i, know, young, kiss, now, they, are, j...\n",
      "6     [so, if, if, like, a, guy, and, guy, are, kiss...\n",
      "7                    [well, it, is, just, not, natural]\n",
      "8     [to, you, like, to, them, like, that, that, is...\n",
      "9     [what, about, the, decreased, populationdecrea...\n",
      "10    [like, right, now, we, are, already, becoming,...\n",
      "11                                  [i, guess, nothing]\n",
      "12    [right, hut, so, then, even, though, you, are,...\n",
      "13           [i, have, nothing, else, i, do, not, know]\n",
      "14    [let, us, see, let, us, see, what, else, comes...\n",
      "15    [i, guess, it, is, it, is, their, generation, ...\n",
      "16    [yeah, like, right, now, it, seems, like, ther...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/15_2-1-3.txt\n",
      "0                     [okay, so, you, said, yes, right]\n",
      "1     [so, yeah, but, like, in, a, way, its, like, t...\n",
      "2     [yeah, but, i, think, everybody, should, have,...\n",
      "3                          [yeah, that, is, what, yeah]\n",
      "4     [an, opposing, view, i, feel, like, usually, p...\n",
      "5     [it, is, usually, that, yeah, just, like, base...\n",
      "6     [when, they, think, it, should, be, man, and, ...\n",
      "7     [yeah, but, at, the, end, of, the, day, it, ma...\n",
      "8     [cause, usually, people, like, the, older, peo...\n",
      "9     [yeah, but, to, them, it, is, like, not, right...\n",
      "10    [i, mean, i, well, i, do, not, know, like, pas...\n",
      "11    [you, are, saying, like, there, there, is, you...\n",
      "12    [it, is, like, why, should, they, not, be, abl...\n",
      "13    [yeah, which, is, easily, debatable, because, ...\n",
      "14    [it, is, just, based, on, tradition, we, are, ...\n",
      "15    [what, else, we, will, be, asking, them, to, c...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/19_1-1-3.txt\n",
      "0     [okay, what, were, you, thinking, about, do, y...\n",
      "1                            [yeah, i, remember, those]\n",
      "2     [that, was, great, sorry, i, just, had, to, re...\n",
      "3     [yeah, but, definitely, got, its, point, acros...\n",
      "4     [i, mean, i, i, i, literally, just, walked, pa...\n",
      "5     [well, if, they, were, rapid, i, can, kind, of...\n",
      "6     [yeah, like, i, am, all, for, choice, even, th...\n",
      "7     [so, question, you, yourself, would, you, have...\n",
      "8     [yeah, i, was, going, to, say, you, probably, ...\n",
      "9     [would, you, have, an, abortion, but, i, have,...\n",
      "10    [yeah, but, see, that, is, that, choice, like,...\n",
      "11                 [well, damn, that, is, pretty, hard]\n",
      "12    [yeah, like, their, development, they, would, ...\n",
      "13    [but, in, the, end, if, you, do, not, even, al...\n",
      "14    [yeah, also, many, that, have, thirty, familie...\n",
      "15    [well, it, kind, of, depends, like, the, term,...\n",
      "16    [it, does, depend, that, is, why, like, you, t...\n",
      "17    [the, choice, itself, is, something, that, is,...\n",
      "18    [no, no, i, i, do, not, want, to, say, we, hav...\n",
      "19    [well, we, do, have, more, than, others, but, ...\n",
      "20    [a, lot, of, things, are, not, allowed, in, ok...\n",
      "21    [yeah, like, censorship, in, general, is, just...\n",
      "22    [yeah, i, guess, i, do, not, with, them, that,...\n",
      "23    [would, not, know, what, to, choose, i, can, p...\n",
      "24          [yeah, so, that, is, my, opinion, on, that]\n",
      "25                 [this, topic, has, a, lot, of, like]\n",
      "26                        [yeah, it, is, a, very, very]\n",
      "27    [my, side, requires, lots, of, graph, and, pic...\n",
      "28    [trust, me, i, already, went, to, the, ontolog...\n",
      "29    [which, which, class, like, those, mt, shows, ...\n",
      "30    [still, it, is, it, is, one, of, the, main, ma...\n",
      "31    [nine, months, of, a, child, that, most, likel...\n",
      "32    [you, know, just, a, little, child, no, yeah, ...\n",
      "33    [yeah, you, just, like, having, the, choice, o...\n",
      "34    [and, you, are, yeah, you, are, right, like, w...\n",
      "35             [it, is, eight, minutes, is, almost, up]\n",
      "36    [right, can, not, drag, this, on, any, longer,...\n",
      "37    [i, am, not, going, to, lie, i, kind, of, just...\n",
      "38    [yeah, i, walked, right, past, it, then, i, wa...\n",
      "39    [i, wonder, if, i, heart, life, actually, like...\n",
      "40    [i, am, sorry, it, was, pro, abortion, or, no,...\n",
      "41    [would, not, it, be, pro, abortion, though, if...\n",
      "42    [that, is, anti, abortion, abortion, is, killi...\n",
      "43                           [yeah, i, think, that, is]\n",
      "44    [yeah, so, it, is, anti, abortion, it, is, an,...\n",
      "45    [was, it, mainly, just, the, bear, or, was, it...\n",
      "46    [it, was, hey, i, just, love, life, in, genera...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/19_2-0-3.txt\n",
      "0                    [so, lowering, the, drinking, age]\n",
      "1     [yeah, i, believe, we, should, lower, the, dri...\n",
      "2     [that, is, that, is, what, i, said, on, englan...\n",
      "3     [no, but, then, as, well, you, can, be, when, ...\n",
      "4     [yeah, that, is, true, yeah, yeah, that, is, t...\n",
      "5     [i, can, tell, we, should, have, the, right, a...\n",
      "6     [it, i, i, feel, like, because, of, the, kiss,...\n",
      "7     [a, little, bit, they, can, care, les, about, ...\n",
      "8     [that, is, true, yeah, that, is, true, well, m...\n",
      "9     [as, much, as, you, would, want, to, whenever,...\n",
      "10    [that, is, true, that, its, the, worst, drug, ...\n",
      "11    [we, tend, to, find, out, ways, already, so, w...\n",
      "12    [yeah, pretty, much, i, meant, is, like, drugs...\n",
      "13    [and, sometimes, the, way, you, find, it, migh...\n",
      "14    [operable, form, of, yeah, and, i, feel, in, a...\n",
      "15    [but, then, again, i, understand, why, they, w...\n",
      "16    [but, then, again, you, can, smoke, when, you,...\n",
      "17    [but, cigarettes, only, give, you, a, sudden, ...\n",
      "18    [yeah, that, is, true, that, is, true, but, i,...\n",
      "19    [to, drink, and, there, is, something, i, feel...\n",
      "20    [yeah, yeah, that, is, true, even, though, i, ...\n",
      "21    [i, do, not, think, year, old, are, irresponsi...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/20_2-1-2.txt\n",
      "0     [okay, so, we, both, agree, that, abortion, is...\n",
      "1     [because, god, does, not, like, that, it, is, ...\n",
      "2     [so, your, saying, based, on, your, like, your...\n",
      "3     [yeah, so, religious, based, too, nice, it, is...\n",
      "4     [because, it, is, part, of, the, i, well, a, l...\n",
      "5     [yeah, they, do, not, need, to, die, for, some...\n",
      "6     [because, it, is, it, is, like, somebody, it, ...\n",
      "7     [but, they, can, go, you, have, the, right, so...\n",
      "8     [and, another, thing, that, i, think, is, that...\n",
      "9     [i, mean, if, you, did, not, want, to, have, a...\n",
      "10    [that, it, is, like, the, girl, 's, body, or, ...\n",
      "11    [and, then, there, is, adoption, you, could, j...\n",
      "12    [you, do, not, have, to, kill, it, if, you, do...\n",
      "13    [yeah, a, lot, of, people, say, because, they,...\n",
      "14    [then, do, not, have, sex, in, the, first, pla...\n",
      "15    [i, would, kill, her, just, bidding, i, do, no...\n",
      "16    [if, like, you, could, say, that, like, if, yo...\n",
      "17    [i, think, it, is, the, most, amazing, gift, t...\n",
      "18    [and, it, is, against, the, law, anyway, so, y...\n",
      "19    [yeah, and, then, if, they, start, doing, it, ...\n",
      "20    [yeah, and, then, a, lot, of, girls, are, goin...\n",
      "21    [i, talk, to, this, lady, that, had, an, abort...\n",
      "22    [yeah, i, did, for, my, church, i, did, like, ...\n",
      "23    [yeah, you, just, have, to, take, your, own, j...\n",
      "24    [follow, your, heart, but, do, not, abort, wha...\n",
      "25    [that, you, could, probably, have, more, babie...\n",
      "26    [but, there, be, like, risks, for, the, girl, ...\n",
      "27    [yeah, there, is, other, ways, out, you, know,...\n",
      "28    [give, it, up, or, just, leave, it, at, a, chu...\n",
      "29                      [are, you, done, do, you, know]\n",
      "30    [and, then, there, is, one, article, i, read, ...\n",
      "31    [babies, are, pretty, why, would, you, want, t...\n",
      "32    [if, we, knew, babies, i, would, say, they, sa...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/34_2-1-3.txt\n",
      "0     [so, i, i, am, against, abortion, and, you, ar...\n",
      "1     [yeah, yeah, i, am, i, am, against, it, due, y...\n",
      "2     [i, mean, the, way, i, feel, is, that, i, mean...\n",
      "3     [right, under, bad, circumstances, unconsensua...\n",
      "4                           [i, feel, pretty, strongly]\n",
      "5                        [yeah, me, too, man, yeahyeah]\n",
      "6     [and, it, is, like, up, for, as, a, personal, ...\n",
      "7     [yeah, that, is, true, man, i, means, feel, li...\n",
      "8     [yeah, i, agree, i, mean, it, is, just, like, ...\n",
      "9            [yeah, i, totally, agree, with, that, man]\n",
      "10    [yeah, i, mean, it, is, definitely, it, is, de...\n",
      "11    [yeah, true, man, gash, i, can, not, even, loo...\n",
      "12    [yeah, i, mean, that, is, definitely, i, mean,...\n",
      "13    [yeahthey, are, all, like, like, imagine, if, ...\n",
      "14    [i, mean, yeah, i, mean, and, even, i, mean, i...\n",
      "15    [yeah, that, is, i, see, what, you, mean, man,...\n",
      "16    [yeah, it, is, like, now, it, is, like, now, y...\n",
      "17    [sang, that, is, bad, yeah, right, like, somet...\n",
      "18    [yeah, it, is, definitely, something, very, se...\n",
      "19    [and, some, reasons, for, it, would, just, be,...\n",
      "20    [yeah, i, know, yeah, it, is, i, mean, it, is,...\n",
      "21    [yeah, i, mean, i, feel, like, yeah, knowing, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/34_1-0-3.txt\n",
      "0     [i, think, that, i, i, disagree, i, do, not, t...\n",
      "1     [i, if, you, have, like, a, woman, got, pregna...\n",
      "2     [yeah, i, think, that, if, somebody, 's, been,...\n",
      "3     [okay, i, just, i, will, just, say, if, they, ...\n",
      "4     [if, they, got, rapid, right, it, is, not, but...\n",
      "5     [like, the, case, in, china, you, know, for, t...\n",
      "6     [yeah, i, i, mean, if, somebody, is, worried, ...\n",
      "7     [it, is, also, making, the, children, 's, life...\n",
      "8     [but, that, was, you, know, the, the, mother, ...\n",
      "9     [but, that, is, not, only, the, woman, 's, lik...\n",
      "10    [of, course, i, i, understand, like, in, china...\n",
      "11    [okay, i, still, think, that, is, a, personal,...\n",
      "12    [i, mean, you, are, right, it, is, a, personal...\n",
      "13    [i, do, know, people, who, have, thought, abou...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/15_1-0-2.txt\n",
      "0      [okay, so, what, did, you, say, about, abortion]\n",
      "1     [i, say, abortion, should, be, able, to, in, a...\n",
      "2     [yeah, that, is, what, i, pretty, much, said, ...\n",
      "3      [well, that, is, their, choice, then, you, know]\n",
      "4     [yeah, i, know, it, is, like, it, is, complete...\n",
      "5     [no, i, have, not, due, i, had, people, in, my...\n",
      "6     [now, okay, yeah, ah, see, you, see, that, is,...\n",
      "7     [hey, due, as, long, as, it, works, out, for, ...\n",
      "8     [i, guess, yeah, i, guess, but, see, that, is,...\n",
      "9     [i, guess, as, far, as, many, times, or, as, f...\n",
      "10    [yeah, that, is, what, it, comes, like, you, d...\n",
      "11    [you, know, those, images, were, inaccurate, r...\n",
      "12        [yeah, those, were, like, they, were, really]\n",
      "13    [but, you, are, a, big, major, you, know, at, ...\n",
      "14    [yeah, i, know, because, they, said, at, six, ...\n",
      "15    [granted, they, were, speak, about, that, one,...\n",
      "16    [they, were, just, showing, yeah, yeah, i, was...\n",
      "17    [you, know, i, just, think, that, is, kind, of...\n",
      "18    [yeah, but, i, think, it, is, like, their, tac...\n",
      "19    [oh, that, is, right, i, believe, it, is, like...\n",
      "20                                  [are, you, serious]\n",
      "21    [yeah, because, it, was, something, about, the...\n",
      "22    [but, that, is, see, that, is, that, is, that,...\n",
      "23    [and, like, and, like, i, think, it, is, in, l...\n",
      "24    [imagine, it, is, just, pretty, much, one, bec...\n",
      "25    [or, like, what, what, is, the, odds, of, that...\n",
      "26    [pretty, it, depends, on, when, the, mother, i...\n",
      "27    [no, i, am, saying, even, if, it, is, fertiliz...\n",
      "28    [what, are, the, times, of, it, being, a, baby...\n",
      "29    [i, heard, it, was, less, than, twenty, percen...\n",
      "                            ...                        \n",
      "33    [well, maybe, we, should, look, at, how, they,...\n",
      "34    [that, is, true, but, i, do, not, know, that, ...\n",
      "35    [i, think, that, there, should, probably, be, ...\n",
      "36    [yeah, like, there, has, to, be, a, time, like...\n",
      "37    [i, would, say, an, incentive, like, i, do, no...\n",
      "38    [the, first, trimester, that, is, what, i, am,...\n",
      "39    [well, i, i, do, not, know, i, mean, i, think,...\n",
      "40    [well, i, think, that, like, if, they, are, af...\n",
      "41    [there, is, not, question, already, but, legis...\n",
      "42    [are, sure, yeah, yeah, i, know, yeah, you, ha...\n",
      "43    [definitely, i, mean, you, know, go, to, colle...\n",
      "44    [yeah, okay, look, if, you, seriously, have, s...\n",
      "45    [well, what, do, you, think, about, adult, con...\n",
      "46    [i, think, there, should, be, if, the, like, i...\n",
      "47    [i, mean, what, if, my, parents, do, not, want...\n",
      "48    [if, they, want, okay, yea, alright, yeah, oka...\n",
      "49    [you, know, if, i, am, a, minor, and, my, pare...\n",
      "50                       [yeah, that, is, a, good, one]\n",
      "51    [i, mean, the, other, big, factor, in, these, ...\n",
      "52    [who, wants, to, have, a, baby, yeah, so, i, d...\n",
      "53    [but, you, realize, what, goes, on, with, that...\n",
      "54                          [yeah, those, people, have]\n",
      "55            [people, have, them, anyway, and, either]\n",
      "56       [and, they, are, killing, girls, pretty, much]\n",
      "57    [yeah, either, abort, them, immediately, becau...\n",
      "58    [and, then, raise, it, yeah, i, heard, about, ...\n",
      "59                   [yeah, and, they, also, buy, them]\n",
      "60    [yeah, yeah, i, have, i, have, seen, there, is...\n",
      "61    [yeah, i, i, think, the, legislation, kind, of...\n",
      "62             [i, do, not, know, there, is, too, many]\n",
      "Name: token, Length: 63, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/38_1-1-3.txt\n",
      "0     [kill, him, i, would, say, so, but, then, like...\n",
      "1     [yeah, but, the, labor, thing, but, when, we, ...\n",
      "2     [well, for, as, for, the, taxes, i, feel, that...\n",
      "3     [i, do, not, know, otherwise, it, is, like, it...\n",
      "4     [the, whole, eye, for, an, eye, thing, yeah, a...\n",
      "5     [yeah, like, we, were, watching, this, video, ...\n",
      "6     [yeah, and, just, and, applying, that, to, lik...\n",
      "7     [yeah, how, long, do, you, have, to, is, not, ...\n",
      "8     [yeah, you, are, you, are, on, death, row, for...\n",
      "9     [yeah, some, people, like, die, while, they, a...\n",
      "10    [but, yeah, again, a, lot, of, people, end, up...\n",
      "11    [yeah, but, then, again, there, is, always, th...\n",
      "12    [right, so, there, are, like, some, benefits, ...\n",
      "13    [yeah, i, do, not, know, then, you, can, alway...\n",
      "14    [yeah, again, you, know, if, god, is, the, one...\n",
      "15                              [yeah, he, would, have]\n",
      "16    [he, would, have, so, the, fact, that, they, a...\n",
      "17    [yeah, this, is, kind, of, like, this, is, pre...\n",
      "18    [yeah, the, whole, to, ban, the, death, penalt...\n",
      "19    [yeah, the, whole, i, think, so, i, wonder, wh...\n",
      "20    [i, do, not, think, it, passed, because, we, a...\n",
      "21    [you, do, not, think, okay, a, lot, of, other,...\n",
      "22    [no, and, prisoners, and, because, every, stat...\n",
      "23    [yeah, that, had, be, that, had, be, two, more...\n",
      "24    [so, like, for, the, people, that, were, on, d...\n",
      "25    [is, not, it, like, killing, someone, that, is...\n",
      "26    [possibly, like, if, it, was, like, because, l...\n",
      "27    [to, life, is, not, it, or, something, yeah, b...\n",
      "28    [well, i, feel, like, they, vote, more, just, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/18_1-0-3.txt\n",
      "0     [okay, my, dad, is, a, doctor, so, i, directly...\n",
      "1     [but, can, you, can, you, explain, your, persp...\n",
      "2     [yeah, my, my, dad, owns, his, own, private, c...\n",
      "3     [so, a, universal, healthcare, for, like, the,...\n",
      "4     [the, doctors, yeah, by, by, a, significant, a...\n",
      "5     [so, you, are, you, think, we, should, not, re...\n",
      "6     [yeah, and, i, mean, a, lot, of, other, doctor...\n",
      "7     [so, okay, so, i, guess, well, of, course, i, ...\n",
      "8     [i, mean, i, mean, i, would, actually, really,...\n",
      "9     [i, mean, going, back, to, what, you, were, sa...\n",
      "10    [yeah, i, know, what, you, are, talking, about...\n",
      "11    [i, know, about, that, i, think, i, think, tha...\n",
      "12    [well, the, well, right, now, it, is, it, is, ...\n",
      "13    [i, mean, i, think, like, i, said, it, goes, a...\n",
      "14    [then, they, should, have, a, then, they, shou...\n",
      "15    [yeah, definitely, that, is, what, i, think, t...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/14_2-0-3.txt\n",
      "0       [so, how, do, you, feel, about, gay, marriage]\n",
      "1    [i, think, gay, marriage, should, be, it, shou...\n",
      "2    [yeah, me, too, i, personally, grew, up, or, s...\n",
      "3    [yeah, yeah, i, feel, like, it, is, based, on,...\n",
      "4    [yeah, most, definitely, i, feel, like, the, p...\n",
      "5    [yeah, yeah, i, am, pretty, close, i, like, i,...\n",
      "6    [yeah, i, feel, like, the, like, the, us, in, ...\n",
      "7    [yeah, yeah, like, i, guess, like, you, know, ...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/7_1-0-2.txt\n",
      "0                             [are, you, prochoice, or]\n",
      "1                                        [i, guess, it]\n",
      "2                                    [i, am, prochoice]\n",
      "3                               [so, you, are, for, it]\n",
      "4          [well, i, am, not, for, abortion, like, but]\n",
      "5                   [to, a, degree, okay, start, there]\n",
      "6                              [you, can, start, first]\n",
      "7     [okay, well, i, am, against, abortion, because...\n",
      "8     [but, do, you, believe, that, women, should, h...\n",
      "9     [yeah, but, then, yeah, it, is, not, just, the...\n",
      "10    [what, if, what, if, they, are, incapable, of,...\n",
      "11    [but, then, there, is, there, is, there, is, l...\n",
      "12    [what, what, what, about, those, kiss, that, d...\n",
      "13    [they, still, get, raised, within, that, like,...\n",
      "14    [so, women, should, just, not, have, the, choi...\n",
      "15    [they, have, the, choice, i, but, i, still, go...\n",
      "16    [but, they, should, have, a, choice, you, just...\n",
      "17    [well, they, have, the, choice, they, make, th...\n",
      "18    [and, they, are, probably, aware, of, that, to...\n",
      "19    [yeah, but, then, i, know, someone, personally...\n",
      "20    [but, what, if, she, did, not, get, an, aborti...\n",
      "21    [she, might, like, right, now, she, is, consid...\n",
      "22    [but, do, you, think, that, is, healthy, for, ...\n",
      "23    [they, are, going, to, be, there, she, is, not...\n",
      "24    [do, you, have, a, here, or, where, is, your, ...\n",
      "25    [it, is, not, related, to, me, it, is, like, a...\n",
      "26    [but, do, you, think, that, is, healthy, if, s...\n",
      "27    [no, but, she, can, not, raise, the, baby, her...\n",
      "28    [okay, but, what, about, all, the, other, case...\n",
      "29    [that, is, the, worst, case, but, not, everyon...\n",
      "30    [but, if, she, did, not, want, to, it, would, ...\n",
      "31    [but, that, the, kiss, still, raised, up, like...\n",
      "32    [if, she, chose, to, keep, the, baby, that, me...\n",
      "33    [well, again, the, topic, was, abortion, so, i...\n",
      "34    [so, so, if, it, were, to, like, vote, you, wo...\n",
      "35    [i, mean, yeah, like, they, well, ah, it, is, ...\n",
      "36    [but, i, like, i, do, not, necessarily, agree,...\n",
      "37    [i, mean, it, is, a, choice, like, between, ge...\n",
      "38    [but, it, is, like, having, it, is, like, havi...\n",
      "39    [yea, it, is, up, to, their, choice, i, agreed...\n",
      "40    [okay, and, then, but, like, what, do, you, sa...\n",
      "41    [well, abortion, is, only, given, in, the, beg...\n",
      "42                      [yea, after, a, certain, point]\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/21_1-1-2.txt\n",
      "0     [well, i, believe, that, it, should, be, lower...\n",
      "1     [i, think, that, it, should, not, be, lowered,...\n",
      "2     [again, that, yeah, i, agree, about, the, matu...\n",
      "3     [i, know, the, eighteen, and, over, but, they,...\n",
      "4     [yeah, but, i, do, not, have, much, to, say, n...\n",
      "5     [i, do, not, know, i, just, i, just, feel, lik...\n",
      "6     [yeah, i, got, your, point, but, yea, but, but...\n",
      "7     [i, do, not, know, i, just, feel, like, when, ...\n",
      "8     [but, then, i, kind, of, believe, that, is, no...\n",
      "9     [i, know, they, are, not, still, in, the, scho...\n",
      "10    [that, is, why, i, am, i, mean, if, they, are,...\n",
      "11    [but, does, not, drinking, come, responsibilit...\n",
      "12    [yeah, i, mean, if, they, are, told, about, th...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/21_2-0-3.txt\n",
      "0      [gay, marriage, what, do, you, think, about, it]\n",
      "1                    [i, think, it, should, be, it, is]\n",
      "2     [i, think, it, should, be, i, mean, there, is,...\n",
      "3     [i, think, it, should, be, legal, i, think, ot...\n",
      "4     [what, was, like, your, opinion, about, the, w...\n",
      "5     [i, think, that, its, i, think, if, its, legal...\n",
      "6     [they, use, it, to, more, i, think, there, is,...\n",
      "7                            [you, are, talking, about]\n",
      "8     [cause, i, heard, that, like, some, people, ca...\n",
      "9     [i, think, for, california, it, was, in, the, ...\n",
      "10    [well, i, am, not, sure, i, think, i, think, t...\n",
      "11    [i, think, that, the, government, might, have,...\n",
      "12    [do, you, think, it, it, would, be, like, diff...\n",
      "13    [no, i, do, not, think, well, like, i, do, not...\n",
      "14    [i, think, for, women, it, is, much, more, acc...\n",
      "15    [i, think, it, should, be, like, everywhere, i...\n",
      "16                                 [like, every, state]\n",
      "17    [i, think, south, states, would, be, way, hard...\n",
      "18    [yeah, i, was, thinking, that, yeah, and, ther...\n",
      "19    [i, think, in, like, the, more, liberal, state...\n",
      "20    [i, know, you, have, do, you, think, it, would...\n",
      "21    [i, mean, personally, i, think, that, that, wo...\n",
      "22    [like, there, is, no, peer, pressure, like, it...\n",
      "23    [i, think, it, would, like, the, way, our, soc...\n",
      "24    [yeah, so, it, is, like, is, our, society, rea...\n",
      "25    [i, do, not, think, it, is, ever, going, to, b...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/35_1-1-3.txt\n",
      "0     [ok, so, i, said, yes, because, it, will, help...\n",
      "1     [but, like, for, that, point, if, that, is, th...\n",
      "2     [yeah, but, i, mean, people, have, i, mean, pe...\n",
      "3     [but, the, issue, with, that, is, like, if, yo...\n",
      "4                     [i, mean, same, way, as, alcohol]\n",
      "5     [well, is, it, is, the, age, really, going, to...\n",
      "6     [ok, maybe, not, the, age, but, like, you, can...\n",
      "7     [but, see, then, that, is, like, yeah, yeah, b...\n",
      "8     [well, marijuana, does, not, have, any, effect...\n",
      "9     [well, it, effects, right, but, it, does, alte...\n",
      "10                 [only, when, you, are, high, on, it]\n",
      "11                    [yeah, but, what, else, did, you]\n",
      "12    [but, i, mean, it, is, not, like, it, just, it...\n",
      "13    [but, maybe, that, is, the, issue, you, know, ...\n",
      "14    [but, then, why, do, we, make, alcohol, illega...\n",
      "15    [it, does, but, like, hata, at, one, point, it...\n",
      "16    [i, just, feel, like, alcohol, already, it, is...\n",
      "17    [i, think, with, alcohol, it, is, like, they, ...\n",
      "18    [hata, that, is, true, i, mean, there, is, lik...\n",
      "19    [well, people, could, just, be, like, you, kno...\n",
      "20    [but, police, like, if, there, is, indication,...\n",
      "21    [well, they, have, the, little, breathalizer, ...\n",
      "22    [they, can, have, they, have, brethalizers, fo...\n",
      "23    [but, do, like, so, but, what, was, their, lik...\n",
      "24    [i, am, not, sure, hata, i, do, not, know, i, ...\n",
      "25    [hata, ok, i, guess, then, that, is, another, ...\n",
      "26    [yeah, but, i, mean, like, they, do, that, wit...\n",
      "27    [but, see, alcohol, is, more, a, clear, thing,...\n",
      "28    [well, i, feel, like, alcohol, and, marijuana,...\n",
      "29    [yeah, but, with, alcohol, like, yeah, also, i...\n",
      "30    [you, could, do, that, with, marijuana, too, t...\n",
      "31    [but, i, do, not, mean, maybe, there, is, not,...\n",
      "32    [i, feel, like, there, is, i, mean, why, else,...\n",
      "33    [well, because, it, is, like, well, i, mean, i...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/26_1-0-3.txt\n",
      "0                                         [go, for, it]\n",
      "1                            [you, can, go, first, now]\n",
      "2     [i, am, i, am, pro, gay, and, lesion, marriage...\n",
      "3     [well, like, i, am, like, against, it, because...\n",
      "4     [i, grew, up, in, a, similar, situation, but, ...\n",
      "5     [okay, yeah, like, you, mean, like, well, well...\n",
      "6     [i, think, there, are, several, religions, tha...\n",
      "7     [what, if, like, you, had, like, a, child, and...\n",
      "8     [if, i, had, a, gay, or, lesion, child, yeah, ...\n",
      "9                [do, you, want, to, ask, me, anything]\n",
      "10    [what, would, what, would, you, do, if, you, h...\n",
      "11    [well, i, would, like, like, talk, like, with,...\n",
      "12    [see, just, be, clear, you, said, like, you, w...\n",
      "13    [but, but, like, i, would, not, like, support,...\n",
      "14                            [you, i, am, sorry, what]\n",
      "15    [like, i, like, would, not, you, know, support...\n",
      "16    [so, like, support, in, like, what, kind, of, ...\n",
      "17    [yeah, like, financially, or, yeah, because, i...\n",
      "18    [yeah, i, do, not, i, do, not, know, how, much...\n",
      "19    [like, i, do, not, like, really, control, it, ...\n",
      "20                          [it, should, be, what, way]\n",
      "21    [like, like, like, it, should, not, like, be, ...\n",
      "22    [so, i, am, just, trying, to, like, wrap, my, ...\n",
      "23    [like, if, if, you, like, want, like, raised, ...\n",
      "24    [yeah, i, have, a, lot, of, yeah, like, are, y...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/13_1-0-2.txt\n",
      "0     [how, do, you, feel, about, marijuana, being, ...\n",
      "1     [well, before, i, was, completely, for, it, bu...\n",
      "2     [and, you, do, not, think, like, i, mean, i, d...\n",
      "3     [that, is, kind, of, scar, to, think, about, b...\n",
      "4     [yeah, that, is, the, only, thing, i, would, t...\n",
      "5     [yeah, i, mean, that, is, the, only, way, it, ...\n",
      "6     [yeah, so, you, think, that, it, would, benefi...\n",
      "7                                    [i, do, not, know]\n",
      "8     [so, i, guess, overall, you, would, say, no, t...\n",
      "9     [i, guess, there, is, more, up, sides, than, d...\n",
      "10    [so, if, you, were, like, to, say, like, if, y...\n",
      "11                       [even, walking, is, dangerous]\n",
      "12    [no, walking, while, you, are, high, you, must...\n",
      "13    [yeah, probably, because, i, think, if, you, h...\n",
      "14    [yeah, that, is, true, but, then, you, also, h...\n",
      "15    [do, you, think, it, would, help, college, stu...\n",
      "16    [some, claim, that, it, does, i, do, not, know...\n",
      "17                         [only, as, stress, relieved]\n",
      "18    [yeah, i, mean, totally, that, is, what, it, i...\n",
      "19    [yeah, and, i, think, if, it, does, get, legal...\n",
      "20    [yea, that, is, true, the, other, thing, about...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/32_2-1-2.txt\n",
      "0                                 [do, you, oppose, it]\n",
      "1                                  [i, agree, with, it]\n",
      "2           [i, agree, also, what, is, your, reasoning]\n",
      "3     [well, it, is, a, way, of, life, or, it, is, a...\n",
      "4     [that, is, practically, my, point, yeah, prett...\n",
      "5     [yeah, and, i, mean, like, you, know, we, pret...\n",
      "6     [eventually, we, will, get, it, i, am, trying,...\n",
      "7     [yeah, i, also, do, not, like, the, idea, of, ...\n",
      "8     [him, know, the, a, big, issue, they, had, aro...\n",
      "9     [in, the, bible, i, do, not, know, i, do, not,...\n",
      "10    [i, do, not, either, but, i, am, pretty, sure,...\n",
      "11    [i, am, not, religious, yeah, yeah, i, would, ...\n",
      "12    [yeah, that, is, true, i, know, there, is, oth...\n",
      "13    [i, know, it, is, kind, of, embarrassing, to, ...\n",
      "14    [yeah, looking, at, it, from, outside, having,...\n",
      "15    [i, mean, other, countries, out, there, alread...\n",
      "16                               [that, is, very, true]\n",
      "17                          [is, there, anything, else]\n",
      "18    [pretty, sure, just, that, i, am, trying, to, ...\n",
      "19                   [and, i, think, hawaii, just, did]\n",
      "20             [new, york, does, not, it, allow, right]\n",
      "21    [pretty, sure, a, lot, of, the, eastern, east,...\n",
      "22    [yeahbut, i, do, not, know, if, this, actually...\n",
      "23    [do, not, know, about, that, but, yeah, that, ...\n",
      "24    [that, is, for, sure, actually, another, thing...\n",
      "25    [they, want, it, yeah, exactly, i, see, no, po...\n",
      "26    [i, do, not, know, about, this, but, you, know...\n",
      "27    [yeah, but, still, it, is, you, need, all, the...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/13_2-1-3.txt\n",
      "0     [i, said, that, it, i, said, that, it, had, th...\n",
      "1     [yeah, that, is, that, is, how, i, feel, like,...\n",
      "2     [yeah, that, that, is, that, is, basically, th...\n",
      "3     [yeah, well, i, think, that, could, happen, wi...\n",
      "4     [and, sometimes, they, may, even, get, out, li...\n",
      "5     [like, you, know, how, some, are, like, on, pr...\n",
      "6     [that, is, that, is, exactly, what, i, feel, a...\n",
      "7     [yeah, and, then, well, when, you, think, abou...\n",
      "8     [that, is, true, and, that, and, that, like, t...\n",
      "9     [yeah, they, could, contribute, themselves, to...\n",
      "10    [yeah, exactly, like, there, is, there, is, al...\n",
      "11    [yeah, it, was, not, just, like, a, one, time,...\n",
      "12    [cause, i, mean, like, they, they, do, have, l...\n",
      "13    [yeah, so, you, think, it, is, just, like, a, ...\n",
      "14    [and, yeah, anything, else, him, nothing, come...\n",
      "15    [than, money, and, then, it, would, be, like, ...\n",
      "16    [money, safety, i, guess, like, victim, comfor...\n",
      "17    [they, or, they, did, not, deserve, what, they...\n",
      "18    [yeah, it, is, hard, to, explain, let, us, see...\n",
      "19                      [take, someone, else, 's, life]\n",
      "20    [yeah, you, should, never, take, a, lifeand, i...\n",
      "21    [it, is, just, that, like, how, do, you, know,...\n",
      "22    [exactly, like, there, is, so, much, uncertainty]\n",
      "23    [yeah, or, even, if, they, do, like, what, if,...\n",
      "24    [yea, that, is, true, and, you, never, know, w...\n",
      "25                           [send, letters, or, email]\n",
      "26    [and, you, do, not, know, who, else, and, plus...\n",
      "27    [i, think, that, they, do, not, they, will, be...\n",
      "28    [yea, and, i, feel, like, that, would, be, if,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/26_2-1-2.txt\n",
      "0     [so, i, believe, that, abortion, should, be, i...\n",
      "1     [okay, i, agree, i, think, it, is, it, is, wro...\n",
      "2     [yeah, like, it, should, not, be, in, your, ha...\n",
      "3     [yeah, i, think, there, are, so, many, other, ...\n",
      "4     [exactly, like, like, abortion, should, not, b...\n",
      "5     [i, think, abortion, now, is, kind, of, used, ...\n",
      "6     [yeah, that, is, true, i, never, i, never, rea...\n",
      "7     [and, it, is, not, the, child, 's, fault, i, k...\n",
      "8     [exactly, like, they, say, like, i, do, not, k...\n",
      "9     [okay, it, was, that, it, is, a, life, that, t...\n",
      "10    [my, main, argument, is, just, that, just, bec...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/1_2-1-3.txt\n",
      "0        [okay, how, old, are, you, you, are, not, yet]\n",
      "1                                          [i, am, not]\n",
      "2     [i, am, so, how, do, you, feel, about, illegal...\n",
      "3     [i, have, like, mixed, feelings, about, it, i,...\n",
      "4     [i, think, you, should, be, up, to, drink, and...\n",
      "5     [that, is, true, yeah, i, think, one, thing, t...\n",
      "6     [maybe, they, think, that, but, maybe, they, t...\n",
      "7     [yeah, i, mean, that, is, true, it, is, on, na...\n",
      "8     [i, think, they, think, because, you, are, lik...\n",
      "9     [yeah, that, is, definitely, true, i, think, t...\n",
      "10            [what, else, we, can, go, to, hook, bars]\n",
      "11    [yeah, it, seems, silly, you, can, do, everyth...\n",
      "12    [and, there, is, no, like, strict, strict, rul...\n",
      "13    [yeah, it, is, like, so, easy, to, get, by, it...\n",
      "14     [yeah, and, my, other, friend, has, a, face, id]\n",
      "15    [yeah, you, can, get, face, is, online, yeah, ...\n",
      "16    [that, is, crazy, exactly, so, they, should, j...\n",
      "17    [yeah, yeah, that, definitely, true, i, feel, ...\n",
      "18    [yeah, my, friend, got, a, ticket, for, being,...\n",
      "19    [yeah, that, 'll, happen, yeah, well, no, and,...\n",
      "20    [yeah, she, is, silly, yeah, and, then, they, ...\n",
      "21    [yeah, i, definitely, think, it, should, yeah,...\n",
      "22    [and, you, can, get, married, at, it, is, not,...\n",
      "23    [well, it, is, like, you, can, go, to, the, ar...\n",
      "24    [that, is, crazy, yeah, because, the, age, cha...\n",
      "25    [i, did, drink, i, had, a, drink, before, i, w...\n",
      "26                            [go, to, parties, a, lot]\n",
      "27    [i, am, trying, to, like, yeah, is, not, that,...\n",
      "28    [we, all, go, to, parties, we, are, in, colleg...\n",
      "29    [yeah, if, they, lowered, it, to, it, might, m...\n",
      "30    [true, i, got, like, super, drunk, one, time, ...\n",
      "31    [yeah, so, if, you, are, introduced, to, it, a...\n",
      "32    [you, keep, doing, something, for, so, long, i...\n",
      "33    [yeah, yeah, it, is, less, of, this, like, for...\n",
      "34    [white, boy, wasted, yeah, people, think, that...\n",
      "35    [yeah, definitely, there, is, a, lot, yeah, i,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/33_2-0-3.txt\n",
      "0           [so, what, do, you, think, about, abortion]\n",
      "1     [personally, i, think, that, for, the, current...\n",
      "2         [so, you, think, like, allocation, of, funds]\n",
      "3     [yeah, i, think, allocation, of, funds, is, a,...\n",
      "4     [it, should, be, legal, i, agree, it, should, ...\n",
      "5     [i, think, that, would, that, is, one, of, the...\n",
      "6     [so, that, is, something, you, would, push, th...\n",
      "7     [yeah, i, would, push, for, that, as, opposed,...\n",
      "8     [prevent, it, okay, things, like, condemn, or,...\n",
      "9     [yeah, like, you, know, providing, yeah, and, ...\n",
      "10    [so, for, for, stuff, like, that, like, the, c...\n",
      "11    [it, is, a, largely, but, i, think, that, a, l...\n",
      "12    [you, still, think, that, it, should, be, the,...\n",
      "13    [i, think, that, there, and, maybe, even, yeah...\n",
      "14    [i, mean, my, my, personal, opinion, is, like,...\n",
      "15    [and, i, agree, i, i, that, is, what, i, am, s...\n",
      "16    [so, i, do, i, do, believe, in, that, adoption...\n",
      "17    [and, i, am, not, necessarily, maybe, not, eve...\n",
      "18              [i, agree, it, should, be, legal, yeah]\n",
      "19    [i, think, that, it, and, i, i, do, not, think...\n",
      "20    [i, mean, moral, morally, i, can, understand, ...\n",
      "21    [yeah, i, do, not, have, any, like, you, know,...\n",
      "22    [like, if, like, like, if, something, had, hap...\n",
      "23    [i, think, that, yeah, and, i, think, that, a,...\n",
      "24    [i, feel, i, i, i, understand, what, you, wher...\n",
      "25    [but, yeah, i, do, there, is, i, do, not, thin...\n",
      "26    [like, illegal, or, anything, yeah, i, i, unde...\n",
      "27                  [that, abortion, should, be, legal]\n",
      "28    [abortion, should, be, legal, what, we, do, no...\n",
      "29    [but, i, i, believe, that, like, the, those, f...\n",
      "30    [okay, so, what, so, so, correct, me, if, i, a...\n",
      "31    [and, eventually, i, think, the, long, term, c...\n",
      "32    [i, mean, so, i, i, actually, agree, with, tha...\n",
      "33    [yeah, i, think, there, is, like, and, the, i,...\n",
      "34    [finding, something, that, works, better, and,...\n",
      "35    [exactly, so, i, think, that, putting, money, ...\n",
      "36    [i, i, agree, i, agree, with, you, on, that, y...\n",
      "37    [but, yeah, re, i, do, not, think, you, should...\n",
      "38    [so, abortion, should, be, legal, funds, shoul...\n",
      "39    [yeah, and, not, just, and, i, yeah, i, mean, ...\n",
      "40    [birth, control, is, the, female, yeah, i, any...\n",
      "41    [and, i, agree, and, i, think, reducing, i, am...\n",
      "42    [exactly, it, it, is, i, feel, like, i, feel, ...\n",
      "43    [and, along, those, lines, i, think, one, thin...\n",
      "44    [no, not, in, cal, yeah, wheat, where, they, j...\n",
      "45    [it, should, be, up, to, the, states, to, deci...\n",
      "46    [i, agree, with, you, on, that, it, the, way, ...\n",
      "47    [and, the, children, that, they, would, be, ha...\n",
      "48    [citizens, yeah, yeah, exactly, i, i, agree, s...\n",
      "49            [but, increase, funds, for, alternatives]\n",
      "50    [increase, funds, for, alternatives, or, contr...\n",
      "51    [i, think, that, is, yeah, i, mean, largely, i...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/12_2-0-3.txt\n",
      "0     [my, beliefs, on, gay, marriage, are, people, ...\n",
      "1     [right, and, this, life, is, not, like, they, ...\n",
      "2     [yeah, and, then, many, people, argue, the, bi...\n",
      "3     [right, and, then, on, other, for, that, that,...\n",
      "4     [i, agree, and, then, bay, marriage, now, well...\n",
      "5     [that, is, true, also, one, other, thing, that...\n",
      "6     [and, then, also, the, some, people, argue, th...\n",
      "7     [yeah, also, like, why, should, not, they, get...\n",
      "8     [ok, and, i, think, even, more, if, you, had, ...\n",
      "9     [because, they, love, each, other, or, they, w...\n",
      "10    [yeah, and, also, we, have, no, control, over,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/12_1-1-3.txt\n",
      "0       [so, yours, is, democrats, i, have, republican]\n",
      "1                                     [what, is, yours]\n",
      "2     [so, okay, so, for, you, the, whole, agenda, a...\n",
      "3     [honestly, being, a, democrat, came, from, my,...\n",
      "4     [well, here, is, my, reason, for, republican, ...\n",
      "5     [yeah, but, it, is, like, i, mean, where, are,...\n",
      "6     [well, with, all, the, money, you, are, going,...\n",
      "7     [i, do, not, know, about, that, honestly, i, d...\n",
      "8     [no, no, so, go, ahead, it, is, so, so, right,...\n",
      "9                          [give, an, example, of, who]\n",
      "10    [let, us, say, up, let, us, go, with, let, us,...\n",
      "11    [what, was, sorry, what, was, the, whole, poin...\n",
      "12    [so, basically, weapons, of, mass, destruction...\n",
      "13                                   [no, not, at, all]\n",
      "14    [so, basically, it, is, going, to, regulate, l...\n",
      "15                  [yeah, yeah, i, heard, about, that]\n",
      "16    [but, it, is, not, only, illegal, download, it...\n",
      "17    [i, i, agree, i, mean, what, is, the, point, o...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/33_1-1-2.txt\n",
      "0     [do, we, have, to, debate, or, because, i, so,...\n",
      "1     [yeah, exactly, the, thing, is, like, some, pe...\n",
      "2     [so, basically, it, is, like, depending, on, t...\n",
      "3     [exactly, yeah, so, exactly, if, you, were, to...\n",
      "4     [i, i, do, not, know, what, to, say, well, i, ...\n",
      "5     [you, know, let, us, just, say, my, was, kille...\n",
      "6                       [can, not, think, of, anything]\n",
      "7     [i, got, nothing, we, will, see, does, it, dep...\n",
      "8     [they, have, psychology, like, criminal, actio...\n",
      "9     [i, think, we, should, enforce, it, here, the,...\n",
      "10    [i, think, they, have, that, course, my, roomm...\n",
      "11    [whoa, i, never, knew, that, what, would, you,...\n",
      "12    [okay, so, i, think, it, should, should, not, ...\n",
      "13    [i, believe, that, marijuana, should, be, lega...\n",
      "14    [but, if, you, use, it, for, a, long, time, then]\n",
      "15    [of, course, they, do, not, mess, with, their,...\n",
      "16                   [do, you, do, it, do, you, do, it]\n",
      "17             [no, i, do, not, do, it, no, i, do, not]\n",
      "18    [well, so, basically, i, think, it, should, no...\n",
      "19                       [yeah, i, believe, it, should]\n",
      "20    [because, one, of, my, friends, he, did, it, b...\n",
      "21    [but, then, with, parietes, you, know, yeah, i...\n",
      "22    [that, is, why, you, should, not, drink, in, c...\n",
      "23    [yeah, that, is, true, though, yeah, i, think,...\n",
      "24    [anything, to, say, you, start, like, taking, ...\n",
      "25    [yeah, you, you, are, going, to, college, too,...\n",
      "Name: token, dtype: object\n",
      "Processing: /Users/nduran/Desktop/GitProjects/align-linguistic-alignment/PREPPED_TRANSCRIPTS/27_1-1-2.txt\n",
      "0         [so, yeah, what, do, you, think, yes, or, no]\n",
      "1     [i, think, it, is, no, no, no, because, yeah, ...\n",
      "2     [i, argued, that, they, kind, of, should, be, ...\n",
      "3     [but, like, you, know, like, now, is, like, ec...\n",
      "4     [it, is, technically, well, it, is, they, are,...\n",
      "5     [so, you, saying, that, like, they, should, be...\n",
      "6     [no, they, just, like, for, now, and, then, if...\n",
      "7     [ok, but, this, is, kind, of, like, a, difficu...\n",
      "8                                       [yeah, i, mean]\n",
      "9           [like, not, now, because, i, do, not, know]\n",
      "10    [well, it, is, like, i, understand, that, like...\n",
      "11    [but, what, if, like, the, rich, actually, the...\n",
      "12                       [if, they, are, losing, money]\n",
      "13           [yeah, what, if, they, are, losing, money]\n",
      "14    [well, it, is, like, what, for, business, vent...\n",
      "15                                    [they, are, rich]\n",
      "16    [yeah, yeah, i, mean, how, would, they, like, ...\n",
      "17                                   [i, do, not, know]\n",
      "18    [i, do, not, know, you, are, just, they, are, ...\n",
      "19    [it, is, like, it, is, not, fair, for, the, ri...\n",
      "20    [okay, well, if, like, the, way, i, view, it, ...\n",
      "21    [let, us, talk, about, like, how, would, the, ...\n",
      "22    [an, spend, the, money, taxed, more, okay, you...\n",
      "23    [okay, so, how, how, do, you, like, set, the, ...\n",
      "24    [let, us, see, here, i, guess, we, should, loo...\n",
      "Name: token, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nduran/anaconda/lib/python2.7/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    }
   ],
   "source": [
    "[turn_real,convo_real]= PHASE2RUN_REAL(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                        output_file_directory = INPUT_PATH+ANALYSIS_READY,\n",
    "                        semantic_model_input_file = INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                        high_sd_cutoff=3,\n",
    "                        low_n_cutoff=1,\n",
    "                        delay=1,\n",
    "                        maxngram=3,\n",
    "                        ignore_duplicates=True,\n",
    "                        ADD_STANFORD_TAGS=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase 2: Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[turn_surrogate,convo_surrogate] = PHASE2RUN_SURROGATE(input_file_directory = INPUT_PATH+PREPPED_TRANSCRIPTS, \n",
    "                             surrogate_file_directory= INPUT_PATH+SURROGATE_TRANSCRIPTS,\n",
    "                             output_file_directory= INPUT_PATH+ANALYSIS_READY,\n",
    "                             semantic_model_input_file=INPUT_PATH+'align_concatenated_dataframe.txt',\n",
    "                             high_sd_cutoff=3,\n",
    "                             low_n_cutoff=1,\n",
    "                             id_separator = '\\-',\n",
    "                             condition_label='cond',\n",
    "                             dyad_label='dyad',\n",
    "                             all_surrogates=False,\n",
    "                             keep_original_turn_order = False,\n",
    "                             delay=1,\n",
    "                             maxngram=3,\n",
    "                             ignore_duplicates=True,\n",
    "                             ADD_STANFORD_TAGS=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Speed calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2real - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 2 real time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "start_phase2surrogate - start_phase2real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Phase 2 surrogate time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end - start_phase2surrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All 3 phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "end - start_phase1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Printouts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>cosine_lexical_tok2</th>\n",
       "      <th>cosine_lexical_tok3</th>\n",
       "      <th>condition_info</th>\n",
       "      <th>cosine_lexical_lem3</th>\n",
       "      <th>cosine_syntax_penn_tok3</th>\n",
       "      <th>cosine_syntax_penn_tok2</th>\n",
       "      <th>cosine_syntax_penn_lex2</th>\n",
       "      <th>cosine_syntax_penn_lex3</th>\n",
       "      <th>cosine_semanticL</th>\n",
       "      <th>partner_direction</th>\n",
       "      <th>cosine_lexical_lem2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.5</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999796</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99902</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0237893</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>0.0594733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.217571</td>\n",
       "      <td>0.120386</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0.120386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>0.217571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999261</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0353996</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999078</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>0.0353996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0481869</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99946</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>0.0444262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0428746</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998847</td>\n",
       "      <td>0&gt;1.0</td>\n",
       "      <td>0.0395285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.140636</td>\n",
       "      <td>0</td>\n",
       "      <td>41_1-1-3.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999153</td>\n",
       "      <td>1&gt;0.0</td>\n",
       "      <td>0.135582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time cosine_lexical_tok2 cosine_lexical_tok3 condition_info  \\\n",
       "0     0             0.57735                 0.5   41_1-1-3.txt   \n",
       "1     1                   0                   0   41_1-1-3.txt   \n",
       "2     2           0.0237893                   0   41_1-1-3.txt   \n",
       "3     3                   0                   0   41_1-1-3.txt   \n",
       "4     4            0.217571            0.120386   41_1-1-3.txt   \n",
       "5     5                   0                   0   41_1-1-3.txt   \n",
       "6     6           0.0353996                   0   41_1-1-3.txt   \n",
       "7     7           0.0481869                   0   41_1-1-3.txt   \n",
       "8     8           0.0428746                   0   41_1-1-3.txt   \n",
       "9     9            0.140636                   0   41_1-1-3.txt   \n",
       "\n",
       "  cosine_lexical_lem3 cosine_syntax_penn_tok3 cosine_syntax_penn_tok2  \\\n",
       "0                 0.5                       0                       0   \n",
       "1                   0                       0                       0   \n",
       "2                   0                       0                       0   \n",
       "3                   0                       0                       0   \n",
       "4            0.120386                       0                       0   \n",
       "5                   0                       0                       0   \n",
       "6                   0                       0                       0   \n",
       "7                   0                       0                       0   \n",
       "8                   0                       0                       0   \n",
       "9                   0                       0                       0   \n",
       "\n",
       "  cosine_syntax_penn_lex2 cosine_syntax_penn_lex3 cosine_semanticL  \\\n",
       "0                       0                       0         0.999796   \n",
       "1                       0                       0          0.99902   \n",
       "2                       0                       0         0.999381   \n",
       "3                       0                       0         0.999715   \n",
       "4                       0                       0         0.999787   \n",
       "5                       0                       0         0.999261   \n",
       "6                       0                       0         0.999078   \n",
       "7                       0                       0          0.99946   \n",
       "8                       0                       0         0.998847   \n",
       "9                       0                       0         0.999153   \n",
       "\n",
       "  partner_direction cosine_lexical_lem2  \n",
       "0             0>1.0             0.57735  \n",
       "1             1>0.0                   0  \n",
       "2             0>1.0           0.0594733  \n",
       "3             1>0.0                   0  \n",
       "4             0>1.0            0.217571  \n",
       "5             1>0.0                   0  \n",
       "6             0>1.0           0.0353996  \n",
       "7             1>0.0           0.0444262  \n",
       "8             0>1.0           0.0395285  \n",
       "9             1>0.0            0.135582  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convo_real.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "turn_surrogate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "convo_surrogate.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
